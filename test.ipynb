{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from early_stop import EarlyStopping\n",
    "from torch.optim import Adam\n",
    "from models.SVDD_model import SVDD\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def anomaly_score(node_embedding, c):\n",
    "    # anomaly score of an instance is calculated by \n",
    "    # square Euclidean distance between the node embedding and the center c\n",
    "    return torch.sum((node_embedding - c) ** 2)\n",
    "\n",
    "def nor_loss(node_embedding_list, c):\n",
    "    # normal loss is calculated by mean squared Euclidian distance of \n",
    "    # the normal node embeddings to hypersphere center c \n",
    "    s = 0\n",
    "    num_node = node_embedding_list.size()[0]\n",
    "    for i in range(num_node):\n",
    "        s = s + anomaly_score(node_embedding_list[i], c)\n",
    "    return s/num_node\n",
    "\n",
    "# def AUC_loss(anomaly_node_emb, normal_node_emb, c):\n",
    "#     # AUC_loss encourages the score of anomaly instances to be higher than those of normal instances\n",
    "#     s = 0\n",
    "#     num_anomaly_node = anomaly_node_emb.size()[0]\n",
    "#     num_normal_node = normal_node_emb.size()[0]\n",
    "#     for i in range(num_anomaly_node):\n",
    "#         for j in range(num_normal_node):\n",
    "#             s1 = anomaly_score(anomaly_node_emb[i], c)\n",
    "#             s2 = anomaly_score(normal_node_emb[j], c)\n",
    "#             s = s + torch.sigmoid(s1 - s2)\n",
    "#     return s/(num_anomaly_node * num_normal_node) # check devide by zero\n",
    "\n",
    "def AUC_loss(anomaly_node_emb, normal_node_emb, c):\n",
    "    # AUC_loss encourages the score of anomaly instances to be higher than those of normal instances\n",
    "    s = 0\n",
    "    num_anomaly_node = anomaly_node_emb.size()[0]\n",
    "    num_normal_node = normal_node_emb.size()[0]\n",
    "    s2_list = []\n",
    "    for j in range(num_normal_node):\n",
    "        s2_list.append(anomaly_score(normal_node_emb[j], c))\n",
    "    for i in range(num_anomaly_node):\n",
    "            s1 = anomaly_score(anomaly_node_emb[i], c)\n",
    "            s = s + torch.sigmoid(s1 - torch.tensor(s2_list)).sum()\n",
    "    return s/(num_anomaly_node * num_normal_node) # check devide by zero\n",
    "\n",
    "def objecttive_loss(anomaly_node_emb, normal_node_emb, c, regularizer=1):\n",
    "    Nloss = nor_loss(normal_node_emb, c)\n",
    "    AUCloss = AUC_loss(anomaly_node_emb, normal_node_emb, c)\n",
    "    loss = Nloss - regularizer * AUCloss\n",
    "    return loss\n",
    "\n",
    "def normalize(feature):\n",
    "    \"\"\"Input: feature must be a 1d numpy array\n",
    "    \"\"\"\n",
    "    feature = np.array(feature)\n",
    "    mean = feature.mean()\n",
    "    std = feature.std()\n",
    "    return (feature - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Histogram-base Outlier Detection (HBOS)\n",
      "Model 2 Isolation Forest\n",
      "Model 3 Local Outlier Factor (LOF)\n",
      "Model 4 One-class SVM (OCSVM)\n",
      "Model 5 GCN\n",
      "Model 6 GIN\n",
      "Model 7 GAT\n",
      "Model 8 Mul-GAD\n",
      "\n",
      "1 fitting Histogram-base Outlier Detection (HBOS)\n",
      "\n",
      "2 fitting Isolation Forest\n",
      "\n",
      "3 fitting Local Outlier Factor (LOF)\n",
      "\n",
      "4 fitting One-class SVM (OCSVM)\n",
      "\n",
      "5 fitting GCN\n",
      "\n",
      "6 fitting GIN\n",
      "\n",
      "7 fitting GAT\n",
      "\n",
      "8 fitting Mul-GAD\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataset import pyg_dataset\n",
    "from pyod.models.hbos import HBOS\n",
    "graph = pyg_dataset(dataset_name=\"books\", dataset_spilt=[0.064,0.3,0.3]).dataset\n",
    "model = HBOS()\n",
    "model.fit(graph.x[graph.train_mask])\n",
    "\n",
    "test_auc = roc_auc_score(graph.y[graph.test_mask], y_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\AI Learning\\Mul-Graph_baseline\\dataset.py:100: UserWarning: Anomaly is min class of dataset and anomaly rate is not conformed to setting\n",
      "  warnings.warn(f\"Anomaly is min class of dataset and anomaly rate is not conformed to setting\")\n",
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\dgl\\heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "graph = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.064,0.3,0.3], anomaly_type=\"min\").dataset\n",
    "dgl_graph = pyg_to_dgl(graph)\n",
    "\n",
    "# train_normal mask and train_anomaly mask\n",
    "train_anomaly = [bool(graph.y[i] & graph.train_mask[i]) for i in range(len(graph.train_mask))] \n",
    "train_normal = [bool((~graph.y[i]) & graph.train_mask[i]) for i in range(len(graph.train_mask))]\n",
    "val_anomaly = [bool(graph.y[i] & graph.val_mask[i]) for i in range(len(graph.val_mask))]\n",
    "val_normal = [bool((~graph.y[i]) & graph.val_mask[i]) for i in range(len(graph.val_mask))]\n",
    "test_anomaly = [bool(graph.y[i] & graph.test_mask[i]) for i in range(len(graph.test_mask))]\n",
    "test_normal = [bool((~graph.y[i]) & graph.test_mask[i]) for i in range(len(graph.test_mask))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = graph.x\n",
    "edge_index = graph.edge_index\n",
    "true_label = graph.y\n",
    "node_features = graph.num_node_features\n",
    "AUC_regularizer = 1\n",
    "n_embedding = 32\n",
    "hiddle_dim = 32\n",
    "hiddle_layer = 2\n",
    "\n",
    "model = SVDD(input_dim = node_features, hiddle_dim=hiddle_dim, number_class= n_embedding, hiddle_layer= hiddle_layer, dropout=0.5)\n",
    "lr = 1e-2\n",
    "epochs = 50\n",
    "weight_decay = 5e-4\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "early_stopping = EarlyStopping(patience = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: -0.4991134703159332 loss_val: -0.49394768476486206 roc_auc: 0.5978 time: 0.2763s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0002 loss_train: -0.49533024430274963 loss_val: -0.4989123046398163 roc_auc: 0.6643 time: 0.5230s\n",
      "Epoch: 0003 loss_train: -0.5002880096435547 loss_val: -0.49801501631736755 roc_auc: 0.4931 time: 0.1616s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0004 loss_train: -0.5000959038734436 loss_val: -0.49885308742523193 roc_auc: 0.7487 time: 0.1822s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0005 loss_train: -0.5035908818244934 loss_val: -0.4991309940814972 roc_auc: 0.9104 time: 0.1583s\n",
      "Epoch: 0006 loss_train: -0.5061190724372864 loss_val: -0.498595267534256 roc_auc: 0.9063 time: 0.1645s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0007 loss_train: -0.5086079835891724 loss_val: -0.4986422657966614 roc_auc: 0.9176 time: 0.1633s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0008 loss_train: -0.5116948485374451 loss_val: -0.49955451488494873 roc_auc: 0.9324 time: 0.2254s\n",
      "Epoch: 0009 loss_train: -0.5255290269851685 loss_val: -0.5006705522537231 roc_auc: 0.9396 time: 0.2052s\n",
      "Epoch: 0010 loss_train: -0.5256335139274597 loss_val: -0.501409649848938 roc_auc: 0.9391 time: 0.1852s\n",
      "Epoch: 0011 loss_train: -0.5264497399330139 loss_val: -0.50298011302948 roc_auc: 0.9291 time: 0.2220s\n",
      "Epoch: 0012 loss_train: -0.5569727420806885 loss_val: -0.5007383227348328 roc_auc: 0.9167 time: 0.1827s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0013 loss_train: -0.5735666751861572 loss_val: -0.5024852156639099 roc_auc: 0.9229 time: 0.1881s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0014 loss_train: -0.5658811926841736 loss_val: -0.5046675801277161 roc_auc: 0.9264 time: 0.2843s\n",
      "Epoch: 0015 loss_train: -0.6222572922706604 loss_val: -0.5114119648933411 roc_auc: 0.9325 time: 0.1636s\n",
      "Epoch: 0016 loss_train: -0.670291006565094 loss_val: -0.5221772789955139 roc_auc: 0.9392 time: 0.2108s\n",
      "Epoch: 0017 loss_train: -0.6974895596504211 loss_val: -0.5340816974639893 roc_auc: 0.9436 time: 0.1969s\n",
      "Epoch: 0018 loss_train: -0.7222279906272888 loss_val: -0.5457738041877747 roc_auc: 0.9441 time: 0.1963s\n",
      "Epoch: 0019 loss_train: -0.741157054901123 loss_val: -0.5570805668830872 roc_auc: 0.9428 time: 0.1614s\n",
      "Epoch: 0020 loss_train: -0.814022421836853 loss_val: -0.5715610384941101 roc_auc: 0.9424 time: 0.1634s\n",
      "Epoch: 0021 loss_train: -0.8372465372085571 loss_val: -0.5758395195007324 roc_auc: 0.9300 time: 0.1673s\n",
      "Epoch: 0022 loss_train: -0.8618900179862976 loss_val: -0.5886828899383545 roc_auc: 0.9318 time: 0.1589s\n",
      "Epoch: 0023 loss_train: -0.8887550234794617 loss_val: -0.5995365381240845 roc_auc: 0.9265 time: 0.1455s\n",
      "Epoch: 0024 loss_train: -0.9135417342185974 loss_val: -0.6061921715736389 roc_auc: 0.9234 time: 0.1738s\n",
      "Epoch: 0025 loss_train: -0.9189140796661377 loss_val: -0.6082810163497925 roc_auc: 0.9236 time: 0.1403s\n",
      "Epoch: 0026 loss_train: -0.9442622065544128 loss_val: -0.6095720529556274 roc_auc: 0.9178 time: 0.1494s\n",
      "Epoch: 0027 loss_train: -0.9193253517150879 loss_val: -0.6119514107704163 roc_auc: 0.9213 time: 0.1435s\n",
      "Epoch: 0028 loss_train: -0.959021806716919 loss_val: -0.6128370761871338 roc_auc: 0.9218 time: 0.1492s\n",
      "Epoch: 0029 loss_train: -0.9607048630714417 loss_val: -0.6072400808334351 roc_auc: 0.9223 time: 0.1415s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0030 loss_train: -0.9593890309333801 loss_val: -0.6084549427032471 roc_auc: 0.9202 time: 0.1530s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0031 loss_train: -0.9681957960128784 loss_val: -0.6047887802124023 roc_auc: 0.9175 time: 0.1606s\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch: 0032 loss_train: -0.9782869219779968 loss_val: -0.603501558303833 roc_auc: 0.9151 time: 0.1540s\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch: 0033 loss_train: -0.9691705703735352 loss_val: -0.605461835861206 roc_auc: 0.9089 time: 0.1479s\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch: 0034 loss_train: -0.9792960286140442 loss_val: -0.6040345430374146 roc_auc: 0.9089 time: 0.1519s\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch: 0035 loss_train: -0.9780930876731873 loss_val: -0.5937054753303528 roc_auc: 0.9137 time: 0.1440s\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch: 0036 loss_train: -0.9800262451171875 loss_val: -0.6041312217712402 roc_auc: 0.9032 time: 0.1786s\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch: 0037 loss_train: -0.9838575720787048 loss_val: -0.6036945581436157 roc_auc: 0.9013 time: 0.2244s\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch: 0038 loss_train: -0.982297956943512 loss_val: -0.591678261756897 roc_auc: 0.9070 time: 0.1530s\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch: 0039 loss_train: -0.9808268547058105 loss_val: -0.6032336950302124 roc_auc: 0.8983 time: 0.1529s\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch: 0040 loss_train: -0.9892123937606812 loss_val: -0.6039518117904663 roc_auc: 0.8914 time: 0.1469s\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch: 0041 loss_train: -0.9787673354148865 loss_val: -0.6046081185340881 roc_auc: 0.8937 time: 0.1697s\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch: 0042 loss_train: -0.9852832555770874 loss_val: -0.6019249558448792 roc_auc: 0.8952 time: 0.1469s\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch: 0043 loss_train: -0.9846121668815613 loss_val: -0.6009445786476135 roc_auc: 0.8923 time: 0.1529s\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch: 0044 loss_train: -0.9842661619186401 loss_val: -0.6042711734771729 roc_auc: 0.8813 time: 0.1457s\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch: 0045 loss_train: -0.9865118265151978 loss_val: -0.6046160459518433 roc_auc: 0.8795 time: 0.1545s\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch: 0046 loss_train: -0.9886618256568909 loss_val: -0.6009578704833984 roc_auc: 0.8790 time: 0.1475s\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch: 0047 loss_train: -0.9862549304962158 loss_val: -0.6069980263710022 roc_auc: 0.8714 time: 0.1530s\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch: 0048 loss_train: -0.9775724411010742 loss_val: -0.6062687635421753 roc_auc: 0.8640 time: 0.1460s\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# trainning\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    node_embedding = model(dgl_graph,features)\n",
    "    if epoch % 10 == 0:\n",
    "        center = model(dgl_graph, features)[train_normal].detach().mean(0)\n",
    "    \n",
    "    loss_train = objecttive_loss(node_embedding[train_anomaly], node_embedding[train_normal], center, AUC_regularizer)\n",
    "    model.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    node_embedding = model(dgl_graph, features)\n",
    "\n",
    "    loss_val = objecttive_loss(node_embedding[val_anomaly], node_embedding[val_normal], center, AUC_regularizer)\n",
    "    val_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "    auc = roc_auc_score(graph.y[graph.val_mask], val_anomaly_score)\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.item()),\n",
    "          'loss_val: {}'.format(loss_val.item()),\n",
    "          'roc_auc: {:.4f}'.format(auc),\n",
    "          'time: {:.4f}s'.format(time.time() - start))    \n",
    "    early_stopping(loss_val, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Auc: 0.9038879440665154\n"
     ]
    }
   ],
   "source": [
    "test_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.test_mask].cpu().detach()]\n",
    "auc = roc_auc_score(graph.y[graph.test_mask], test_anomaly_score)\n",
    "\n",
    "print(f\"Test Auc: {auc}\")\n",
    "\n",
    "test_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "auc = roc_auc_score(graph.y[graph.val_mask], test_anomaly_score)\n",
    "\n",
    "print(f\"Val Auc: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\AI Learning\\Mul-Graph_baseline\\dataset.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.dataset = Data(x=temp.x, edge_index=temp.edge_index,y=torch.tensor(temp.y, dtype=torch.long),train_mask=position,val_mask=position,test_mask=position)\n",
      "g:\\AI Learning\\Mul-Graph_baseline\\dataset.py:108: UserWarning: Anomaly is organic and anomaly rate is not conformed to setting\n",
      "  warnings.warn(f\"Anomaly is organic and anomaly rate is not conformed to setting\")\n",
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\dgl\\heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    }
   ],
   "source": [
    "from label_SVDD_model import label_SVDD\n",
    "\n",
    "# load the dataset\n",
    "# graph = pyg_dataset(dataset_name=\"citeseer\", dataset_spilt=[0.1,0.2,0.2], anomaly_type=\"min\").dataset\n",
    "graph = pyg_dataset(dataset_name=\"reddit\", dataset_spilt=[0.2,0.2,0.2]).dataset\n",
    "dgl_graph = pyg_to_dgl(graph)\n",
    "\n",
    "# train_normal mask and train_anomaly mask\n",
    "train_anomaly = [bool(graph.y[i] & graph.train_mask[i]) for i in range(len(graph.train_mask))] \n",
    "train_normal = [bool((~graph.y[i]) & graph.train_mask[i]) for i in range(len(graph.train_mask))]\n",
    "val_anomaly = [bool(graph.y[i] & graph.val_mask[i]) for i in range(len(graph.val_mask))]\n",
    "val_normal = [bool((~graph.y[i]) & graph.val_mask[i]) for i in range(len(graph.val_mask))]\n",
    "test_anomaly = [bool(graph.y[i] & graph.test_mask[i]) for i in range(len(graph.test_mask))]\n",
    "test_normal = [bool((~graph.y[i]) & graph.test_mask[i]) for i in range(len(graph.test_mask))]\n",
    "\n",
    "features = graph.x\n",
    "edge_index = graph.edge_index\n",
    "true_label = graph.y\n",
    "node_features = graph.num_node_features\n",
    "AUC_regularizer = 1\n",
    "n_embedding = 32\n",
    "hiddle_dim = 32\n",
    "hiddle_layer = 2\n",
    "number_class = 2\n",
    "label_svdd_balance = 1\n",
    "\n",
    "model = label_SVDD(input_dim = node_features, hiddle_dim=hiddle_dim, embedding_dim=n_embedding, number_class=number_class, hiddle_layer= hiddle_layer, dropout=0.5)\n",
    "lr = 1e-2\n",
    "epochs = 50\n",
    "weight_decay = 5e-4\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "early_stopping1 = EarlyStopping(patience = 20)\n",
    "early_stopping2 = EarlyStopping(patience = 20)\n",
    "early_stopping3 = EarlyStopping(patience = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: -0.49779295921325684 loss_val: -0.4875163435935974 roc_auc: 0.4837 time: 0.9552s\n",
      "Epoch: 0002 loss_train: -0.48815345764160156 loss_val: -0.49781185388565063 roc_auc: 0.5813 time: 1.0550s\n",
      "Epoch: 0003 loss_train: -0.49753686785697937 loss_val: -0.496715247631073 roc_auc: 0.5293 time: 1.0915s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0004 loss_train: -0.4964378774166107 loss_val: -0.4958604872226715 roc_auc: 0.5133 time: 0.9278s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0005 loss_train: -0.49571889638900757 loss_val: -0.4973612427711487 roc_auc: 0.5083 time: 0.9538s\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch: 0006 loss_train: -0.49690282344818115 loss_val: -0.4983936846256256 roc_auc: 0.5010 time: 0.9993s\n",
      "Epoch: 0007 loss_train: -0.49823835492134094 loss_val: -0.49883249402046204 roc_auc: 0.5543 time: 0.8781s\n",
      "Epoch: 0008 loss_train: -0.49871698021888733 loss_val: -0.4991220533847809 roc_auc: 0.5455 time: 0.8664s\n",
      "Epoch: 0009 loss_train: -0.4991081953048706 loss_val: -0.49894627928733826 roc_auc: 0.5345 time: 0.8627s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0010 loss_train: -0.49895575642585754 loss_val: -0.49878454208374023 roc_auc: 0.5405 time: 0.9598s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0011 loss_train: -0.4996812641620636 loss_val: -0.4995187520980835 roc_auc: 0.6002 time: 0.8871s\n",
      "Epoch: 0012 loss_train: -0.4995933771133423 loss_val: -0.49946075677871704 roc_auc: 0.6110 time: 0.8420s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0013 loss_train: -0.4995097815990448 loss_val: -0.49956223368644714 roc_auc: 0.6276 time: 0.8519s\n",
      "Epoch: 0014 loss_train: -0.4995690882205963 loss_val: -0.49969539046287537 roc_auc: 0.6411 time: 0.8468s\n",
      "Epoch: 0015 loss_train: -0.4996795356273651 loss_val: -0.4997853636741638 roc_auc: 0.6429 time: 0.8573s\n",
      "Epoch: 0016 loss_train: -0.4997471272945404 loss_val: -0.4998256266117096 roc_auc: 0.6253 time: 0.8506s\n",
      "Epoch: 0017 loss_train: -0.49978601932525635 loss_val: -0.49982157349586487 roc_auc: 0.5991 time: 0.9324s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0018 loss_train: -0.4997785985469818 loss_val: -0.4998113811016083 roc_auc: 0.5385 time: 0.9471s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0019 loss_train: -0.4997725486755371 loss_val: -0.4998264014720917 roc_auc: 0.5263 time: 1.0494s\n",
      "Epoch: 0020 loss_train: -0.4997921288013458 loss_val: -0.4998619556427002 roc_auc: 0.5310 time: 1.1328s\n",
      "Epoch: 0021 loss_train: -0.49989452958106995 loss_val: -0.49991676211357117 roc_auc: 0.5523 time: 1.0478s\n",
      "Epoch: 0022 loss_train: -0.49988284707069397 loss_val: -0.49990132451057434 roc_auc: 0.5384 time: 0.8811s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0023 loss_train: -0.4998747408390045 loss_val: -0.49990466237068176 roc_auc: 0.5067 time: 0.8051s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0024 loss_train: -0.49987608194351196 loss_val: -0.49992454051971436 roc_auc: 0.4770 time: 1.9047s\n",
      "Epoch: 0025 loss_train: -0.4998972415924072 loss_val: -0.49994611740112305 roc_auc: 0.4767 time: 1.1495s\n",
      "Epoch: 0026 loss_train: -0.4999251365661621 loss_val: -0.49995800852775574 roc_auc: 0.5153 time: 0.8505s\n",
      "Epoch: 0027 loss_train: -0.4999394416809082 loss_val: -0.49996212124824524 roc_auc: 0.5624 time: 0.7596s\n",
      "Epoch: 0028 loss_train: -0.49994614720344543 loss_val: -0.4999629557132721 roc_auc: 0.5333 time: 0.7638s\n",
      "Epoch: 0029 loss_train: -0.4999493956565857 loss_val: -0.49996379017829895 roc_auc: 0.5212 time: 0.7658s\n",
      "Epoch: 0030 loss_train: -0.49995291233062744 loss_val: -0.499967098236084 roc_auc: 0.5176 time: 0.7873s\n",
      "Epoch: 0031 loss_train: -0.49997565150260925 loss_val: -0.4999845623970032 roc_auc: 0.5222 time: 1.2416s\n",
      "Epoch: 0032 loss_train: -0.49997615814208984 loss_val: -0.4999832808971405 roc_auc: 0.5144 time: 2.6592s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0033 loss_train: -0.4999748170375824 loss_val: -0.4999856650829315 roc_auc: 0.4994 time: 0.9872s\n",
      "Epoch: 0034 loss_train: -0.49997952580451965 loss_val: -0.49999016523361206 roc_auc: 0.4933 time: 0.9235s\n",
      "Epoch: 0035 loss_train: -0.49998563528060913 loss_val: -0.4999921917915344 roc_auc: 0.5272 time: 0.9574s\n",
      "Epoch: 0036 loss_train: -0.49998795986175537 loss_val: -0.4999922811985016 roc_auc: 0.5347 time: 1.2607s\n",
      "Epoch: 0037 loss_train: -0.4999885857105255 loss_val: -0.49999234080314636 roc_auc: 0.5280 time: 1.7909s\n",
      "Epoch: 0038 loss_train: -0.4999890923500061 loss_val: -0.4999934732913971 roc_auc: 0.5266 time: 1.6714s\n",
      "Epoch: 0039 loss_train: -0.4999912977218628 loss_val: -0.49999478459358215 roc_auc: 0.5416 time: 1.7372s\n",
      "Epoch: 0040 loss_train: -0.49999290704727173 loss_val: -0.49999532103538513 roc_auc: 0.5551 time: 1.6360s\n",
      "Epoch: 0041 loss_train: -0.49999576807022095 loss_val: -0.49999672174453735 roc_auc: 0.5587 time: 1.6522s\n",
      "Epoch: 0042 loss_train: -0.49999579787254333 loss_val: -0.4999959468841553 roc_auc: 0.5462 time: 1.6469s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0043 loss_train: -0.4999951720237732 loss_val: -0.49999648332595825 roc_auc: 0.5518 time: 1.9692s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0044 loss_train: -0.499995619058609 loss_val: -0.499997615814209 roc_auc: 0.5585 time: 1.6853s\n",
      "Epoch: 0045 loss_train: -0.4999969005584717 loss_val: -0.4999982416629791 roc_auc: 0.5635 time: 1.6079s\n",
      "Epoch: 0046 loss_train: -0.4999978542327881 loss_val: -0.4999980628490448 roc_auc: 0.5588 time: 1.6542s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0047 loss_train: -0.4999977648258209 loss_val: -0.4999981224536896 roc_auc: 0.5479 time: 1.7750s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0048 loss_train: -0.4999978840351105 loss_val: -0.4999985098838806 roc_auc: 0.5414 time: 1.6699s\n",
      "Epoch: 0049 loss_train: -0.49999821186065674 loss_val: -0.49999886751174927 roc_auc: 0.5456 time: 1.0813s\n",
      "Epoch: 0050 loss_train: -0.49999868869781494 loss_val: -0.4999988079071045 roc_auc: 0.5340 time: 1.0011s\n",
      "EarlyStopping counter: 1 out of 20\n"
     ]
    }
   ],
   "source": [
    "from torch.functional import F\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# trainning\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    node_embedding, logits = model(dgl_graph,features)\n",
    "    if epoch % 10 == 0:\n",
    "        center = model(dgl_graph, features)[0][train_normal].detach().mean(0)\n",
    "    loss_train = objecttive_loss(node_embedding[train_anomaly], node_embedding[train_normal], center, AUC_regularizer)\n",
    "    model.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    node_embedding, logits = model(dgl_graph, features)\n",
    "\n",
    "    loss_val = objecttive_loss(node_embedding[val_anomaly], node_embedding[val_normal], center, AUC_regularizer)\n",
    "    val_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "    auc = roc_auc_score(graph.y[graph.val_mask], val_anomaly_score)\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.item()),\n",
    "          'loss_val: {}'.format(loss_val.item()),\n",
    "          'roc_auc: {:.4f}'.format(auc),\n",
    "          'time: {:.4f}s'.format(time.time() - start))    \n",
    "    early_stopping1(loss_val, model)\n",
    "    if early_stopping1.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     start = time.time()\n",
    "#     model.train()\n",
    "#     node_embedding, logits = model(dgl_graph,features)\n",
    "#     if epoch % 10 == 0:\n",
    "#         center = model(dgl_graph, features)[0][train_normal].detach().mean(0)\n",
    "#     # SVDD loss function\n",
    "#     loss_train = F.cross_entropy(logits[graph.train_mask],graph.y[graph.train_mask],weight=torch.tensor([1.0, 14.5]))\n",
    "#     model.zero_grad()\n",
    "#     loss_train.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     node_embedding, logits = model(dgl_graph, features)\n",
    "\n",
    "#     loss_val = F.cross_entropy(logits[graph.val_mask],graph.y[graph.val_mask],weight=torch.tensor([1.0, 14.5]))\n",
    "#     auc_label = roc_auc_score(graph.y[graph.val_mask].detach().numpy(),logits[graph.val_mask][:,1].detach().numpy())\n",
    "\n",
    "#     val_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "#     auc_svdd = roc_auc_score(graph.y[graph.val_mask], val_anomaly_score)\n",
    "    \n",
    "#     print('Epoch: {:04d}'.format(epoch+1),\n",
    "#           'loss_train: {}'.format(loss_train.item()),\n",
    "#           'loss_val: {}'.format(loss_val.item()),\n",
    "#           'roc_auc_label: {:.4f}'.format(auc_label),\n",
    "#           'roc_auc_svdd: {:.4f}'.format(auc_svdd),\n",
    "#           'time: {:.4f}s'.format(time.time() - start))    \n",
    "#     early_stopping2(loss_val, model)\n",
    "#     if early_stopping2.early_stop:\n",
    "#         print(\"Early stopping\")\n",
    "#         break\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     start = time.time()\n",
    "#     model.train()\n",
    "#     node_embedding, logits = model(dgl_graph,features)\n",
    "#     if epoch % 10 == 0:\n",
    "#         center = model(dgl_graph, features)[0][train_normal].detach().mean(0)\n",
    "\n",
    "#     # SVDD loss function\n",
    "#     loss_svdd = objecttive_loss(node_embedding[train_anomaly], node_embedding[train_normal], center, AUC_regularizer)\n",
    "#     loss_label = F.cross_entropy(logits[graph.train_mask],graph.y[graph.train_mask],weight=torch.tensor([1.0, 7]))\n",
    "#     loss_train = loss_svdd + label_svdd_balance * loss_label\n",
    "#     model.zero_grad()\n",
    "#     loss_train.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     node_embedding, logits = model(dgl_graph, features)\n",
    "\n",
    "#     loss_label = F.cross_entropy(logits[graph.val_mask],graph.y[graph.val_mask],weight=torch.tensor([1.0, 7]))\n",
    "#     loss_svdd = objecttive_loss(node_embedding[train_anomaly], node_embedding[train_normal], center, AUC_regularizer)\n",
    "#     loss_val = loss_svdd + loss_label\n",
    "    \n",
    "#     auc_label = roc_auc_score(graph.y[graph.val_mask].detach().numpy(),logits[graph.val_mask][:,1].detach().numpy())\n",
    "#     val_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "#     auc_svdd = roc_auc_score(graph.y[graph.val_mask], val_anomaly_score)\n",
    "    \n",
    "#     print('Epoch: {:04d}'.format(epoch+1),\n",
    "#           'loss_train: {}'.format(loss_train.item()),\n",
    "#           'loss_val: {}'.format(loss_val.item()),\n",
    "#           'roc_auc_label: {:.4f}'.format(auc_label),\n",
    "#           'roc_auc_svdd: {:.4f}'.format(auc_svdd),\n",
    "#           'time: {:.4f}s'.format(time.time() - start))    \n",
    "#     early_stopping3(loss_val, model)\n",
    "#     if early_stopping3.early_stop:\n",
    "#         print(\"Early stopping\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test svdd_Auc: 0.5614562396089233\n",
      "Val svdd_Auc: 0.5340128005769275\n",
      "\n",
      "Test label_Auc: 0.607921412441996\n",
      "Val label_Auc: 0.5534333941811462\n",
      "\n",
      "Test label_svdd_Auc: 0.4578122285912802\n",
      "Val label_svdd_Auc: 0.5028790029973181\n"
     ]
    }
   ],
   "source": [
    "node_embedding, logits = model(dgl_graph,features)\n",
    "\n",
    "test_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.test_mask].cpu().detach()]\n",
    "auc_svdd = roc_auc_score(graph.y[graph.test_mask], test_anomaly_score)\n",
    "\n",
    "print(f\"Test svdd_Auc: {auc_svdd}\")\n",
    "\n",
    "val_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "auc_svdd = roc_auc_score(graph.y[graph.val_mask], val_anomaly_score)\n",
    "\n",
    "print(f\"Val svdd_Auc: {auc_svdd}\\n\")\n",
    "\n",
    "auc_label = roc_auc_score(graph.y[graph.test_mask].detach().numpy(),logits[graph.test_mask][:,0].detach().numpy())\n",
    "print(f\"Test label_Auc: {auc_label}\")\n",
    "\n",
    "auc_label = roc_auc_score(graph.y[graph.val_mask].detach().numpy(),logits[graph.val_mask][:,0].detach().numpy())\n",
    "print(f\"Val label_Auc: {auc_label}\\n\")\n",
    "\n",
    "\n",
    "auc_label_svdd = roc_auc_score(graph.y[graph.test_mask].detach().numpy(), normalize(logits[graph.test_mask][:,1].detach().numpy())  + normalize(test_anomaly_score))\n",
    "print(f\"Test label_svdd_Auc: {auc_label_svdd}\")\n",
    "\n",
    "auc_label_svdd = roc_auc_score(graph.y[graph.val_mask].detach().numpy(), normalize(logits[graph.val_mask][:,1].detach().numpy()) + normalize(val_anomaly_score))\n",
    "print(f\"Val label_svdd_Auc: {auc_label_svdd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 BWGNN 与 DOMINANT 的数据融合效果\n",
    "\n",
    "Expected: 能达到BWGNN的性能，同时能利用DOMINANT的表征，对syn异常也有很好的表现\n",
    "\n",
    "1. **设计实验，判断DOMINANT的表征，训练一个二分类网络，判断效果如何**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly syntheic is on processing\n",
      "using 0.08 seconds\n",
      "Epoch 0000: Loss 0.5380\n",
      "Epoch 0001: Loss 0.5364\n",
      "Epoch 0002: Loss 0.5346\n",
      "Epoch 0003: Loss 0.5337\n",
      "Epoch 0004: Loss 0.5334\n",
      "Epoch 0005: Loss 0.5333\n",
      "Epoch 0006: Loss 0.5329\n",
      "Epoch 0007: Loss 0.5326\n",
      "Epoch 0008: Loss 0.5322\n",
      "Epoch 0009: Loss 0.5320\n",
      "Raw scores: [0.55513704 0.44958687 0.54521406 ... 0.3541517  0.54282665 0.55279076]\n",
      "AUC Score: 0.94759867558353\n",
      "AUC My Score: 0.94759867558353\n"
     ]
    }
   ],
   "source": [
    "from pygod.models import DOMINANT\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "\n",
    "data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.2], anomaly_type=\"syn\", anomaly_ratio=0.1).dataset\n",
    "# data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.2], anomaly_type=\"min\").dataset\n",
    "model = DOMINANT(verbose=True, gpu=-1, epoch=10, lr=1e-3)\n",
    "model = model.fit(data)\n",
    "\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "x_, s_, hid_dominate = model.model(data.x, data.edge_index)\n",
    "s = to_dense_adj(data.edge_index)[0]\n",
    "score = model.loss_func(data.x,x_,s,s_)\n",
    "score = score.detach().cpu().numpy()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "outlier_scores = model.decision_function(data)\n",
    "print(f'Raw scores: {outlier_scores}')\n",
    "auc_score = roc_auc_score(data.y.numpy(), outlier_scores)\n",
    "my_score = roc_auc_score(data.y.numpy(), score)\n",
    "print('AUC Score:', auc_score)\n",
    "print('AUC My Score:', my_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOMINANT 表征数据获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BWGNN 表征数据获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly syntheic is on processing\n",
      "using 0.09 seconds\n",
      "Epoch 1/300: loss: 0.6900728344917297, auc: 0.721964782205746\n",
      "Epoch 2/300: loss: 0.6899038553237915, auc: 0.7219750798064052\n",
      "Epoch 3/300: loss: 0.6897387504577637, auc: 0.7219647822057461\n",
      "Epoch 4/300: loss: 0.6895783543586731, auc: 0.7220059726083823\n",
      "Epoch 5/300: loss: 0.6894214749336243, auc: 0.7220162702090412\n",
      "Epoch 6/300: loss: 0.6892673969268799, auc: 0.7216764493872929\n",
      "Epoch 7/300: loss: 0.6891182065010071, auc: 0.721511687776748\n",
      "Epoch 8/300: loss: 0.6889744997024536, auc: 0.7212027597569767\n",
      "Epoch 9/300: loss: 0.688834011554718, auc: 0.7211409741530225\n",
      "Epoch 10/300: loss: 0.6886962056159973, auc: 0.7209762125424777\n",
      "Epoch 11/300: loss: 0.688559889793396, auc: 0.7208320461332509\n",
      "Epoch 12/300: loss: 0.6884261965751648, auc: 0.7207496653279785\n",
      "Epoch 13/300: loss: 0.6882947683334351, auc: 0.7205849037174339\n",
      "Epoch 14/300: loss: 0.6881651878356934, auc: 0.7206672845227061\n",
      "Epoch 15/300: loss: 0.688037633895874, auc: 0.7206260941200701\n",
      "Epoch 16/300: loss: 0.6879118084907532, auc: 0.7205231181134795\n",
      "Epoch 17/300: loss: 0.6877872347831726, auc: 0.7205643085161156\n",
      "Epoch 18/300: loss: 0.6876636743545532, auc: 0.7206672845227062\n",
      "Epoch 19/300: loss: 0.6875424981117249, auc: 0.7206363917207291\n",
      "Epoch 20/300: loss: 0.6874231696128845, auc: 0.7207496653279786\n",
      "Epoch 21/300: loss: 0.68730628490448, auc: 0.720832046133251\n",
      "Epoch 22/300: loss: 0.6871911883354187, auc: 0.7208938317372053\n",
      "Epoch 23/300: loss: 0.6870778203010559, auc: 0.7208526413345689\n",
      "Epoch 24/300: loss: 0.6869662404060364, auc: 0.7207290701266604\n",
      "Epoch 25/300: loss: 0.6868559718132019, auc: 0.7208114509319329\n",
      "Epoch 26/300: loss: 0.6867470145225525, auc: 0.7208114509319329\n",
      "Epoch 27/300: loss: 0.6866387724876404, auc: 0.7208938317372052\n",
      "Epoch 28/300: loss: 0.6865314841270447, auc: 0.7208732365358871\n",
      "Epoch 29/300: loss: 0.686424195766449, auc: 0.7208320461332509\n",
      "Epoch 30/300: loss: 0.686316967010498, auc: 0.7208114509319328\n",
      "Epoch 31/300: loss: 0.6862099170684814, auc: 0.7207496653279786\n",
      "Epoch 32/300: loss: 0.6861028671264648, auc: 0.720852641334569\n",
      "Epoch 33/300: loss: 0.6859956383705139, auc: 0.720852641334569\n",
      "Epoch 34/300: loss: 0.6858887672424316, auc: 0.7208320461332509\n",
      "Epoch 35/300: loss: 0.6857825517654419, auc: 0.7207084749253424\n",
      "Epoch 36/300: loss: 0.685676634311676, auc: 0.72062609412007\n",
      "Epoch 37/300: loss: 0.6855700612068176, auc: 0.7204819277108434\n",
      "Epoch 38/300: loss: 0.685463011264801, auc: 0.7204819277108434\n",
      "Epoch 39/300: loss: 0.6853549480438232, auc: 0.7204613325095252\n",
      "Epoch 40/300: loss: 0.6852456331253052, auc: 0.7205025229121614\n",
      "Epoch 41/300: loss: 0.685136079788208, auc: 0.7206054989187519\n",
      "Epoch 42/300: loss: 0.6850258111953735, auc: 0.7206878797240243\n",
      "Epoch 43/300: loss: 0.6849146485328674, auc: 0.7208114509319329\n",
      "Epoch 44/300: loss: 0.6848028302192688, auc: 0.7209144269385234\n",
      "Epoch 45/300: loss: 0.6846897602081299, auc: 0.7211409741530224\n",
      "Epoch 46/300: loss: 0.6845754384994507, auc: 0.7212027597569767\n",
      "Epoch 47/300: loss: 0.6844596266746521, auc: 0.721264545360931\n",
      "Epoch 48/300: loss: 0.684343159198761, auc: 0.7213263309648852\n",
      "Epoch 49/300: loss: 0.6842257380485535, auc: 0.7214704973741118\n",
      "Epoch 50/300: loss: 0.6841066479682922, auc: 0.721511687776748\n",
      "Epoch 51/300: loss: 0.6839859485626221, auc: 0.7216764493872928\n",
      "Epoch 52/300: loss: 0.6838635802268982, auc: 0.7217588301925651\n",
      "Epoch 53/300: loss: 0.6837401986122131, auc: 0.7218618061991557\n",
      "Epoch 54/300: loss: 0.6836150884628296, auc: 0.7220059726083824\n",
      "Epoch 55/300: loss: 0.6834882497787476, auc: 0.7221501390176088\n",
      "Epoch 56/300: loss: 0.6833592057228088, auc: 0.7222943054268357\n",
      "Epoch 57/300: loss: 0.6832279562950134, auc: 0.7224384718360622\n",
      "Epoch 58/300: loss: 0.6830941438674927, auc: 0.7226444238492431\n",
      "Epoch 59/300: loss: 0.682957649230957, auc: 0.7228709710637422\n",
      "Epoch 60/300: loss: 0.6828188300132751, auc: 0.7229121614663783\n",
      "Epoch 61/300: loss: 0.6826773285865784, auc: 0.7230975182782411\n",
      "Epoch 62/300: loss: 0.6825323700904846, auc: 0.7233652558953764\n",
      "Epoch 63/300: loss: 0.6823843121528625, auc: 0.7236741839151478\n",
      "Epoch 64/300: loss: 0.6822326183319092, auc: 0.7237565647204202\n",
      "Epoch 65/300: loss: 0.6820778846740723, auc: 0.7241066831428277\n",
      "Epoch 66/300: loss: 0.6819202303886414, auc: 0.7243126351560087\n",
      "Epoch 67/300: loss: 0.6817594170570374, auc: 0.7244156111625991\n",
      "Epoch 68/300: loss: 0.6815962791442871, auc: 0.7245803727731439\n",
      "Epoch 69/300: loss: 0.681430995464325, auc: 0.7248275151889609\n",
      "Epoch 70/300: loss: 0.6812632083892822, auc: 0.7252394192153229\n",
      "Epoch 71/300: loss: 0.6810929179191589, auc: 0.7253835856245495\n",
      "Epoch 72/300: loss: 0.6809207797050476, auc: 0.7256101328390486\n",
      "Epoch 73/300: loss: 0.6807469725608826, auc: 0.7256719184430028\n",
      "Epoch 74/300: loss: 0.6805709600448608, auc: 0.7258984656575019\n",
      "Epoch 75/300: loss: 0.6803923845291138, auc: 0.7262279888785914\n",
      "Epoch 76/300: loss: 0.6802111864089966, auc: 0.7265575120996808\n",
      "Epoch 77/300: loss: 0.6800270676612854, auc: 0.7267840593141799\n",
      "Epoch 78/300: loss: 0.6798405051231384, auc: 0.727051796931315\n",
      "Epoch 79/300: loss: 0.6796510815620422, auc: 0.7273195345484502\n",
      "Epoch 80/300: loss: 0.6794589757919312, auc: 0.7276078673669035\n",
      "Epoch 81/300: loss: 0.6792640089988708, auc: 0.7279167953866749\n",
      "Epoch 82/300: loss: 0.6790655255317688, auc: 0.7280609617959014\n",
      "Epoch 83/300: loss: 0.6788632273674011, auc: 0.7284728658222634\n",
      "Epoch 84/300: loss: 0.6786579489707947, auc: 0.7287817938420348\n",
      "Epoch 85/300: loss: 0.6784487962722778, auc: 0.7290495314591701\n",
      "Epoch 86/300: loss: 0.6782357692718506, auc: 0.7293996498815776\n",
      "Epoch 87/300: loss: 0.6780195832252502, auc: 0.7298733395118938\n",
      "Epoch 88/300: loss: 0.6777992844581604, auc: 0.7302852435382556\n",
      "Epoch 89/300: loss: 0.6775750517845154, auc: 0.7308001235712079\n",
      "Epoch 90/300: loss: 0.6773463487625122, auc: 0.7311914323962516\n",
      "Epoch 91/300: loss: 0.677112877368927, auc: 0.7316445268252497\n",
      "Epoch 92/300: loss: 0.6768742203712463, auc: 0.7321800020595202\n",
      "Epoch 93/300: loss: 0.6766306757926941, auc: 0.7326433940891772\n",
      "Epoch 94/300: loss: 0.6763818860054016, auc: 0.7332921429306972\n",
      "Epoch 95/300: loss: 0.6761279106140137, auc: 0.7338070229636495\n",
      "Epoch 96/300: loss: 0.675868570804596, auc: 0.7343836886005561\n",
      "Epoch 97/300: loss: 0.6756033897399902, auc: 0.7347749974255999\n",
      "Epoch 98/300: loss: 0.6753327250480652, auc: 0.7352280918545979\n",
      "Epoch 99/300: loss: 0.675056517124176, auc: 0.7357429718875502\n",
      "Epoch 100/300: loss: 0.6747748255729675, auc: 0.7364020183297292\n",
      "Epoch 101/300: loss: 0.6744869351387024, auc: 0.7366903511481825\n",
      "Epoch 102/300: loss: 0.6741925477981567, auc: 0.7373699927916796\n",
      "Epoch 103/300: loss: 0.6738913655281067, auc: 0.7377407064154052\n",
      "Epoch 104/300: loss: 0.6735833883285522, auc: 0.7384203480589022\n",
      "Epoch 105/300: loss: 0.6732683181762695, auc: 0.7388940376892185\n",
      "Epoch 106/300: loss: 0.672945499420166, auc: 0.7395942745340337\n",
      "Epoch 107/300: loss: 0.6726147532463074, auc: 0.7402533209762125\n",
      "Epoch 108/300: loss: 0.6722766160964966, auc: 0.7409329626197095\n",
      "Epoch 109/300: loss: 0.6719304919242859, auc: 0.7415714138605706\n",
      "Epoch 110/300: loss: 0.6715764403343201, auc: 0.742127484296159\n",
      "Epoch 111/300: loss: 0.6712136268615723, auc: 0.7428277211409742\n",
      "Epoch 112/300: loss: 0.6708422303199768, auc: 0.743466172381835\n",
      "Epoch 113/300: loss: 0.6704623103141785, auc: 0.7440016476161054\n",
      "Epoch 114/300: loss: 0.6700729131698608, auc: 0.7446812892596025\n",
      "Epoch 115/300: loss: 0.6696733832359314, auc: 0.7454227165070538\n",
      "Epoch 116/300: loss: 0.6692639589309692, auc: 0.7460817629492329\n",
      "Epoch 117/300: loss: 0.6688444018363953, auc: 0.7468437853980022\n",
      "Epoch 118/300: loss: 0.6684141755104065, auc: 0.7475028318401812\n",
      "Epoch 119/300: loss: 0.6679745316505432, auc: 0.7481824734836785\n",
      "Epoch 120/300: loss: 0.6675242781639099, auc: 0.7491092575429925\n",
      "Epoch 121/300: loss: 0.6670625805854797, auc: 0.7496241375759448\n",
      "Epoch 122/300: loss: 0.6665899157524109, auc: 0.7504067552260323\n",
      "Epoch 123/300: loss: 0.6661056280136108, auc: 0.7510863968695294\n",
      "Epoch 124/300: loss: 0.6656090617179871, auc: 0.7518896097209349\n",
      "Epoch 125/300: loss: 0.6650993824005127, auc: 0.7527134177736587\n",
      "Epoch 126/300: loss: 0.6645759344100952, auc: 0.7534136546184739\n",
      "Epoch 127/300: loss: 0.664039671421051, auc: 0.7542992482751518\n",
      "Epoch 128/300: loss: 0.6634895205497742, auc: 0.7551848419318299\n",
      "Epoch 129/300: loss: 0.66292405128479, auc: 0.7564617444135516\n",
      "Epoch 130/300: loss: 0.6623438596725464, auc: 0.7571619812583668\n",
      "Epoch 131/300: loss: 0.6617501378059387, auc: 0.7579445989084543\n",
      "Epoch 132/300: loss: 0.6611412167549133, auc: 0.7590361445783131\n",
      "Epoch 133/300: loss: 0.6605167984962463, auc: 0.7600041190402637\n",
      "Epoch 134/300: loss: 0.6598770022392273, auc: 0.7608073318916692\n",
      "Epoch 135/300: loss: 0.659220814704895, auc: 0.7618576871588919\n",
      "Epoch 136/300: loss: 0.6585476398468018, auc: 0.7631963752445681\n",
      "Epoch 137/300: loss: 0.6578571200370789, auc: 0.764287920914427\n",
      "Epoch 138/300: loss: 0.6571481227874756, auc: 0.7655236329935126\n",
      "Epoch 139/300: loss: 0.6564211845397949, auc: 0.7669241066831427\n",
      "Epoch 140/300: loss: 0.6556744575500488, auc: 0.767850890742457\n",
      "Epoch 141/300: loss: 0.6549078226089478, auc: 0.7688806508083617\n",
      "Epoch 142/300: loss: 0.6541230082511902, auc: 0.770281124497992\n",
      "Epoch 143/300: loss: 0.6533183455467224, auc: 0.7714138605704872\n",
      "Epoch 144/300: loss: 0.6524942517280579, auc: 0.7724436206363917\n",
      "Epoch 145/300: loss: 0.6516499519348145, auc: 0.7732880238904335\n",
      "Epoch 146/300: loss: 0.6507841944694519, auc: 0.7743177839563382\n",
      "Epoch 147/300: loss: 0.6498949527740479, auc: 0.7758006384512408\n",
      "Epoch 148/300: loss: 0.6489835977554321, auc: 0.7770981361342807\n",
      "Epoch 149/300: loss: 0.6480494737625122, auc: 0.7785398002265472\n",
      "Epoch 150/300: loss: 0.6470917463302612, auc: 0.7793224178766348\n",
      "Epoch 151/300: loss: 0.6461105942726135, auc: 0.7805787251570384\n",
      "Epoch 152/300: loss: 0.6451072096824646, auc: 0.7817320564308516\n",
      "Epoch 153/300: loss: 0.6440789103507996, auc: 0.7824116980743486\n",
      "Epoch 154/300: loss: 0.6430270671844482, auc: 0.7832149109257542\n",
      "Epoch 155/300: loss: 0.6419517993927002, auc: 0.7849655030377922\n",
      "Epoch 156/300: loss: 0.6408530473709106, auc: 0.7863453815261043\n",
      "Epoch 157/300: loss: 0.639729380607605, auc: 0.7875399032025537\n",
      "Epoch 158/300: loss: 0.6385812759399414, auc: 0.7888374008855936\n",
      "Epoch 159/300: loss: 0.637407660484314, auc: 0.7901554937699516\n",
      "Epoch 160/300: loss: 0.6362069845199585, auc: 0.7916795386674904\n",
      "Epoch 161/300: loss: 0.634979784488678, auc: 0.7927504891360313\n",
      "Epoch 162/300: loss: 0.63372403383255, auc: 0.7937596540006179\n",
      "Epoch 163/300: loss: 0.6324417591094971, auc: 0.7947688188652045\n",
      "Epoch 164/300: loss: 0.631132185459137, auc: 0.7956338173205644\n",
      "Epoch 165/300: loss: 0.6297944784164429, auc: 0.7963752445680157\n",
      "Epoch 166/300: loss: 0.6284269690513611, auc: 0.7975491710431469\n",
      "Epoch 167/300: loss: 0.6270301938056946, auc: 0.7981670270826898\n",
      "Epoch 168/300: loss: 0.6256037950515747, auc: 0.799320358356503\n",
      "Epoch 169/300: loss: 0.6241475939750671, auc: 0.8001235712079086\n",
      "Epoch 170/300: loss: 0.6226612329483032, auc: 0.800906188857996\n",
      "Epoch 171/300: loss: 0.6211441159248352, auc: 0.8022654721449902\n",
      "Epoch 172/300: loss: 0.6195963621139526, auc: 0.8027597569766245\n",
      "Epoch 173/300: loss: 0.6180179715156555, auc: 0.8035217794253938\n",
      "Epoch 174/300: loss: 0.6164077520370483, auc: 0.8046133250952529\n",
      "Epoch 175/300: loss: 0.6147661209106445, auc: 0.805087014725569\n",
      "Epoch 176/300: loss: 0.6130920648574829, auc: 0.8060343939862012\n",
      "Epoch 177/300: loss: 0.6113861203193665, auc: 0.8069611780455154\n",
      "Epoch 178/300: loss: 0.6096464395523071, auc: 0.807311296467923\n",
      "Epoch 179/300: loss: 0.6078740358352661, auc: 0.807743795695603\n",
      "Epoch 180/300: loss: 0.6060693860054016, auc: 0.8084234373390999\n",
      "Epoch 181/300: loss: 0.6042323708534241, auc: 0.8088765317680982\n",
      "Epoch 182/300: loss: 0.602364182472229, auc: 0.8095973638142313\n",
      "Epoch 183/300: loss: 0.6004638671875, auc: 0.8102358150550921\n",
      "Epoch 184/300: loss: 0.5985326170921326, auc: 0.8105241478735454\n",
      "Epoch 185/300: loss: 0.5965694785118103, auc: 0.8109154566985892\n",
      "Epoch 186/300: loss: 0.5945746302604675, auc: 0.8112655751209968\n",
      "Epoch 187/300: loss: 0.5925484299659729, auc: 0.8116774791473588\n",
      "Epoch 188/300: loss: 0.590490996837616, auc: 0.812192359180311\n",
      "Epoch 189/300: loss: 0.588402509689331, auc: 0.8122335495829472\n",
      "Epoch 190/300: loss: 0.5862829685211182, auc: 0.8127484296158995\n",
      "Epoch 191/300: loss: 0.5841341018676758, auc: 0.8128925960251262\n",
      "Epoch 192/300: loss: 0.5819553136825562, auc: 0.8132633096488519\n",
      "Epoch 193/300: loss: 0.5797472596168518, auc: 0.8134074760580785\n",
      "Epoch 194/300: loss: 0.5775074362754822, auc: 0.8135722376686233\n",
      "Epoch 195/300: loss: 0.5752357840538025, auc: 0.8134074760580785\n",
      "Epoch 196/300: loss: 0.5729337334632874, auc: 0.8133456904541242\n",
      "Epoch 197/300: loss: 0.5706021785736084, auc: 0.8130779528369889\n",
      "Epoch 198/300: loss: 0.5682414770126343, auc: 0.8128514056224899\n",
      "Epoch 199/300: loss: 0.5658499598503113, auc: 0.8132633096488519\n",
      "Epoch 200/300: loss: 0.5634276866912842, auc: 0.81371640407785\n",
      "Epoch 201/300: loss: 0.5609757304191589, auc: 0.8132427144475337\n",
      "Epoch 202/300: loss: 0.558495283126831, auc: 0.8132633096488519\n",
      "Epoch 203/300: loss: 0.5559860467910767, auc: 0.8133456904541242\n",
      "Epoch 204/300: loss: 0.553449273109436, auc: 0.8131397384409432\n",
      "Epoch 205/300: loss: 0.5508869290351868, auc: 0.8129749768303985\n",
      "Epoch 206/300: loss: 0.5482980608940125, auc: 0.8130367624343529\n",
      "Epoch 207/300: loss: 0.5456820726394653, auc: 0.81285140562249\n",
      "Epoch 208/300: loss: 0.5430384874343872, auc: 0.8123365255895376\n",
      "Epoch 209/300: loss: 0.5403696894645691, auc: 0.8117804551539491\n",
      "Epoch 210/300: loss: 0.5376750826835632, auc: 0.811286170322315\n",
      "Epoch 211/300: loss: 0.5349559187889099, auc: 0.811100813510452\n",
      "Epoch 212/300: loss: 0.5322121381759644, auc: 0.8108536710946349\n",
      "Epoch 213/300: loss: 0.5294446349143982, auc: 0.8102564102564103\n",
      "Epoch 214/300: loss: 0.5266532301902771, auc: 0.809741530223458\n",
      "Epoch 215/300: loss: 0.5238397717475891, auc: 0.8093708165997323\n",
      "Epoch 216/300: loss: 0.5210046768188477, auc: 0.8086293893522809\n",
      "Epoch 217/300: loss: 0.5181477069854736, auc: 0.8082380805272372\n",
      "Epoch 218/300: loss: 0.5152711272239685, auc: 0.8078055812995573\n",
      "Epoch 219/300: loss: 0.5123744010925293, auc: 0.8067140356296983\n",
      "Epoch 220/300: loss: 0.5094574093818665, auc: 0.8060961795901556\n",
      "Epoch 221/300: loss: 0.5065217614173889, auc: 0.8057254659664298\n",
      "Epoch 222/300: loss: 0.5035679936408997, auc: 0.8049428483163422\n",
      "Epoch 223/300: loss: 0.5005964636802673, auc: 0.8039542786530738\n",
      "Epoch 224/300: loss: 0.4976101517677307, auc: 0.8035217794253939\n",
      "Epoch 225/300: loss: 0.49460965394973755, auc: 0.8028833281845329\n",
      "Epoch 226/300: loss: 0.49159833788871765, auc: 0.8020801153331274\n",
      "Epoch 227/300: loss: 0.48857513070106506, auc: 0.801544640098857\n",
      "Epoch 228/300: loss: 0.4855406880378723, auc: 0.8006178560395428\n",
      "Epoch 229/300: loss: 0.4824965298175812, auc: 0.7997116671815467\n",
      "Epoch 230/300: loss: 0.4794408679008484, auc: 0.7988466687261868\n",
      "Epoch 231/300: loss: 0.4763743579387665, auc: 0.7979095870662136\n",
      "Epoch 232/300: loss: 0.47329676151275635, auc: 0.7970342910101947\n",
      "Epoch 233/300: loss: 0.47020867466926575, auc: 0.7959015549376995\n",
      "Epoch 234/300: loss: 0.467112272977829, auc: 0.7950159612810216\n",
      "Epoch 235/300: loss: 0.46400728821754456, auc: 0.794212748429616\n",
      "Epoch 236/300: loss: 0.46089574694633484, auc: 0.7932859643703016\n",
      "Epoch 237/300: loss: 0.45777764916419983, auc: 0.7921738234991247\n",
      "Epoch 238/300: loss: 0.454653799533844, auc: 0.7911646586345381\n",
      "Epoch 239/300: loss: 0.4515247344970703, auc: 0.7904026361857687\n",
      "Epoch 240/300: loss: 0.44839000701904297, auc: 0.7893316857172279\n",
      "Epoch 241/300: loss: 0.4452529549598694, auc: 0.788466687261868\n",
      "Epoch 242/300: loss: 0.44211456179618835, auc: 0.7876840696117804\n",
      "Epoch 243/300: loss: 0.43897560238838196, auc: 0.7866337143445578\n",
      "Epoch 244/300: loss: 0.4358372390270233, auc: 0.7852950262588817\n",
      "Epoch 245/300: loss: 0.4326974153518677, auc: 0.7841005045824323\n",
      "Epoch 246/300: loss: 0.4295573830604553, auc: 0.7832766965297087\n",
      "Epoch 247/300: loss: 0.42641913890838623, auc: 0.7821645556585316\n",
      "Epoch 248/300: loss: 0.42328211665153503, auc: 0.7810318195860365\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 249/300: loss: 0.42014560103416443, auc: 0.7800638451240862\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 250/300: loss: 0.41700810194015503, auc: 0.7787663474410462\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 251/300: loss: 0.41387009620666504, auc: 0.7776336113685511\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 252/300: loss: 0.4107333719730377, auc: 0.7763361136855113\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 253/300: loss: 0.4075965881347656, auc: 0.7754093296261971\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 254/300: loss: 0.40446141362190247, auc: 0.7744207599629287\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 255/300: loss: 0.4013242721557617, auc: 0.7731026670785708\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 256/300: loss: 0.3981867730617523, auc: 0.7718051693955309\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 257/300: loss: 0.39505043625831604, auc: 0.7705900525177634\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 258/300: loss: 0.39191609621047974, auc: 0.7696426732571312\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 259/300: loss: 0.3887803554534912, auc: 0.768715889197817\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 260/300: loss: 0.38564708828926086, auc: 0.7679126763464112\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 261/300: loss: 0.382525771856308, auc: 0.7670888682936876\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 262/300: loss: 0.37941455841064453, auc: 0.7659767274225106\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 263/300: loss: 0.3763170838356018, auc: 0.7646792297394707\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 264/300: loss: 0.3732345998287201, auc: 0.7634641128617032\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 265/300: loss: 0.3701647222042084, auc: 0.762537328802389\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 266/300: loss: 0.36710476875305176, auc: 0.7615899495417567\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 267/300: loss: 0.36405500769615173, auc: 0.7608485222943054\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Auc: 0.7763687600644124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from BWGNN_model import BWGNN_em\n",
    "\n",
    "data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.3,0.25], anomaly_type=\"syn\", anomaly_ratio=0.1).dataset\n",
    "# data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.3,0.25], anomaly_type=\"min\").dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "number_class = 2\n",
    "hid_dim = 64\n",
    "number_class = 2\n",
    "BWGNN_model = BWGNN_em(data.x.shape[1], 64, number_class, dgl_data)\n",
    "\n",
    "optimizer = Adam(BWGNN_model.parameters(), lr = 1e-4)\n",
    "epochs = 300\n",
    "early_stop = EarlyStopping(patience=20)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    BWGNN_model.train()\n",
    "    logits, BW_hid = BWGNN_model(data.x)\n",
    "    train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, 10.0]))\n",
    "    BWGNN_model.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    BWGNN_model.eval()\n",
    "    logits, BW_hid= BWGNN_model(data.x)\n",
    "    val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, 10.0]))\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "\n",
    "    early_stop(val_loss, BWGNN_model)\n",
    "    if early_stop.early_stop == True:\n",
    "        print (\"Early stopping\")\n",
    "        break\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: loss: {train_loss}, auc: {auc}\")\n",
    "BWGNN_model.eval()\n",
    "logits, BW_hid= BWGNN_model(data.x)\n",
    "probs = logits.softmax(1)\n",
    "auc = roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "print (f\"Auc: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两层 GAT，获取其表征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly syntheic is on processing\n",
      "using 0.08 seconds\n",
      "Epoch 1/300: loss: 0.6930190920829773, auc: 0.6255411255411256\n",
      "Epoch 2/300: loss: 0.6923465728759766, auc: 0.680952380952381\n",
      "Epoch 3/300: loss: 0.6916289329528809, auc: 0.7054951850870219\n",
      "Epoch 4/300: loss: 0.6911395788192749, auc: 0.7191801395883028\n",
      "Epoch 5/300: loss: 0.690441906452179, auc: 0.729039667815178\n",
      "Epoch 6/300: loss: 0.6896844506263733, auc: 0.73551550490326\n",
      "Epoch 7/300: loss: 0.6889178156852722, auc: 0.7387401713932324\n",
      "Epoch 8/300: loss: 0.6880950927734375, auc: 0.7414170863150455\n",
      "Epoch 9/300: loss: 0.6874065399169922, auc: 0.7438731336690521\n",
      "Epoch 10/300: loss: 0.6862128973007202, auc: 0.7452866861030126\n",
      "Epoch 11/300: loss: 0.6855784058570862, auc: 0.7462938422122096\n",
      "Epoch 12/300: loss: 0.6845324039459229, auc: 0.7469122714020675\n",
      "Epoch 13/300: loss: 0.684036910533905, auc: 0.7471419736725858\n",
      "Epoch 14/300: loss: 0.682813286781311, auc: 0.7476720558353211\n",
      "Epoch 15/300: loss: 0.6814088225364685, auc: 0.7478664192949906\n",
      "Epoch 16/300: loss: 0.6806892156600952, auc: 0.7481314603763582\n",
      "Epoch 17/300: loss: 0.6799058318138123, auc: 0.748573195511971\n",
      "Epoch 18/300: loss: 0.6783350110054016, auc: 0.748749889566216\n",
      "Epoch 19/300: loss: 0.6773656010627747, auc: 0.7491386164855552\n",
      "Epoch 20/300: loss: 0.6759372353553772, auc: 0.7490502694584329\n",
      "Epoch 21/300: loss: 0.6746343374252319, auc: 0.7485201872956975\n",
      "Epoch 22/300: loss: 0.6730208396911621, auc: 0.7485555261065464\n",
      "Epoch 23/300: loss: 0.6723797917366028, auc: 0.748467179079424\n",
      "Epoch 24/300: loss: 0.671117901802063, auc: 0.7488559059987632\n",
      "Epoch 25/300: loss: 0.6697293519973755, auc: 0.7486262037282445\n",
      "Epoch 26/300: loss: 0.6680763959884644, auc: 0.7485378567011219\n",
      "Epoch 27/300: loss: 0.6669040322303772, auc: 0.7485731955119711\n",
      "Epoch 28/300: loss: 0.6651526093482971, auc: 0.7482728156197545\n",
      "Epoch 29/300: loss: 0.6636767983436584, auc: 0.7483788320523015\n",
      "Epoch 30/300: loss: 0.6622686386108398, auc: 0.7480784521600847\n",
      "Epoch 31/300: loss: 0.6611220240592957, auc: 0.7477957416732925\n",
      "Epoch 32/300: loss: 0.6591197848320007, auc: 0.747601378213623\n",
      "Epoch 33/300: loss: 0.657880425453186, auc: 0.7477073946461701\n",
      "Epoch 34/300: loss: 0.6562241911888123, auc: 0.7476190476190475\n",
      "Epoch 35/300: loss: 0.6551492810249329, auc: 0.7473716759431045\n",
      "Epoch 36/300: loss: 0.6530749201774597, auc: 0.7468769325912183\n",
      "Epoch 37/300: loss: 0.6513475179672241, auc: 0.7467179079423978\n",
      "Epoch 38/300: loss: 0.6489876508712769, auc: 0.7468592631857938\n",
      "Epoch 39/300: loss: 0.6481548547744751, auc: 0.7466825691315486\n",
      "Epoch 40/300: loss: 0.6463913917541504, auc: 0.7465588832935771\n",
      "Epoch 41/300: loss: 0.6443049311637878, auc: 0.7462054951850872\n",
      "Epoch 42/300: loss: 0.6419411897659302, auc: 0.7460288011308419\n",
      "Epoch 43/300: loss: 0.6407523155212402, auc: 0.7457107518332009\n",
      "Epoch 44/300: loss: 0.6386325359344482, auc: 0.7452513472921636\n",
      "Epoch 45/300: loss: 0.6380699276924133, auc: 0.7450569838324941\n",
      "Epoch 46/300: loss: 0.6356416344642639, auc: 0.7445799098860324\n",
      "Epoch 47/300: loss: 0.6320264339447021, auc: 0.744155844155844\n",
      "Epoch 48/300: loss: 0.6312928795814514, auc: 0.7440498277232971\n",
      "Epoch 49/300: loss: 0.6290233731269836, auc: 0.744014488912448\n",
      "Epoch 50/300: loss: 0.6258529424667358, auc: 0.744155844155844\n",
      "Epoch 51/300: loss: 0.6253867745399475, auc: 0.7440851665341461\n",
      "Epoch 52/300: loss: 0.6233693361282349, auc: 0.7440674971287216\n",
      "Epoch 53/300: loss: 0.6206021904945374, auc: 0.7439261418853256\n",
      "Epoch 54/300: loss: 0.6201527118682861, auc: 0.7438554642636276\n",
      "Epoch 55/300: loss: 0.6171501874923706, auc: 0.7438024560473541\n",
      "Epoch 56/300: loss: 0.6143662929534912, auc: 0.7437141090202315\n",
      "Epoch 57/300: loss: 0.6140579581260681, auc: 0.7436964396148069\n",
      "Epoch 58/300: loss: 0.6102825999259949, auc: 0.7436964396148069\n",
      "Epoch 59/300: loss: 0.6065794229507446, auc: 0.743802456047354\n",
      "Epoch 60/300: loss: 0.6056587100028992, auc: 0.7436787702093824\n",
      "Epoch 61/300: loss: 0.6022721529006958, auc: 0.7437494478310804\n",
      "Epoch 62/300: loss: 0.6000188589096069, auc: 0.7437141090202315\n",
      "Epoch 63/300: loss: 0.5986719131469727, auc: 0.743837794858203\n",
      "Epoch 64/300: loss: 0.5958698391914368, auc: 0.743802456047354\n",
      "Epoch 65/300: loss: 0.5927953124046326, auc: 0.7438554642636275\n",
      "Epoch 66/300: loss: 0.5898914337158203, auc: 0.7438731336690522\n",
      "Epoch 67/300: loss: 0.5880499482154846, auc: 0.7439614806961746\n",
      "Epoch 68/300: loss: 0.587751567363739, auc: 0.7438642989663397\n",
      "Epoch 69/300: loss: 0.5839194059371948, auc: 0.7438201254527785\n",
      "Epoch 70/300: loss: 0.5820345282554626, auc: 0.7439261418853256\n",
      "Epoch 71/300: loss: 0.5793848633766174, auc: 0.7440321583178726\n",
      "Epoch 72/300: loss: 0.5776073932647705, auc: 0.7441735135612686\n",
      "Epoch 73/300: loss: 0.5750411152839661, auc: 0.7444208852372117\n",
      "Epoch 74/300: loss: 0.5730869770050049, auc: 0.7443855464263627\n",
      "Epoch 75/300: loss: 0.5708670616149902, auc: 0.7444562240480608\n",
      "Epoch 76/300: loss: 0.5682804584503174, auc: 0.7443678770209382\n",
      "Epoch 77/300: loss: 0.5648173093795776, auc: 0.7444738934534854\n",
      "Epoch 78/300: loss: 0.5621539950370789, auc: 0.7445622404806078\n",
      "Epoch 79/300: loss: 0.5603573322296143, auc: 0.7445269016697589\n",
      "Epoch 80/300: loss: 0.5561749935150146, auc: 0.7446152486968814\n",
      "Epoch 81/300: loss: 0.5559780597686768, auc: 0.7447389345348527\n",
      "Epoch 82/300: loss: 0.552431583404541, auc: 0.7448979591836735\n",
      "Epoch 83/300: loss: 0.5493655204772949, auc: 0.745021645021645\n",
      "Epoch 84/300: loss: 0.5505591630935669, auc: 0.7450039756162204\n",
      "Epoch 85/300: loss: 0.5450475215911865, auc: 0.7450216450216449\n",
      "Epoch 86/300: loss: 0.5421479344367981, auc: 0.7450039756162206\n",
      "Epoch 87/300: loss: 0.540468156337738, auc: 0.7451099920487676\n",
      "Epoch 88/300: loss: 0.5364289879798889, auc: 0.7451806696704656\n",
      "Epoch 89/300: loss: 0.534278392791748, auc: 0.7452690166975882\n",
      "Epoch 90/300: loss: 0.5302850604057312, auc: 0.7453573637247106\n",
      "Epoch 91/300: loss: 0.527696967124939, auc: 0.7454280413464086\n",
      "Epoch 92/300: loss: 0.5287835597991943, auc: 0.7457284212386254\n",
      "Epoch 93/300: loss: 0.5248222947120667, auc: 0.7458874458874457\n",
      "Epoch 94/300: loss: 0.5212804675102234, auc: 0.7460464705362665\n",
      "Epoch 95/300: loss: 0.520513117313385, auc: 0.7461348175633888\n",
      "Epoch 96/300: loss: 0.5161835551261902, auc: 0.7461524869688134\n",
      "Epoch 97/300: loss: 0.5130747556686401, auc: 0.7462231645905114\n",
      "Epoch 98/300: loss: 0.5133597254753113, auc: 0.7463821892393321\n",
      "Epoch 99/300: loss: 0.5079924464225769, auc: 0.7466118915098507\n",
      "Epoch 100/300: loss: 0.5062246322631836, auc: 0.7468062549695202\n",
      "Epoch 101/300: loss: 0.5036818981170654, auc: 0.7470182878346143\n",
      "Epoch 102/300: loss: 0.502230167388916, auc: 0.7471419736725858\n",
      "Epoch 103/300: loss: 0.496965616941452, auc: 0.7472656595105575\n",
      "Epoch 104/300: loss: 0.4964645802974701, auc: 0.7474246841593781\n",
      "Epoch 105/300: loss: 0.49120965600013733, auc: 0.7477250640515947\n",
      "Epoch 106/300: loss: 0.49400171637535095, auc: 0.7478134110787171\n",
      "Epoch 107/300: loss: 0.48847246170043945, auc: 0.7482021379980562\n",
      "Epoch 108/300: loss: 0.4854966700077057, auc: 0.7484848484848485\n",
      "Epoch 109/300: loss: 0.4878586232662201, auc: 0.7486792119445179\n",
      "Epoch 110/300: loss: 0.4809446930885315, auc: 0.7488382365933386\n",
      "Epoch 111/300: loss: 0.4764974117279053, auc: 0.7491032776747061\n",
      "Epoch 112/300: loss: 0.47741425037384033, auc: 0.7492092941072531\n",
      "Epoch 113/300: loss: 0.4736383557319641, auc: 0.7494566657831964\n",
      "Epoch 114/300: loss: 0.4729905426502228, auc: 0.7495096739994699\n",
      "Epoch 115/300: loss: 0.46711480617523193, auc: 0.749615690432017\n",
      "Epoch 116/300: loss: 0.46808168292045593, auc: 0.7497570456754131\n",
      "Epoch 117/300: loss: 0.46621719002723694, auc: 0.7501281031893277\n",
      "Epoch 118/300: loss: 0.46108314394950867, auc: 0.7503578054598462\n",
      "Epoch 119/300: loss: 0.46071213483810425, auc: 0.7506405159466383\n",
      "Epoch 120/300: loss: 0.45890986919403076, auc: 0.7510999204876756\n",
      "Epoch 121/300: loss: 0.4549638032913208, auc: 0.7511705981093737\n",
      "Epoch 122/300: loss: 0.45346909761428833, auc: 0.7515593250287128\n",
      "Epoch 123/300: loss: 0.45135223865509033, auc: 0.7516830108666842\n",
      "Epoch 124/300: loss: 0.44641295075416565, auc: 0.7517183496775334\n",
      "Epoch 125/300: loss: 0.4437958002090454, auc: 0.7519833907589009\n",
      "Epoch 126/300: loss: 0.4428689181804657, auc: 0.751983390758901\n",
      "Epoch 127/300: loss: 0.4421742856502533, auc: 0.7521777542185706\n",
      "Epoch 128/300: loss: 0.43702444434165955, auc: 0.7524251258945137\n",
      "Epoch 129/300: loss: 0.436171293258667, auc: 0.7525311423270606\n",
      "Epoch 130/300: loss: 0.4338628053665161, auc: 0.7526724975704567\n",
      "Epoch 131/300: loss: 0.4294922649860382, auc: 0.7528491916247019\n",
      "Epoch 132/300: loss: 0.43401914834976196, auc: 0.753202579733192\n",
      "Epoch 133/300: loss: 0.4236627519130707, auc: 0.7534322820037107\n",
      "Epoch 134/300: loss: 0.4274688959121704, auc: 0.7539093559501723\n",
      "Epoch 135/300: loss: 0.4213307201862335, auc: 0.7538386783284742\n",
      "Epoch 136/300: loss: 0.41757363080978394, auc: 0.7539800335718703\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 137/300: loss: 0.41638779640197754, auc: 0.7540860500044174\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 138/300: loss: 0.41540244221687317, auc: 0.7542274052478135\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 139/300: loss: 0.4153117537498474, auc: 0.7544394381129074\n",
      "Epoch 140/300: loss: 0.41311225295066833, auc: 0.7549165120593692\n",
      "Epoch 141/300: loss: 0.40860074758529663, auc: 0.7551108755190389\n",
      "Epoch 142/300: loss: 0.41042330861091614, auc: 0.7552168919515858\n",
      "Epoch 143/300: loss: 0.40436455607414246, auc: 0.7555879494655005\n",
      "Epoch 144/300: loss: 0.4038582742214203, auc: 0.755852990546868\n",
      "Epoch 145/300: loss: 0.3998177945613861, auc: 0.7561003622228112\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 146/300: loss: 0.3944350779056549, auc: 0.7564184115204523\n",
      "Epoch 147/300: loss: 0.3961695432662964, auc: 0.7565244279529995\n",
      "Epoch 148/300: loss: 0.39534878730773926, auc: 0.7564714197367258\n",
      "Epoch 149/300: loss: 0.39339688420295715, auc: 0.7567717996289425\n",
      "Epoch 150/300: loss: 0.3909951150417328, auc: 0.7571781959537062\n",
      "Epoch 151/300: loss: 0.3876781761646271, auc: 0.7574078982242247\n",
      "Epoch 152/300: loss: 0.38996121287345886, auc: 0.757831963954413\n",
      "Epoch 153/300: loss: 0.38307249546051025, auc: 0.7582736990900255\n",
      "Epoch 154/300: loss: 0.3817324936389923, auc: 0.7584857319551197\n",
      "Epoch 155/300: loss: 0.37983423471450806, auc: 0.7586977648202138\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 156/300: loss: 0.37841346859931946, auc: 0.7589097976853078\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 157/300: loss: 0.37601232528686523, auc: 0.7592455163883735\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 158/300: loss: 0.3739970624446869, auc: 0.7597225903348352\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 159/300: loss: 0.37379634380340576, auc: 0.7598639455782313\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 160/300: loss: 0.36669301986694336, auc: 0.7599346231999293\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 161/300: loss: 0.36524489521980286, auc: 0.7600406396324764\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 162/300: loss: 0.36772674322128296, auc: 0.7603586889301175\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 163/300: loss: 0.3625677824020386, auc: 0.7605530523897871\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 164/300: loss: 0.3603628873825073, auc: 0.7607650852548812\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 165/300: loss: 0.35996854305267334, auc: 0.7611361427687958\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 166/300: loss: 0.357974112033844, auc: 0.7614188532555879\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 167/300: loss: 0.35640501976013184, auc: 0.7616308861206821\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 168/300: loss: 0.3518032431602478, auc: 0.7619489354183231\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 169/300: loss: 0.35251039266586304, auc: 0.7621609682834173\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 170/300: loss: 0.3507187068462372, auc: 0.7625320257973319\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 171/300: loss: 0.352195680141449, auc: 0.7626557116353034\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 172/300: loss: 0.34976208209991455, auc: 0.7629384221220955\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 173/300: loss: 0.34349218010902405, auc: 0.7631857937980386\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 174/300: loss: 0.34406137466430664, auc: 0.7631504549871896\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Auc: 0.7712921065862243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.nn.conv import GATConv, GCNConv, MessagePassing\n",
    "\n",
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels=in_dim, out_channels=hid_dim)\n",
    "        self.conv2 = GCNConv(in_channels=hid_dim, out_channels=out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        h = F.dropout(F.relu(x), 0.3, self.training)\n",
    "        x = self.conv2(h, edge_index)\n",
    "        return x, h\n",
    "\n",
    "data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.3,0.25], anomaly_type=\"syn\", anomaly_ratio=0.1).dataset\n",
    "# data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.3,0.25], anomaly_type=\"min\").dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "number_class = 2\n",
    "hid_dim = 64\n",
    "number_class = 2\n",
    "edge_index = data.edge_index\n",
    "GAT_model = GAT(data.x.shape[1], 64, number_class)\n",
    "\n",
    "optimizer = Adam(GAT_model.parameters(), lr = 1e-3)\n",
    "epochs = 300\n",
    "early_stop = EarlyStopping(patience=20)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    GAT_model.train()\n",
    "    logits, GAT_hid = GAT_model(data.x, edge_index)\n",
    "    train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, 10.0]))\n",
    "    GAT_model.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    GAT_model.eval()\n",
    "    logits, GAT_hid= GAT_model(data.x, edge_index)\n",
    "    val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, 10.0]))\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "\n",
    "    early_stop(val_loss, GAT_model)\n",
    "    if early_stop.early_stop == True:\n",
    "        print (\"Early stopping\")\n",
    "        break\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: loss: {train_loss}, auc: {auc}\")\n",
    "GAT_model.eval()\n",
    "logits, GAT_hid= GAT_model(data.x, edge_index)\n",
    "probs = logits.softmax(1)\n",
    "auc = roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "print (f\"Auc: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 消除冗余特征函数\n",
    "\n",
    "### 消除不同任务表征量纲的影响，归一化融合的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x1, x2=None, eps=1e-8):\n",
    "    x2 = x1 if x2 is None else x2\n",
    "    w1 = x1.norm(p=2, dim=1, keepdim=True)\n",
    "    w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\n",
    "    # .clamp(min=eps)\n",
    "    # 0-1\n",
    "    return (torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)).absolute()\n",
    "\n",
    "# 越无关，权重越大\n",
    "def zero2one(feature):\n",
    "    \"\"\"Input: feature must be a 1d numpy array\n",
    "    \"\"\"\n",
    "    # feature = np.array(feature)\n",
    "    min = feature.min()\n",
    "    max = feature.max()\n",
    "    return (feature - min)/(max-min)\n",
    "\n",
    "def feature_normalize(feature, axis=1, eps=1e-10):\n",
    "    \"\"\"2D array feature to row normalize\"\"\"\n",
    "    mean = None\n",
    "    std = None\n",
    "    if axis == 1:\n",
    "        mean = feature.mean(axis=axis).reshape(-1,1)\n",
    "        std = feature.std(axis=axis).reshape(-1,1)\n",
    "    elif axis == 0:\n",
    "        mean = feature.mean(axis=axis).reshape(1,-1)\n",
    "        std = feature.std(axis=axis).reshape(1,-1)\n",
    "    return (feature - mean) / (std + eps)\n",
    "\n",
    "# hiddle = hid_dominate.detach()\n",
    "# hiddle = BW_hid.detach()\n",
    "# hiddle = GAT_hid.detach()\n",
    "# 融合的特征未曾归一化\n",
    "\n",
    "# 补充信息最好策略\n",
    "hiddle = torch.concat((1 * feature_normalize(GAT_hid.detach(),axis=0), 1 * feature_normalize(BW_hid.detach(),axis=0), 0.2 * feature_normalize(hid_dominate.detach(),axis=0)), axis=1)\n",
    "\n",
    "# 可学习化的参数\n",
    "# l_weight = [nn.Parameter(torch.randn([hiddle.shape[-1]], dtype=torch.float32, requires_grad=True))]\n",
    "# hiddle = torch.mul(hiddle, torch.softmax(*l_weight, dim = 0))\n",
    "# optimizer_ = Adam(l_weight, lr = 1e-3)\n",
    "\n",
    "# 计算hiddle特征之间的相似度，放缩到0-1之间 （专家系统）\n",
    "# hiddle = zero2one((1-cosine_distance(hiddle.T)).mean(axis=0))*hiddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签导向，检测融合的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000: loss: 0.7500657439231873, auc: 0.46432712375357504\n",
      "Epoch 2/2000: loss: 0.7401655912399292, auc: 0.5405941614490738\n",
      "Epoch 3/2000: loss: 0.7304065227508545, auc: 0.6193347246914536\n",
      "Epoch 4/2000: loss: 0.7207922339439392, auc: 0.6925098554533509\n",
      "Epoch 5/2000: loss: 0.7113261818885803, auc: 0.7547860142743037\n",
      "Epoch 6/2000: loss: 0.702012300491333, auc: 0.8108525933369405\n",
      "Epoch 7/2000: loss: 0.6928532719612122, auc: 0.8542681198629254\n",
      "Epoch 8/2000: loss: 0.6838525533676147, auc: 0.887763778310273\n",
      "Epoch 9/2000: loss: 0.6750122904777527, auc: 0.9125505655613099\n",
      "Epoch 10/2000: loss: 0.6663348078727722, auc: 0.9306897529051041\n",
      "Epoch 11/2000: loss: 0.6578222513198853, auc: 0.942207106232769\n",
      "Epoch 12/2000: loss: 0.6494759917259216, auc: 0.9509675092113061\n",
      "Epoch 13/2000: loss: 0.6412971019744873, auc: 0.9567133029295818\n",
      "Epoch 14/2000: loss: 0.6332863569259644, auc: 0.9607327819432636\n",
      "Epoch 15/2000: loss: 0.6254439353942871, auc: 0.9632836051634845\n",
      "Epoch 16/2000: loss: 0.6177700161933899, auc: 0.9650099198680786\n",
      "Epoch 17/2000: loss: 0.6102643609046936, auc: 0.9662724485326325\n",
      "Epoch 18/2000: loss: 0.6029260158538818, auc: 0.9670196593749194\n",
      "Epoch 19/2000: loss: 0.5957545638084412, auc: 0.9677411043260931\n",
      "Epoch 20/2000: loss: 0.5887486338615417, auc: 0.9680502950194532\n",
      "Epoch 21/2000: loss: 0.581906795501709, auc: 0.9683852516039266\n",
      "Epoch 22/2000: loss: 0.5752272605895996, auc: 0.96810182680168\n",
      "Epoch 23/2000: loss: 0.5687080025672913, auc: 0.96810182680168\n",
      "Epoch 24/2000: loss: 0.5623467564582825, auc: 0.9680245291283399\n",
      "Epoch 25/2000: loss: 0.5561410784721375, auc: 0.9676380407616396\n",
      "Epoch 26/2000: loss: 0.5500882267951965, auc: 0.9677153384349798\n",
      "Epoch 27/2000: loss: 0.5441855192184448, auc: 0.9677153384349798\n",
      "Epoch 28/2000: loss: 0.5384295582771301, auc: 0.9675092113060729\n",
      "Epoch 29/2000: loss: 0.5328174829483032, auc: 0.9675349771971864\n",
      "Epoch 30/2000: loss: 0.5273460745811462, auc: 0.9675349771971865\n",
      "Epoch 31/2000: loss: 0.522011935710907, auc: 0.9673803818505063\n",
      "Epoch 32/2000: loss: 0.5168116688728333, auc: 0.9674061477416198\n",
      "Epoch 33/2000: loss: 0.5117418169975281, auc: 0.9672773182860529\n",
      "Epoch 34/2000: loss: 0.506799042224884, auc: 0.9671484888304862\n",
      "Epoch 35/2000: loss: 0.5019799470901489, auc: 0.9672000206127128\n",
      "Epoch 36/2000: loss: 0.4972807765007019, auc: 0.9670454252660329\n",
      "Epoch 37/2000: loss: 0.49269843101501465, auc: 0.9670711911571461\n",
      "Epoch 38/2000: loss: 0.48822933435440063, auc: 0.9669681275926928\n",
      "Epoch 39/2000: loss: 0.48387008905410767, auc: 0.9668650640282395\n",
      "Epoch 40/2000: loss: 0.4796173572540283, auc: 0.9669165958104661\n",
      "Epoch 41/2000: loss: 0.47546783089637756, auc: 0.9668135322460126\n",
      "Epoch 42/2000: loss: 0.47141826152801514, auc: 0.9667362345726727\n",
      "Epoch 43/2000: loss: 0.4674655497074127, auc: 0.9667877663548994\n",
      "Epoch 44/2000: loss: 0.4636063873767853, auc: 0.9667877663548994\n",
      "Epoch 45/2000: loss: 0.45983779430389404, auc: 0.966762000463786\n",
      "Epoch 46/2000: loss: 0.45615679025650024, auc: 0.9667362345726727\n",
      "Epoch 47/2000: loss: 0.4525603950023651, auc: 0.9666589368993327\n",
      "Epoch 48/2000: loss: 0.4490458369255066, auc: 0.9666074051171061\n",
      "Epoch 49/2000: loss: 0.4456102252006531, auc: 0.9664270438793126\n",
      "Epoch 50/2000: loss: 0.4422508478164673, auc: 0.9664012779881992\n",
      "Epoch 51/2000: loss: 0.4389651417732239, auc: 0.9663497462059726\n",
      "Epoch 52/2000: loss: 0.4357506036758423, auc: 0.9662466826415193\n",
      "Epoch 53/2000: loss: 0.4326046407222748, auc: 0.9661693849681792\n",
      "Epoch 54/2000: loss: 0.4295249581336975, auc: 0.9661693849681791\n",
      "Epoch 55/2000: loss: 0.42650917172431946, auc: 0.9660663214037257\n",
      "Epoch 56/2000: loss: 0.4235551059246063, auc: 0.9660405555126124\n",
      "Epoch 57/2000: loss: 0.4206605553627014, auc: 0.9660663214037258\n",
      "Epoch 58/2000: loss: 0.4178233742713928, auc: 0.9659890237303858\n",
      "Epoch 59/2000: loss: 0.4150417447090149, auc: 0.965937491948159\n",
      "Epoch 60/2000: loss: 0.4123136103153229, auc: 0.9658859601659323\n",
      "Epoch 61/2000: loss: 0.40963706374168396, auc: 0.9658344283837057\n",
      "Epoch 62/2000: loss: 0.4070104658603668, auc: 0.9658086624925923\n",
      "Epoch 63/2000: loss: 0.4044319987297058, auc: 0.9657313648192523\n",
      "Epoch 64/2000: loss: 0.4019000232219696, auc: 0.9657313648192523\n",
      "Epoch 65/2000: loss: 0.3994128704071045, auc: 0.9657313648192523\n",
      "Epoch 66/2000: loss: 0.39696913957595825, auc: 0.9657313648192523\n",
      "Epoch 67/2000: loss: 0.3945674002170563, auc: 0.9656798330370255\n",
      "Epoch 68/2000: loss: 0.3922061026096344, auc: 0.9656283012547989\n",
      "Epoch 69/2000: loss: 0.38988396525382996, auc: 0.9656025353636856\n",
      "Epoch 70/2000: loss: 0.38759973645210266, auc: 0.9655767694725722\n",
      "Epoch 71/2000: loss: 0.38535213470458984, auc: 0.9656025353636857\n",
      "Epoch 72/2000: loss: 0.383139967918396, auc: 0.9655510035814588\n",
      "Epoch 73/2000: loss: 0.38096216320991516, auc: 0.9655510035814588\n",
      "Epoch 74/2000: loss: 0.378817617893219, auc: 0.9655252376903455\n",
      "Epoch 75/2000: loss: 0.37670519948005676, auc: 0.9654994717992321\n",
      "Epoch 76/2000: loss: 0.3746240735054016, auc: 0.9654479400170055\n",
      "Epoch 77/2000: loss: 0.3725730776786804, auc: 0.9654479400170055\n",
      "Epoch 78/2000: loss: 0.37055152654647827, auc: 0.9654221741258922\n",
      "Epoch 79/2000: loss: 0.3685583472251892, auc: 0.9654221741258922\n",
      "Epoch 80/2000: loss: 0.36659279465675354, auc: 0.9654221741258922\n",
      "Epoch 81/2000: loss: 0.3646540939807892, auc: 0.9654479400170055\n",
      "Epoch 82/2000: loss: 0.3627414405345917, auc: 0.9654221741258922\n",
      "Epoch 83/2000: loss: 0.3608541488647461, auc: 0.9654221741258922\n",
      "Epoch 84/2000: loss: 0.3589913547039032, auc: 0.9653706423436654\n",
      "Epoch 85/2000: loss: 0.3571526110172272, auc: 0.9653706423436654\n",
      "Epoch 86/2000: loss: 0.3553370535373688, auc: 0.9653191105614388\n",
      "Epoch 87/2000: loss: 0.3535442352294922, auc: 0.9652933446703253\n",
      "Epoch 88/2000: loss: 0.3517734706401825, auc: 0.9652675787792121\n",
      "Epoch 89/2000: loss: 0.3500242233276367, auc: 0.965190281105872\n",
      "Epoch 90/2000: loss: 0.3482958674430847, auc: 0.965190281105872\n",
      "Epoch 91/2000: loss: 0.34658804535865784, auc: 0.9651387493236453\n",
      "Epoch 92/2000: loss: 0.34490007162094116, auc: 0.965112983432532\n",
      "Epoch 93/2000: loss: 0.3432316482067108, auc: 0.9651129834325319\n",
      "Epoch 94/2000: loss: 0.34158214926719666, auc: 0.9651129834325319\n",
      "Epoch 95/2000: loss: 0.33995121717453003, auc: 0.965112983432532\n",
      "Epoch 96/2000: loss: 0.33833837509155273, auc: 0.9651129834325319\n",
      "Epoch 97/2000: loss: 0.3367433249950409, auc: 0.9651129834325319\n",
      "Epoch 98/2000: loss: 0.33516550064086914, auc: 0.9650872175414186\n",
      "Epoch 99/2000: loss: 0.3336046636104584, auc: 0.9650872175414187\n",
      "Epoch 100/2000: loss: 0.3320603668689728, auc: 0.9650872175414187\n",
      "Epoch 101/2000: loss: 0.3305323123931885, auc: 0.9650872175414186\n",
      "Epoch 102/2000: loss: 0.32902011275291443, auc: 0.9650872175414186\n",
      "Epoch 103/2000: loss: 0.32752346992492676, auc: 0.9650872175414187\n",
      "Epoch 104/2000: loss: 0.3260420858860016, auc: 0.9650872175414186\n",
      "Epoch 105/2000: loss: 0.3245756924152374, auc: 0.9650614516503052\n",
      "Epoch 106/2000: loss: 0.32312384247779846, auc: 0.9650356857591919\n",
      "Epoch 107/2000: loss: 0.32168641686439514, auc: 0.965035685759192\n",
      "Epoch 108/2000: loss: 0.3202630579471588, auc: 0.9649841539769652\n",
      "Epoch 109/2000: loss: 0.31885355710983276, auc: 0.9649841539769652\n",
      "Epoch 110/2000: loss: 0.3174575865268707, auc: 0.9649841539769652\n",
      "Epoch 111/2000: loss: 0.3160749673843384, auc: 0.9649841539769652\n",
      "Epoch 112/2000: loss: 0.314705491065979, auc: 0.964958388085852\n",
      "Epoch 113/2000: loss: 0.3133488595485687, auc: 0.9649326221947386\n",
      "Epoch 114/2000: loss: 0.3120048940181732, auc: 0.9649068563036253\n",
      "Epoch 115/2000: loss: 0.3106732964515686, auc: 0.964881090412512\n",
      "Epoch 116/2000: loss: 0.30935394763946533, auc: 0.964881090412512\n",
      "Epoch 117/2000: loss: 0.3080466091632843, auc: 0.9649068563036252\n",
      "Epoch 118/2000: loss: 0.3067510724067688, auc: 0.9649068563036253\n",
      "Epoch 119/2000: loss: 0.3054671883583069, auc: 0.964881090412512\n",
      "Epoch 120/2000: loss: 0.30419474840164185, auc: 0.9648810904125119\n",
      "Epoch 121/2000: loss: 0.30293354392051697, auc: 0.9648553245213985\n",
      "Epoch 122/2000: loss: 0.3016834855079651, auc: 0.9648553245213987\n",
      "Epoch 123/2000: loss: 0.3004443049430847, auc: 0.9648553245213985\n",
      "Epoch 124/2000: loss: 0.2992159426212311, auc: 0.9648810904125118\n",
      "Epoch 125/2000: loss: 0.2979981005191803, auc: 0.9648810904125118\n",
      "Epoch 126/2000: loss: 0.2967907786369324, auc: 0.9648810904125119\n",
      "Epoch 127/2000: loss: 0.2955936789512634, auc: 0.9648810904125119\n",
      "Epoch 128/2000: loss: 0.29440680146217346, auc: 0.9648810904125118\n",
      "Epoch 129/2000: loss: 0.293229877948761, auc: 0.9648810904125118\n",
      "Epoch 130/2000: loss: 0.29206275939941406, auc: 0.9648810904125118\n",
      "Epoch 131/2000: loss: 0.2909054458141327, auc: 0.9648810904125118\n",
      "Epoch 132/2000: loss: 0.28975772857666016, auc: 0.9648810904125118\n",
      "Epoch 133/2000: loss: 0.28861942887306213, auc: 0.9648810904125119\n",
      "Epoch 134/2000: loss: 0.28749048709869385, auc: 0.9648295586302852\n",
      "Epoch 135/2000: loss: 0.28637072443962097, auc: 0.9648295586302852\n",
      "Epoch 136/2000: loss: 0.28526002168655396, auc: 0.9648295586302851\n",
      "Epoch 137/2000: loss: 0.2841583490371704, auc: 0.9648295586302853\n",
      "Epoch 138/2000: loss: 0.2830654978752136, auc: 0.9648295586302852\n",
      "Epoch 139/2000: loss: 0.2819814085960388, auc: 0.9648295586302851\n",
      "Epoch 140/2000: loss: 0.28090593218803406, auc: 0.9648295586302851\n",
      "Epoch 141/2000: loss: 0.27983900904655457, auc: 0.9648553245213987\n",
      "Epoch 142/2000: loss: 0.27878043055534363, auc: 0.9648553245213987\n",
      "Epoch 143/2000: loss: 0.27773022651672363, auc: 0.9648553245213987\n",
      "Epoch 144/2000: loss: 0.27668821811676025, auc: 0.9648295586302853\n",
      "Epoch 145/2000: loss: 0.27565431594848633, auc: 0.9648295586302851\n",
      "Epoch 146/2000: loss: 0.2746284008026123, auc: 0.9648295586302853\n",
      "Epoch 147/2000: loss: 0.2736104428768158, auc: 0.9648295586302851\n",
      "Epoch 148/2000: loss: 0.27260029315948486, auc: 0.9648295586302853\n",
      "Epoch 149/2000: loss: 0.27159783244132996, auc: 0.9648295586302853\n",
      "Epoch 150/2000: loss: 0.2706030607223511, auc: 0.9648295586302853\n",
      "Epoch 151/2000: loss: 0.26961585879325867, auc: 0.9648295586302852\n",
      "Epoch 152/2000: loss: 0.2686360776424408, auc: 0.9648295586302853\n",
      "Epoch 153/2000: loss: 0.26766374707221985, auc: 0.9648295586302853\n",
      "Epoch 154/2000: loss: 0.2666986286640167, auc: 0.9648295586302853\n",
      "Epoch 155/2000: loss: 0.2657407522201538, auc: 0.9648295586302853\n",
      "Epoch 156/2000: loss: 0.26479002833366394, auc: 0.9648295586302853\n",
      "Epoch 157/2000: loss: 0.26384636759757996, auc: 0.9648295586302853\n",
      "Epoch 158/2000: loss: 0.2629096508026123, auc: 0.9648295586302852\n",
      "Epoch 159/2000: loss: 0.2619798481464386, auc: 0.9648295586302852\n",
      "Epoch 160/2000: loss: 0.2610568702220917, auc: 0.9648295586302853\n",
      "Epoch 161/2000: loss: 0.26014065742492676, auc: 0.9648037927391719\n",
      "Epoch 162/2000: loss: 0.2592311203479767, auc: 0.9648037927391718\n",
      "Epoch 163/2000: loss: 0.2583281993865967, auc: 0.9648037927391718\n",
      "Epoch 164/2000: loss: 0.2574317753314972, auc: 0.9648037927391718\n",
      "Epoch 165/2000: loss: 0.2565418481826782, auc: 0.9648037927391718\n",
      "Epoch 166/2000: loss: 0.2556583285331726, auc: 0.9647780268480585\n",
      "Epoch 167/2000: loss: 0.2547811269760132, auc: 0.9647780268480585\n",
      "Epoch 168/2000: loss: 0.25391021370887756, auc: 0.9647522609569451\n",
      "Epoch 169/2000: loss: 0.2530454993247986, auc: 0.9647522609569451\n",
      "Epoch 170/2000: loss: 0.2521868944168091, auc: 0.9647522609569451\n",
      "Epoch 171/2000: loss: 0.25133439898490906, auc: 0.9647522609569451\n",
      "Epoch 172/2000: loss: 0.25048789381980896, auc: 0.9647522609569451\n",
      "Epoch 173/2000: loss: 0.2496473640203476, auc: 0.9647522609569451\n",
      "Epoch 174/2000: loss: 0.2488126903772354, auc: 0.9647522609569452\n",
      "Epoch 175/2000: loss: 0.24798384308815002, auc: 0.9647264950658317\n",
      "Epoch 176/2000: loss: 0.24716080725193024, auc: 0.9647264950658319\n",
      "Epoch 177/2000: loss: 0.2463434636592865, auc: 0.9647264950658317\n",
      "Epoch 178/2000: loss: 0.24553178250789642, auc: 0.9647264950658319\n",
      "Epoch 179/2000: loss: 0.24472571909427643, auc: 0.9647264950658317\n",
      "Epoch 180/2000: loss: 0.24392522871494293, auc: 0.9647264950658319\n",
      "Epoch 181/2000: loss: 0.24313020706176758, auc: 0.9647264950658319\n",
      "Epoch 182/2000: loss: 0.24234060943126678, auc: 0.9647264950658319\n",
      "Epoch 183/2000: loss: 0.24155640602111816, auc: 0.9647007291747184\n",
      "Epoch 184/2000: loss: 0.24077758193016052, auc: 0.9647007291747185\n",
      "Epoch 185/2000: loss: 0.24000398814678192, auc: 0.9647007291747185\n",
      "Epoch 186/2000: loss: 0.23923566937446594, auc: 0.9647007291747185\n",
      "Epoch 187/2000: loss: 0.23847247660160065, auc: 0.9646749632836051\n",
      "Epoch 188/2000: loss: 0.2377144694328308, auc: 0.9646749632836052\n",
      "Epoch 189/2000: loss: 0.2369615137577057, auc: 0.9646749632836051\n",
      "Epoch 190/2000: loss: 0.23621362447738647, auc: 0.9646749632836052\n",
      "Epoch 191/2000: loss: 0.2354707270860672, auc: 0.9646749632836052\n",
      "Epoch 192/2000: loss: 0.2347327321767807, auc: 0.9646749632836051\n",
      "Epoch 193/2000: loss: 0.23399966955184937, auc: 0.9646749632836052\n",
      "Epoch 194/2000: loss: 0.23327143490314484, auc: 0.9646749632836052\n",
      "Epoch 195/2000: loss: 0.2325480580329895, auc: 0.9646749632836051\n",
      "Epoch 196/2000: loss: 0.23182938992977142, auc: 0.9646749632836052\n",
      "Epoch 197/2000: loss: 0.23111547529697418, auc: 0.9646749632836051\n",
      "Epoch 198/2000: loss: 0.23040618002414703, auc: 0.9646749632836051\n",
      "Epoch 199/2000: loss: 0.22970157861709595, auc: 0.9646749632836051\n",
      "Epoch 200/2000: loss: 0.22900152206420898, auc: 0.9646749632836051\n",
      "Epoch 201/2000: loss: 0.22830606997013092, auc: 0.9646491973924918\n",
      "Epoch 202/2000: loss: 0.22761507332324982, auc: 0.9646491973924918\n",
      "Epoch 203/2000: loss: 0.22692856192588806, auc: 0.9646491973924918\n",
      "Epoch 204/2000: loss: 0.22624646127223969, auc: 0.9646491973924918\n",
      "Epoch 205/2000: loss: 0.2255687713623047, auc: 0.9646749632836051\n",
      "Epoch 206/2000: loss: 0.2248953878879547, auc: 0.9646749632836051\n",
      "Epoch 207/2000: loss: 0.22422632575035095, auc: 0.9646749632836051\n",
      "Epoch 208/2000: loss: 0.22356155514717102, auc: 0.9646749632836051\n",
      "Epoch 209/2000: loss: 0.22290100157260895, auc: 0.9646749632836051\n",
      "Epoch 210/2000: loss: 0.22224462032318115, auc: 0.9646749632836051\n",
      "Epoch 211/2000: loss: 0.22159244120121002, auc: 0.9646749632836051\n",
      "Epoch 212/2000: loss: 0.2209443598985672, auc: 0.9646749632836051\n",
      "Epoch 213/2000: loss: 0.2203003317117691, auc: 0.9646749632836051\n",
      "Epoch 214/2000: loss: 0.21966040134429932, auc: 0.9646749632836051\n",
      "Epoch 215/2000: loss: 0.21902446448802948, auc: 0.9646749632836051\n",
      "Epoch 216/2000: loss: 0.2183925211429596, auc: 0.9646749632836051\n",
      "Epoch 217/2000: loss: 0.21776452660560608, auc: 0.9646749632836051\n",
      "Epoch 218/2000: loss: 0.21714039146900177, auc: 0.9646749632836051\n",
      "Epoch 219/2000: loss: 0.21652017533779144, auc: 0.9646491973924918\n",
      "Epoch 220/2000: loss: 0.21590375900268555, auc: 0.9646491973924919\n",
      "Epoch 221/2000: loss: 0.21529123187065125, auc: 0.9646491973924918\n",
      "Epoch 222/2000: loss: 0.2146824151277542, auc: 0.9646491973924918\n",
      "Epoch 223/2000: loss: 0.21407735347747803, auc: 0.9646491973924918\n",
      "Epoch 224/2000: loss: 0.2134760171175003, auc: 0.9646491973924918\n",
      "Epoch 225/2000: loss: 0.21287834644317627, auc: 0.9646491973924918\n",
      "Epoch 226/2000: loss: 0.21228435635566711, auc: 0.9646491973924918\n",
      "Epoch 227/2000: loss: 0.21169395744800568, auc: 0.9646234315013785\n",
      "Epoch 228/2000: loss: 0.21110716462135315, auc: 0.9646234315013784\n",
      "Epoch 229/2000: loss: 0.21052393317222595, auc: 0.9646234315013784\n",
      "Epoch 230/2000: loss: 0.2099442183971405, auc: 0.9646234315013785\n",
      "Epoch 231/2000: loss: 0.20936799049377441, auc: 0.9646234315013785\n",
      "Epoch 232/2000: loss: 0.20879526436328888, auc: 0.9646234315013784\n",
      "Epoch 233/2000: loss: 0.20822595059871674, auc: 0.9645976656102652\n",
      "Epoch 234/2000: loss: 0.20766007900238037, auc: 0.9645976656102652\n",
      "Epoch 235/2000: loss: 0.20709756016731262, auc: 0.964597665610265\n",
      "Epoch 236/2000: loss: 0.20653840899467468, auc: 0.9645976656102652\n",
      "Epoch 237/2000: loss: 0.20598261058330536, auc: 0.9645976656102652\n",
      "Epoch 238/2000: loss: 0.20543010532855988, auc: 0.9645976656102652\n",
      "Epoch 239/2000: loss: 0.20488087832927704, auc: 0.964597665610265\n",
      "Epoch 240/2000: loss: 0.20433488488197327, auc: 0.964597665610265\n",
      "Epoch 241/2000: loss: 0.20379211008548737, auc: 0.9645976656102652\n",
      "Epoch 242/2000: loss: 0.20325252413749695, auc: 0.9645976656102652\n",
      "Epoch 243/2000: loss: 0.20271612703800201, auc: 0.9645976656102652\n",
      "Epoch 244/2000: loss: 0.2021828591823578, auc: 0.964597665610265\n",
      "Epoch 245/2000: loss: 0.20165275037288666, auc: 0.9645976656102652\n",
      "Epoch 246/2000: loss: 0.20112569630146027, auc: 0.9645976656102652\n",
      "Epoch 247/2000: loss: 0.20060168206691742, auc: 0.9645976656102652\n",
      "Epoch 248/2000: loss: 0.20008079707622528, auc: 0.964597665610265\n",
      "Epoch 249/2000: loss: 0.19956284761428833, auc: 0.9645976656102652\n",
      "Epoch 250/2000: loss: 0.19904793798923492, auc: 0.9645976656102652\n",
      "Epoch 251/2000: loss: 0.1985359787940979, auc: 0.964597665610265\n",
      "Epoch 252/2000: loss: 0.19802697002887726, auc: 0.964597665610265\n",
      "Epoch 253/2000: loss: 0.197520911693573, auc: 0.9646234315013785\n",
      "Epoch 254/2000: loss: 0.19701772928237915, auc: 0.9646234315013785\n",
      "Epoch 255/2000: loss: 0.1965174376964569, auc: 0.9646234315013785\n",
      "Epoch 256/2000: loss: 0.1960199773311615, auc: 0.9646234315013785\n",
      "Epoch 257/2000: loss: 0.1955253779888153, auc: 0.9646234315013785\n",
      "Epoch 258/2000: loss: 0.19503359496593475, auc: 0.9646234315013785\n",
      "Epoch 259/2000: loss: 0.19454458355903625, auc: 0.9646234315013785\n",
      "Epoch 260/2000: loss: 0.194058358669281, auc: 0.9646234315013785\n",
      "Epoch 261/2000: loss: 0.19357486069202423, auc: 0.9646234315013785\n",
      "Epoch 262/2000: loss: 0.19309407472610474, auc: 0.9646234315013785\n",
      "Epoch 263/2000: loss: 0.1926160305738449, auc: 0.9646234315013785\n",
      "Epoch 264/2000: loss: 0.1921406388282776, auc: 0.9646234315013785\n",
      "Epoch 265/2000: loss: 0.19166792929172516, auc: 0.9646491973924919\n",
      "Epoch 266/2000: loss: 0.19119782745838165, auc: 0.9646491973924919\n",
      "Epoch 267/2000: loss: 0.19073037803173065, auc: 0.9646491973924919\n",
      "Epoch 268/2000: loss: 0.19026552140712738, auc: 0.9646491973924919\n",
      "Epoch 269/2000: loss: 0.18980325758457184, auc: 0.9646491973924919\n",
      "Epoch 270/2000: loss: 0.18934351205825806, auc: 0.9646491973924919\n",
      "Epoch 271/2000: loss: 0.188886359333992, auc: 0.9646491973924919\n",
      "Epoch 272/2000: loss: 0.1884317249059677, auc: 0.9646491973924918\n",
      "Epoch 273/2000: loss: 0.1879795789718628, auc: 0.9646491973924919\n",
      "Epoch 274/2000: loss: 0.18752989172935486, auc: 0.9646491973924919\n",
      "Epoch 275/2000: loss: 0.1870827078819275, auc: 0.9646491973924919\n",
      "Epoch 276/2000: loss: 0.18663793802261353, auc: 0.9646491973924919\n",
      "Epoch 277/2000: loss: 0.18619562685489655, auc: 0.9646491973924919\n",
      "Epoch 278/2000: loss: 0.18575572967529297, auc: 0.9646491973924919\n",
      "Epoch 279/2000: loss: 0.1853182017803192, auc: 0.9646491973924919\n",
      "Epoch 280/2000: loss: 0.18488307297229767, auc: 0.9646491973924919\n",
      "Epoch 281/2000: loss: 0.18445025384426117, auc: 0.9646491973924919\n",
      "Epoch 282/2000: loss: 0.18401983380317688, auc: 0.9646491973924919\n",
      "Epoch 283/2000: loss: 0.18359170854091644, auc: 0.9646491973924919\n",
      "Epoch 284/2000: loss: 0.18316586315631866, auc: 0.9646491973924919\n",
      "Epoch 285/2000: loss: 0.18274232745170593, auc: 0.9646491973924919\n",
      "Epoch 286/2000: loss: 0.18232107162475586, auc: 0.9646491973924919\n",
      "Epoch 287/2000: loss: 0.18190205097198486, auc: 0.9646491973924919\n",
      "Epoch 288/2000: loss: 0.18148528039455414, auc: 0.9646491973924919\n",
      "Epoch 289/2000: loss: 0.1810707449913025, auc: 0.9646491973924919\n",
      "Epoch 290/2000: loss: 0.18065835535526276, auc: 0.9646491973924919\n",
      "Epoch 291/2000: loss: 0.1802482008934021, auc: 0.9646491973924919\n",
      "Epoch 292/2000: loss: 0.17984020709991455, auc: 0.9646491973924919\n",
      "Epoch 293/2000: loss: 0.1794344037771225, auc: 0.9646491973924919\n",
      "Epoch 294/2000: loss: 0.1790306717157364, auc: 0.9646491973924919\n",
      "Epoch 295/2000: loss: 0.17862913012504578, auc: 0.9646491973924919\n",
      "Epoch 296/2000: loss: 0.1782296746969223, auc: 0.9646491973924919\n",
      "Epoch 297/2000: loss: 0.17783230543136597, auc: 0.9646491973924919\n",
      "Epoch 298/2000: loss: 0.17743700742721558, auc: 0.9646491973924919\n",
      "Epoch 299/2000: loss: 0.1770438253879547, auc: 0.9646491973924919\n",
      "Epoch 300/2000: loss: 0.17665265500545502, auc: 0.9646491973924919\n",
      "Epoch 301/2000: loss: 0.1762634962797165, auc: 0.9646491973924919\n",
      "Epoch 302/2000: loss: 0.17587639391422272, auc: 0.9646491973924919\n",
      "Epoch 303/2000: loss: 0.17549128830432892, auc: 0.9646491973924919\n",
      "Epoch 304/2000: loss: 0.1751081645488739, auc: 0.9646491973924919\n",
      "Epoch 305/2000: loss: 0.17472700774669647, auc: 0.9646491973924918\n",
      "Epoch 306/2000: loss: 0.17434783279895782, auc: 0.9646491973924919\n",
      "Epoch 307/2000: loss: 0.17397060990333557, auc: 0.9646491973924919\n",
      "Epoch 308/2000: loss: 0.17359532415866852, auc: 0.9646491973924919\n",
      "Epoch 309/2000: loss: 0.17322194576263428, auc: 0.9646491973924918\n",
      "Epoch 310/2000: loss: 0.17285048961639404, auc: 0.9646491973924918\n",
      "Epoch 311/2000: loss: 0.17248094081878662, auc: 0.9646491973924918\n",
      "Epoch 312/2000: loss: 0.17211322486400604, auc: 0.9646491973924918\n",
      "Epoch 313/2000: loss: 0.17174741625785828, auc: 0.9646491973924918\n",
      "Epoch 314/2000: loss: 0.17138344049453735, auc: 0.9646491973924918\n",
      "Epoch 315/2000: loss: 0.17102132737636566, auc: 0.9646491973924919\n",
      "Epoch 316/2000: loss: 0.17066103219985962, auc: 0.9646234315013785\n",
      "Epoch 317/2000: loss: 0.1703025847673416, auc: 0.9646234315013785\n",
      "Epoch 318/2000: loss: 0.16994589567184448, auc: 0.9646234315013784\n",
      "Epoch 319/2000: loss: 0.1695910394191742, auc: 0.9646234315013785\n",
      "Epoch 320/2000: loss: 0.1692378968000412, auc: 0.9646234315013785\n",
      "Epoch 321/2000: loss: 0.16888658702373505, auc: 0.9646491973924918\n",
      "Epoch 322/2000: loss: 0.16853702068328857, auc: 0.9646491973924919\n",
      "Epoch 323/2000: loss: 0.1681891828775406, auc: 0.9646491973924919\n",
      "Epoch 324/2000: loss: 0.1678430736064911, auc: 0.9646491973924919\n",
      "Epoch 325/2000: loss: 0.16749869287014008, auc: 0.9646491973924919\n",
      "Epoch 326/2000: loss: 0.16715602576732635, auc: 0.9646491973924918\n",
      "Epoch 327/2000: loss: 0.16681504249572754, auc: 0.9646491973924918\n",
      "Epoch 328/2000: loss: 0.16647572815418243, auc: 0.9646234315013785\n",
      "Epoch 329/2000: loss: 0.16613811254501343, auc: 0.9646234315013784\n",
      "Epoch 330/2000: loss: 0.16580212116241455, auc: 0.9646234315013784\n",
      "Epoch 331/2000: loss: 0.16546784341335297, auc: 0.9646234315013785\n",
      "Epoch 332/2000: loss: 0.16513513028621674, auc: 0.9646234315013784\n",
      "Epoch 333/2000: loss: 0.1648041009902954, auc: 0.9646234315013783\n",
      "Epoch 334/2000: loss: 0.16447466611862183, auc: 0.9646234315013784\n",
      "Epoch 335/2000: loss: 0.16414685547351837, auc: 0.9646234315013784\n",
      "Epoch 336/2000: loss: 0.16382059454917908, auc: 0.9646234315013784\n",
      "Epoch 337/2000: loss: 0.16349594295024872, auc: 0.9646234315013784\n",
      "Epoch 338/2000: loss: 0.1631728857755661, auc: 0.9646234315013784\n",
      "Epoch 339/2000: loss: 0.16285136342048645, auc: 0.9646234315013785\n",
      "Epoch 340/2000: loss: 0.16253139078617096, auc: 0.9646234315013784\n",
      "Epoch 341/2000: loss: 0.16221296787261963, auc: 0.9646491973924919\n",
      "Epoch 342/2000: loss: 0.16189609467983246, auc: 0.9646491973924919\n",
      "Epoch 343/2000: loss: 0.16158075630664825, auc: 0.9646491973924918\n",
      "Epoch 344/2000: loss: 0.16126687824726105, auc: 0.9646491973924917\n",
      "Epoch 345/2000: loss: 0.1609545201063156, auc: 0.9646491973924918\n",
      "Epoch 346/2000: loss: 0.16064368188381195, auc: 0.9646491973924918\n",
      "Epoch 347/2000: loss: 0.16033430397510529, auc: 0.9646491973924918\n",
      "Epoch 348/2000: loss: 0.16002638638019562, auc: 0.9646491973924919\n",
      "Epoch 349/2000: loss: 0.15971994400024414, auc: 0.9646491973924918\n",
      "Epoch 350/2000: loss: 0.15941496193408966, auc: 0.9646491973924918\n",
      "Epoch 351/2000: loss: 0.15911142528057098, auc: 0.9646491973924919\n",
      "Epoch 352/2000: loss: 0.1588093340396881, auc: 0.9646491973924918\n",
      "Epoch 353/2000: loss: 0.15850861370563507, auc: 0.9646234315013784\n",
      "Epoch 354/2000: loss: 0.15820932388305664, auc: 0.9646234315013784\n",
      "Epoch 355/2000: loss: 0.15791144967079163, auc: 0.9646234315013784\n",
      "Epoch 356/2000: loss: 0.15761500597000122, auc: 0.9646234315013784\n",
      "Epoch 357/2000: loss: 0.15731990337371826, auc: 0.9646234315013784\n",
      "Epoch 358/2000: loss: 0.1570262312889099, auc: 0.9646234315013785\n",
      "Epoch 359/2000: loss: 0.15673388540744781, auc: 0.964597665610265\n",
      "Epoch 360/2000: loss: 0.15644291043281555, auc: 0.9646234315013784\n",
      "Epoch 361/2000: loss: 0.15615330636501312, auc: 0.9646234315013784\n",
      "Epoch 362/2000: loss: 0.15586502850055695, auc: 0.9646234315013784\n",
      "Epoch 363/2000: loss: 0.15557807683944702, auc: 0.9646234315013784\n",
      "Epoch 364/2000: loss: 0.15529248118400574, auc: 0.9646234315013784\n",
      "Epoch 365/2000: loss: 0.15500818192958832, auc: 0.9646234315013784\n",
      "Epoch 366/2000: loss: 0.15472519397735596, auc: 0.9646234315013784\n",
      "Epoch 367/2000: loss: 0.15444353222846985, auc: 0.9646234315013784\n",
      "Epoch 368/2000: loss: 0.15416312217712402, auc: 0.9646234315013784\n",
      "Epoch 369/2000: loss: 0.15388402342796326, auc: 0.9646234315013784\n",
      "Epoch 370/2000: loss: 0.15360620617866516, auc: 0.9646234315013784\n",
      "Epoch 371/2000: loss: 0.15332964062690735, auc: 0.9646234315013784\n",
      "Epoch 372/2000: loss: 0.15305431187152863, auc: 0.9646234315013784\n",
      "Epoch 373/2000: loss: 0.15278026461601257, auc: 0.9646234315013784\n",
      "Epoch 374/2000: loss: 0.152507483959198, auc: 0.9646234315013784\n",
      "Epoch 375/2000: loss: 0.15223592519760132, auc: 0.9646234315013784\n",
      "Epoch 376/2000: loss: 0.15196560323238373, auc: 0.9646234315013784\n",
      "Epoch 377/2000: loss: 0.15169645845890045, auc: 0.9646234315013785\n",
      "Epoch 378/2000: loss: 0.15142856538295746, auc: 0.9646234315013784\n",
      "Epoch 379/2000: loss: 0.15116189420223236, auc: 0.9646234315013784\n",
      "Epoch 380/2000: loss: 0.15089640021324158, auc: 0.9646234315013784\n",
      "Epoch 381/2000: loss: 0.1506320685148239, auc: 0.9646234315013784\n",
      "Epoch 382/2000: loss: 0.15036895871162415, auc: 0.9646234315013784\n",
      "Epoch 383/2000: loss: 0.1501070111989975, auc: 0.9646234315013784\n",
      "Epoch 384/2000: loss: 0.14984624087810516, auc: 0.9646234315013784\n",
      "Epoch 385/2000: loss: 0.14958664774894714, auc: 0.9646234315013784\n",
      "Epoch 386/2000: loss: 0.14932820200920105, auc: 0.9646234315013784\n",
      "Epoch 387/2000: loss: 0.1490708887577057, auc: 0.9646234315013784\n",
      "Epoch 388/2000: loss: 0.14881472289562225, auc: 0.9646234315013784\n",
      "Epoch 389/2000: loss: 0.14855967462062836, auc: 0.9646234315013784\n",
      "Epoch 390/2000: loss: 0.1483057737350464, auc: 0.9646234315013784\n",
      "Epoch 391/2000: loss: 0.14805299043655396, auc: 0.9646234315013784\n",
      "Epoch 392/2000: loss: 0.14780132472515106, auc: 0.9646234315013784\n",
      "Epoch 393/2000: loss: 0.14755073189735413, auc: 0.9646234315013784\n",
      "Epoch 394/2000: loss: 0.14730127155780792, auc: 0.9646234315013784\n",
      "Epoch 395/2000: loss: 0.14705292880535126, auc: 0.9646234315013784\n",
      "Epoch 396/2000: loss: 0.14680562913417816, auc: 0.9646234315013784\n",
      "Epoch 397/2000: loss: 0.14655941724777222, auc: 0.9646234315013784\n",
      "Epoch 398/2000: loss: 0.14631427824497223, auc: 0.9646234315013784\n",
      "Epoch 399/2000: loss: 0.14607024192810059, auc: 0.9646234315013784\n",
      "Epoch 400/2000: loss: 0.14582720398902893, auc: 0.9646234315013784\n",
      "Epoch 401/2000: loss: 0.14558525383472443, auc: 0.9646234315013785\n",
      "Epoch 402/2000: loss: 0.14534437656402588, auc: 0.9646234315013784\n",
      "Epoch 403/2000: loss: 0.14510449767112732, auc: 0.9646234315013785\n",
      "Epoch 404/2000: loss: 0.14486566185951233, auc: 0.9646234315013785\n",
      "Epoch 405/2000: loss: 0.1446278840303421, auc: 0.9646234315013785\n",
      "Epoch 406/2000: loss: 0.14439108967781067, auc: 0.9646234315013784\n",
      "Epoch 407/2000: loss: 0.1441553384065628, auc: 0.9646234315013784\n",
      "Epoch 408/2000: loss: 0.14392058551311493, auc: 0.9646234315013784\n",
      "Epoch 409/2000: loss: 0.14368684589862823, auc: 0.9646234315013785\n",
      "Epoch 410/2000: loss: 0.14345408976078033, auc: 0.9646234315013785\n",
      "Epoch 411/2000: loss: 0.1432223618030548, auc: 0.9646234315013785\n",
      "Epoch 412/2000: loss: 0.1429915875196457, auc: 0.9646234315013784\n",
      "Epoch 413/2000: loss: 0.14276178181171417, auc: 0.9646234315013784\n",
      "Epoch 414/2000: loss: 0.14253298938274384, auc: 0.9646234315013784\n",
      "Epoch 415/2000: loss: 0.1423051655292511, auc: 0.9646234315013784\n",
      "Epoch 416/2000: loss: 0.14207828044891357, auc: 0.9646234315013783\n",
      "Epoch 417/2000: loss: 0.14185236394405365, auc: 0.9646234315013784\n",
      "Epoch 418/2000: loss: 0.14162738621234894, auc: 0.9646234315013784\n",
      "Epoch 419/2000: loss: 0.14140337705612183, auc: 0.9646234315013784\n",
      "Epoch 420/2000: loss: 0.14118032157421112, auc: 0.9646234315013785\n",
      "Epoch 421/2000: loss: 0.14095816016197205, auc: 0.9646234315013784\n",
      "Epoch 422/2000: loss: 0.14073695242404938, auc: 0.9646234315013784\n",
      "Epoch 423/2000: loss: 0.14051668345928192, auc: 0.9646234315013785\n",
      "Epoch 424/2000: loss: 0.1402973234653473, auc: 0.9645976656102652\n",
      "Epoch 425/2000: loss: 0.1400788575410843, auc: 0.964597665610265\n",
      "Epoch 426/2000: loss: 0.1398613154888153, auc: 0.964597665610265\n",
      "Epoch 427/2000: loss: 0.13964469730854034, auc: 0.964597665610265\n",
      "Epoch 428/2000: loss: 0.13942895829677582, auc: 0.964597665610265\n",
      "Epoch 429/2000: loss: 0.13921409845352173, auc: 0.964597665610265\n",
      "Epoch 430/2000: loss: 0.13900016248226166, auc: 0.9645976656102652\n",
      "Epoch 431/2000: loss: 0.13878709077835083, auc: 0.9645976656102652\n",
      "Epoch 432/2000: loss: 0.13857488334178925, auc: 0.964597665610265\n",
      "Epoch 433/2000: loss: 0.1383635699748993, auc: 0.9645976656102652\n",
      "Epoch 434/2000: loss: 0.13815315067768097, auc: 0.964597665610265\n",
      "Epoch 435/2000: loss: 0.13794353604316711, auc: 0.964597665610265\n",
      "Epoch 436/2000: loss: 0.13773483037948608, auc: 0.964597665610265\n",
      "Epoch 437/2000: loss: 0.1375269591808319, auc: 0.9645976656102652\n",
      "Epoch 438/2000: loss: 0.1373199224472046, auc: 0.9645976656102652\n",
      "Epoch 439/2000: loss: 0.13711373507976532, auc: 0.9645976656102652\n",
      "Epoch 440/2000: loss: 0.1369083821773529, auc: 0.9645718997191518\n",
      "Epoch 441/2000: loss: 0.13670389354228973, auc: 0.9645718997191518\n",
      "Epoch 442/2000: loss: 0.13650020956993103, auc: 0.9645718997191518\n",
      "Epoch 443/2000: loss: 0.13629736006259918, auc: 0.9645718997191517\n",
      "Epoch 444/2000: loss: 0.136095330119133, auc: 0.9645718997191518\n",
      "Epoch 445/2000: loss: 0.13589410483837128, auc: 0.9645718997191518\n",
      "Epoch 446/2000: loss: 0.13569369912147522, auc: 0.9645718997191518\n",
      "Epoch 447/2000: loss: 0.13549409806728363, auc: 0.9645718997191518\n",
      "Epoch 448/2000: loss: 0.1352953165769577, auc: 0.9645718997191518\n",
      "Epoch 449/2000: loss: 0.13509730994701385, auc: 0.9645718997191518\n",
      "Epoch 450/2000: loss: 0.13490012288093567, auc: 0.9645718997191518\n",
      "Epoch 451/2000: loss: 0.13470369577407837, auc: 0.9645718997191518\n",
      "Epoch 452/2000: loss: 0.13450805842876434, auc: 0.9645718997191518\n",
      "Epoch 453/2000: loss: 0.1343132108449936, auc: 0.9645718997191518\n",
      "Epoch 454/2000: loss: 0.13411913812160492, auc: 0.9645718997191518\n",
      "Epoch 455/2000: loss: 0.13392584025859833, auc: 0.9645718997191518\n",
      "Epoch 456/2000: loss: 0.13373331725597382, auc: 0.9645718997191518\n",
      "Epoch 457/2000: loss: 0.1335415244102478, auc: 0.9645718997191518\n",
      "Epoch 458/2000: loss: 0.13335052132606506, auc: 0.9645718997191518\n",
      "Epoch 459/2000: loss: 0.13316026329994202, auc: 0.9645718997191518\n",
      "Epoch 460/2000: loss: 0.13297073543071747, auc: 0.9645718997191518\n",
      "Epoch 461/2000: loss: 0.1327819973230362, auc: 0.9645718997191518\n",
      "Epoch 462/2000: loss: 0.13259395956993103, auc: 0.9645718997191518\n",
      "Epoch 463/2000: loss: 0.13240668177604675, auc: 0.9645718997191518\n",
      "Epoch 464/2000: loss: 0.13222013413906097, auc: 0.9645718997191518\n",
      "Epoch 465/2000: loss: 0.1320343315601349, auc: 0.9645718997191518\n",
      "Epoch 466/2000: loss: 0.1318492293357849, auc: 0.9645718997191518\n",
      "Epoch 467/2000: loss: 0.13166485726833344, auc: 0.9645718997191518\n",
      "Epoch 468/2000: loss: 0.13148123025894165, auc: 0.9645718997191518\n",
      "Epoch 469/2000: loss: 0.13129828870296478, auc: 0.9645718997191517\n",
      "Epoch 470/2000: loss: 0.13111606240272522, auc: 0.9645718997191518\n",
      "Epoch 471/2000: loss: 0.13093455135822296, auc: 0.9645718997191518\n",
      "Epoch 472/2000: loss: 0.130753755569458, auc: 0.9645718997191518\n",
      "Epoch 473/2000: loss: 0.13057363033294678, auc: 0.9645718997191518\n",
      "Epoch 474/2000: loss: 0.13039420545101166, auc: 0.9645718997191518\n",
      "Epoch 475/2000: loss: 0.13021548092365265, auc: 0.9645718997191518\n",
      "Epoch 476/2000: loss: 0.13003745675086975, auc: 0.9645718997191518\n",
      "Epoch 477/2000: loss: 0.1298600733280182, auc: 0.9645718997191518\n",
      "Epoch 478/2000: loss: 0.12968340516090393, auc: 0.9645718997191518\n",
      "Epoch 479/2000: loss: 0.1295074224472046, auc: 0.9645718997191518\n",
      "Epoch 480/2000: loss: 0.12933209538459778, auc: 0.9645718997191518\n",
      "Epoch 481/2000: loss: 0.12915745377540588, auc: 0.9645718997191518\n",
      "Epoch 482/2000: loss: 0.12898345291614532, auc: 0.9645718997191518\n",
      "Epoch 483/2000: loss: 0.12881013751029968, auc: 0.9645718997191518\n",
      "Epoch 484/2000: loss: 0.12863746285438538, auc: 0.9645718997191518\n",
      "Epoch 485/2000: loss: 0.128465473651886, auc: 0.9645718997191518\n",
      "Epoch 486/2000: loss: 0.12829412519931793, auc: 0.9645718997191518\n",
      "Epoch 487/2000: loss: 0.12812340259552002, auc: 0.9645718997191518\n",
      "Epoch 488/2000: loss: 0.12795335054397583, auc: 0.9645718997191518\n",
      "Epoch 489/2000: loss: 0.12778393924236298, auc: 0.9645718997191518\n",
      "Epoch 490/2000: loss: 0.12761516869068146, auc: 0.9645718997191518\n",
      "Epoch 491/2000: loss: 0.12744702398777008, auc: 0.9645718997191518\n",
      "Epoch 492/2000: loss: 0.12727953493595123, auc: 0.9645718997191518\n",
      "Epoch 493/2000: loss: 0.12711264193058014, auc: 0.9645718997191518\n",
      "Epoch 494/2000: loss: 0.1269463747739792, auc: 0.9645718997191518\n",
      "Epoch 495/2000: loss: 0.12678073346614838, auc: 0.9645718997191518\n",
      "Epoch 496/2000: loss: 0.1266157329082489, auc: 0.9645718997191518\n",
      "Epoch 497/2000: loss: 0.126451313495636, auc: 0.9645718997191517\n",
      "Epoch 498/2000: loss: 0.1262875497341156, auc: 0.9645718997191518\n",
      "Epoch 499/2000: loss: 0.12612438201904297, auc: 0.9645718997191518\n",
      "Epoch 500/2000: loss: 0.1259617954492569, auc: 0.9645718997191518\n",
      "Epoch 501/2000: loss: 0.12579983472824097, auc: 0.964597665610265\n",
      "Epoch 502/2000: loss: 0.1256384700536728, auc: 0.9645976656102652\n",
      "Epoch 503/2000: loss: 0.12547771632671356, auc: 0.964597665610265\n",
      "Epoch 504/2000: loss: 0.1253175288438797, auc: 0.9645976656102652\n",
      "Epoch 505/2000: loss: 0.1251579374074936, auc: 0.9645976656102652\n",
      "Epoch 506/2000: loss: 0.12499895691871643, auc: 0.9645976656102652\n",
      "Epoch 507/2000: loss: 0.12484055012464523, auc: 0.9645976656102652\n",
      "Epoch 508/2000: loss: 0.1246827244758606, auc: 0.9645976656102652\n",
      "Epoch 509/2000: loss: 0.12452547252178192, auc: 0.964597665610265\n",
      "Epoch 510/2000: loss: 0.12436880171298981, auc: 0.9645976656102652\n",
      "Epoch 511/2000: loss: 0.12421271204948425, auc: 0.9645976656102652\n",
      "Epoch 512/2000: loss: 0.12405716627836227, auc: 0.9645976656102652\n",
      "Epoch 513/2000: loss: 0.12390220910310745, auc: 0.9645976656102652\n",
      "Epoch 514/2000: loss: 0.123747818171978, auc: 0.9645976656102652\n",
      "Epoch 515/2000: loss: 0.12359397858381271, auc: 0.9645976656102652\n",
      "Epoch 516/2000: loss: 0.1234406977891922, auc: 0.9645976656102652\n",
      "Epoch 517/2000: loss: 0.12328798323869705, auc: 0.9645976656102652\n",
      "Epoch 518/2000: loss: 0.12313580513000488, auc: 0.9645976656102652\n",
      "Epoch 519/2000: loss: 0.12298417836427689, auc: 0.964597665610265\n",
      "Epoch 520/2000: loss: 0.12283311039209366, auc: 0.9645718997191518\n",
      "Epoch 521/2000: loss: 0.1226825937628746, auc: 0.9645718997191518\n",
      "Epoch 522/2000: loss: 0.12253260612487793, auc: 0.9645718997191518\n",
      "Epoch 523/2000: loss: 0.12238316237926483, auc: 0.9645718997191518\n",
      "Epoch 524/2000: loss: 0.12223425507545471, auc: 0.9645718997191518\n",
      "Epoch 525/2000: loss: 0.12208588421344757, auc: 0.9645718997191518\n",
      "Epoch 526/2000: loss: 0.1219380646944046, auc: 0.9645718997191518\n",
      "Epoch 527/2000: loss: 0.12179072946310043, auc: 0.9645718997191518\n",
      "Epoch 528/2000: loss: 0.12164394557476044, auc: 0.9645718997191518\n",
      "Epoch 529/2000: loss: 0.12149768322706223, auc: 0.9645718997191518\n",
      "Epoch 530/2000: loss: 0.1213519349694252, auc: 0.9645718997191518\n",
      "Epoch 531/2000: loss: 0.12120670080184937, auc: 0.9645718997191518\n",
      "Epoch 532/2000: loss: 0.12106198817491531, auc: 0.9645718997191517\n",
      "Epoch 533/2000: loss: 0.12091779708862305, auc: 0.9645718997191518\n",
      "Epoch 534/2000: loss: 0.12077413499355316, auc: 0.9645461338280384\n",
      "Epoch 535/2000: loss: 0.12063094228506088, auc: 0.9645461338280384\n",
      "Epoch 536/2000: loss: 0.12048827856779099, auc: 0.9645461338280384\n",
      "Epoch 537/2000: loss: 0.12034611403942108, auc: 0.9645461338280384\n",
      "Epoch 538/2000: loss: 0.12020443379878998, auc: 0.9645461338280384\n",
      "Epoch 539/2000: loss: 0.12006326019763947, auc: 0.9645461338280384\n",
      "Epoch 540/2000: loss: 0.11992259323596954, auc: 0.9645461338280384\n",
      "Epoch 541/2000: loss: 0.11978241056203842, auc: 0.9645461338280384\n",
      "Epoch 542/2000: loss: 0.11964274197816849, auc: 0.9645461338280384\n",
      "Epoch 543/2000: loss: 0.11950353533029556, auc: 0.9645461338280384\n",
      "Epoch 544/2000: loss: 0.11936481297016144, auc: 0.9645461338280386\n",
      "Epoch 545/2000: loss: 0.11922656744718552, auc: 0.9645461338280384\n",
      "Epoch 546/2000: loss: 0.11908882856369019, auc: 0.9645461338280384\n",
      "Epoch 547/2000: loss: 0.11895155906677246, auc: 0.9645461338280384\n",
      "Epoch 548/2000: loss: 0.11881476640701294, auc: 0.9645461338280384\n",
      "Epoch 549/2000: loss: 0.11867845058441162, auc: 0.9645461338280384\n",
      "Epoch 550/2000: loss: 0.11854259669780731, auc: 0.9645461338280384\n",
      "Epoch 551/2000: loss: 0.1184072345495224, auc: 0.9645461338280384\n",
      "Epoch 552/2000: loss: 0.1182723268866539, auc: 0.9645461338280384\n",
      "Epoch 553/2000: loss: 0.11813787370920181, auc: 0.9645461338280386\n",
      "Epoch 554/2000: loss: 0.11800388991832733, auc: 0.9645461338280384\n",
      "Epoch 555/2000: loss: 0.11787037551403046, auc: 0.9645461338280386\n",
      "Epoch 556/2000: loss: 0.11773732304573059, auc: 0.9645461338280384\n",
      "Epoch 557/2000: loss: 0.11760471761226654, auc: 0.9645461338280384\n",
      "Epoch 558/2000: loss: 0.1174725741147995, auc: 0.9645461338280384\n",
      "Epoch 559/2000: loss: 0.11734087765216827, auc: 0.9645461338280384\n",
      "Epoch 560/2000: loss: 0.11720963567495346, auc: 0.9645461338280384\n",
      "Epoch 561/2000: loss: 0.11707883328199387, auc: 0.9645461338280384\n",
      "Epoch 562/2000: loss: 0.11694847792387009, auc: 0.9645461338280384\n",
      "Epoch 563/2000: loss: 0.11681856960058212, auc: 0.9645461338280384\n",
      "Epoch 564/2000: loss: 0.11668910831212997, auc: 0.9645461338280386\n",
      "Epoch 565/2000: loss: 0.11656007915735245, auc: 0.9645461338280384\n",
      "Epoch 566/2000: loss: 0.11643150448799133, auc: 0.9645461338280384\n",
      "Epoch 567/2000: loss: 0.11630333214998245, auc: 0.9645461338280384\n",
      "Epoch 568/2000: loss: 0.11617561429738998, auc: 0.9645461338280384\n",
      "Epoch 569/2000: loss: 0.11604832112789154, auc: 0.9645461338280384\n",
      "Epoch 570/2000: loss: 0.11592147499322891, auc: 0.9645203679369251\n",
      "Epoch 571/2000: loss: 0.11579504609107971, auc: 0.9645203679369251\n",
      "Epoch 572/2000: loss: 0.11566902697086334, auc: 0.9645203679369251\n",
      "Epoch 573/2000: loss: 0.11554345488548279, auc: 0.9645203679369252\n",
      "Epoch 574/2000: loss: 0.11541827768087387, auc: 0.9645203679369251\n",
      "Epoch 575/2000: loss: 0.11529355496168137, auc: 0.9645203679369251\n",
      "Epoch 576/2000: loss: 0.1151692271232605, auc: 0.9645203679369251\n",
      "Epoch 577/2000: loss: 0.11504530906677246, auc: 0.9645203679369251\n",
      "Epoch 578/2000: loss: 0.11492182314395905, auc: 0.9645203679369251\n",
      "Epoch 579/2000: loss: 0.11479871720075607, auc: 0.9645203679369251\n",
      "Epoch 580/2000: loss: 0.11467605829238892, auc: 0.9645203679369252\n",
      "Epoch 581/2000: loss: 0.1145537868142128, auc: 0.9645203679369252\n",
      "Epoch 582/2000: loss: 0.11443192511796951, auc: 0.9645203679369251\n",
      "Epoch 583/2000: loss: 0.11431045085191727, auc: 0.9645203679369251\n",
      "Epoch 584/2000: loss: 0.11418940871953964, auc: 0.9645203679369252\n",
      "Epoch 585/2000: loss: 0.11406876891851425, auc: 0.9645203679369251\n",
      "Epoch 586/2000: loss: 0.11394850164651871, auc: 0.9645203679369252\n",
      "Epoch 587/2000: loss: 0.11382865905761719, auc: 0.9645203679369251\n",
      "Epoch 588/2000: loss: 0.11370918899774551, auc: 0.9645203679369252\n",
      "Epoch 589/2000: loss: 0.11359010636806488, auc: 0.9645203679369252\n",
      "Epoch 590/2000: loss: 0.11347144842147827, auc: 0.9645203679369251\n",
      "Epoch 591/2000: loss: 0.11335315555334091, auc: 0.9645203679369251\n",
      "Epoch 592/2000: loss: 0.1132352277636528, auc: 0.9645203679369252\n",
      "Epoch 593/2000: loss: 0.11311770975589752, auc: 0.9645203679369252\n",
      "Epoch 594/2000: loss: 0.11300058662891388, auc: 0.9645203679369252\n",
      "Epoch 595/2000: loss: 0.11288384348154068, auc: 0.9645203679369251\n",
      "Epoch 596/2000: loss: 0.11276745796203613, auc: 0.9645203679369251\n",
      "Epoch 597/2000: loss: 0.11265148222446442, auc: 0.9645203679369252\n",
      "Epoch 598/2000: loss: 0.11253586411476135, auc: 0.9645203679369251\n",
      "Epoch 599/2000: loss: 0.11242063343524933, auc: 0.9645203679369252\n",
      "Epoch 600/2000: loss: 0.11230577528476715, auc: 0.9645203679369252\n",
      "Epoch 601/2000: loss: 0.11219128221273422, auc: 0.9645203679369252\n",
      "Epoch 602/2000: loss: 0.11207716166973114, auc: 0.9645203679369251\n",
      "Epoch 603/2000: loss: 0.11196340620517731, auc: 0.9645203679369251\n",
      "Epoch 604/2000: loss: 0.11185002326965332, auc: 0.9645203679369251\n",
      "Epoch 605/2000: loss: 0.11173699796199799, auc: 0.9645203679369251\n",
      "Epoch 606/2000: loss: 0.1116243451833725, auc: 0.9645203679369251\n",
      "Epoch 607/2000: loss: 0.11151205003261566, auc: 0.9645203679369251\n",
      "Epoch 608/2000: loss: 0.11140011996030807, auc: 0.9645203679369251\n",
      "Epoch 609/2000: loss: 0.11128854751586914, auc: 0.9645203679369251\n",
      "Epoch 610/2000: loss: 0.11117733269929886, auc: 0.9645203679369252\n",
      "Epoch 611/2000: loss: 0.11106646806001663, auc: 0.9645203679369251\n",
      "Epoch 612/2000: loss: 0.11095596104860306, auc: 0.9645203679369252\n",
      "Epoch 613/2000: loss: 0.11084580421447754, auc: 0.9645203679369251\n",
      "Epoch 614/2000: loss: 0.11073601245880127, auc: 0.9645203679369251\n",
      "Epoch 615/2000: loss: 0.11062654852867126, auc: 0.9645203679369252\n",
      "Epoch 616/2000: loss: 0.11051742732524872, auc: 0.9645203679369251\n",
      "Epoch 617/2000: loss: 0.11040869355201721, auc: 0.9645203679369251\n",
      "Epoch 618/2000: loss: 0.11030027270317078, auc: 0.9645203679369251\n",
      "Epoch 619/2000: loss: 0.1101922020316124, auc: 0.9645203679369251\n",
      "Epoch 620/2000: loss: 0.11008448898792267, auc: 0.9645203679369251\n",
      "Epoch 621/2000: loss: 0.10997708886861801, auc: 0.9645203679369252\n",
      "Epoch 622/2000: loss: 0.109870046377182, auc: 0.9645203679369251\n",
      "Epoch 623/2000: loss: 0.10976334661245346, auc: 0.9645203679369251\n",
      "Epoch 624/2000: loss: 0.10965695977210999, auc: 0.9645203679369252\n",
      "Epoch 625/2000: loss: 0.10955091565847397, auc: 0.9645203679369251\n",
      "Epoch 626/2000: loss: 0.1094452291727066, auc: 0.9645203679369251\n",
      "Epoch 627/2000: loss: 0.10933984071016312, auc: 0.9645203679369252\n",
      "Epoch 628/2000: loss: 0.10923478752374649, auc: 0.9645203679369251\n",
      "Epoch 629/2000: loss: 0.10913007706403732, auc: 0.9645203679369251\n",
      "Epoch 630/2000: loss: 0.10902568697929382, auc: 0.9645203679369251\n",
      "Epoch 631/2000: loss: 0.10892161726951599, auc: 0.9645203679369252\n",
      "Epoch 632/2000: loss: 0.10881787538528442, auc: 0.9645203679369252\n",
      "Epoch 633/2000: loss: 0.10871446132659912, auc: 0.9645203679369251\n",
      "Epoch 634/2000: loss: 0.10861136764287949, auc: 0.9645203679369251\n",
      "Epoch 635/2000: loss: 0.10850860178470612, auc: 0.9645203679369251\n",
      "Epoch 636/2000: loss: 0.10840613394975662, auc: 0.9645203679369252\n",
      "Epoch 637/2000: loss: 0.1083039939403534, auc: 0.9645203679369251\n",
      "Epoch 638/2000: loss: 0.10820216685533524, auc: 0.9645203679369252\n",
      "Epoch 639/2000: loss: 0.10810066759586334, auc: 0.9645203679369252\n",
      "Epoch 640/2000: loss: 0.10799948126077652, auc: 0.9645203679369251\n",
      "Epoch 641/2000: loss: 0.10789858549833298, auc: 0.9645203679369251\n",
      "Epoch 642/2000: loss: 0.1077980175614357, auc: 0.9645203679369251\n",
      "Epoch 643/2000: loss: 0.10769776254892349, auc: 0.9645203679369251\n",
      "Epoch 644/2000: loss: 0.10759781301021576, auc: 0.9645203679369251\n",
      "Epoch 645/2000: loss: 0.1074981614947319, auc: 0.9645203679369251\n",
      "Epoch 646/2000: loss: 0.10739881545305252, auc: 0.9645203679369252\n",
      "Epoch 647/2000: loss: 0.10729978233575821, auc: 0.9645203679369251\n",
      "Epoch 648/2000: loss: 0.10720104724168777, auc: 0.9645203679369251\n",
      "Epoch 649/2000: loss: 0.10710260272026062, auc: 0.9645203679369252\n",
      "Epoch 650/2000: loss: 0.10700448602437973, auc: 0.9645203679369252\n",
      "Epoch 651/2000: loss: 0.10690664499998093, auc: 0.9645203679369252\n",
      "Epoch 652/2000: loss: 0.1068091094493866, auc: 0.9645203679369251\n",
      "Epoch 653/2000: loss: 0.10671187192201614, auc: 0.9645203679369252\n",
      "Epoch 654/2000: loss: 0.10661491751670837, auc: 0.9645203679369252\n",
      "Epoch 655/2000: loss: 0.10651828348636627, auc: 0.9645203679369252\n",
      "Epoch 656/2000: loss: 0.10642191022634506, auc: 0.9645203679369251\n",
      "Epoch 657/2000: loss: 0.10632585734128952, auc: 0.9645203679369251\n",
      "Epoch 658/2000: loss: 0.10623007267713547, auc: 0.9645203679369251\n",
      "Epoch 659/2000: loss: 0.10613460093736649, auc: 0.9645203679369252\n",
      "Epoch 660/2000: loss: 0.1060393899679184, auc: 0.9645203679369252\n",
      "Epoch 661/2000: loss: 0.10594447702169418, auc: 0.9645203679369251\n",
      "Epoch 662/2000: loss: 0.10584985464811325, auc: 0.9645203679369252\n",
      "Epoch 663/2000: loss: 0.105755515396595, auc: 0.9645203679369252\n",
      "Epoch 664/2000: loss: 0.10566143691539764, auc: 0.9645203679369252\n",
      "Epoch 665/2000: loss: 0.10556767135858536, auc: 0.9645203679369252\n",
      "Epoch 666/2000: loss: 0.10547417402267456, auc: 0.9645203679369251\n",
      "Epoch 667/2000: loss: 0.10538095235824585, auc: 0.9645203679369251\n",
      "Epoch 668/2000: loss: 0.10528802126646042, auc: 0.9645203679369251\n",
      "Epoch 669/2000: loss: 0.10519535094499588, auc: 0.9645203679369251\n",
      "Epoch 670/2000: loss: 0.10510296374559402, auc: 0.9645203679369251\n",
      "Epoch 671/2000: loss: 0.10501086711883545, auc: 0.9645203679369251\n",
      "Epoch 672/2000: loss: 0.10491902381181717, auc: 0.9645203679369251\n",
      "Epoch 673/2000: loss: 0.10482744127511978, auc: 0.9645203679369252\n",
      "Epoch 674/2000: loss: 0.10473615676164627, auc: 0.9645203679369251\n",
      "Epoch 675/2000: loss: 0.10464514046907425, auc: 0.9645203679369251\n",
      "Epoch 676/2000: loss: 0.10455439239740372, auc: 0.9645203679369251\n",
      "Epoch 677/2000: loss: 0.10446389764547348, auc: 0.9645203679369252\n",
      "Epoch 678/2000: loss: 0.10437367856502533, auc: 0.9645203679369251\n",
      "Epoch 679/2000: loss: 0.10428373515605927, auc: 0.9645203679369252\n",
      "Epoch 680/2000: loss: 0.10419405996799469, auc: 0.9645203679369251\n",
      "Epoch 681/2000: loss: 0.10410463064908981, auc: 0.9645203679369251\n",
      "Epoch 682/2000: loss: 0.10401546955108643, auc: 0.9645203679369252\n",
      "Epoch 683/2000: loss: 0.10392656177282333, auc: 0.9645203679369252\n",
      "Epoch 684/2000: loss: 0.10383792221546173, auc: 0.9645203679369251\n",
      "Epoch 685/2000: loss: 0.10374954342842102, auc: 0.9645203679369252\n",
      "Epoch 686/2000: loss: 0.1036614254117012, auc: 0.9645203679369251\n",
      "Epoch 687/2000: loss: 0.10357357561588287, auc: 0.9645203679369252\n",
      "Epoch 688/2000: loss: 0.10348595678806305, auc: 0.9645203679369251\n",
      "Epoch 689/2000: loss: 0.10339860618114471, auc: 0.9645203679369252\n",
      "Epoch 690/2000: loss: 0.10331149399280548, auc: 0.9645203679369251\n",
      "Epoch 691/2000: loss: 0.10322466492652893, auc: 0.9645203679369251\n",
      "Epoch 692/2000: loss: 0.10313808172941208, auc: 0.9645203679369252\n",
      "Epoch 693/2000: loss: 0.10305173695087433, auc: 0.9645203679369251\n",
      "Epoch 694/2000: loss: 0.10296563804149628, auc: 0.9645203679369252\n",
      "Epoch 695/2000: loss: 0.10287979990243912, auc: 0.9645203679369251\n",
      "Epoch 696/2000: loss: 0.10279422253370285, auc: 0.9645203679369251\n",
      "Epoch 697/2000: loss: 0.1027088537812233, auc: 0.9645203679369251\n",
      "Epoch 698/2000: loss: 0.10262377560138702, auc: 0.9645203679369251\n",
      "Epoch 699/2000: loss: 0.10253892093896866, auc: 0.9645203679369251\n",
      "Epoch 700/2000: loss: 0.1024543046951294, auc: 0.9645203679369251\n",
      "Epoch 701/2000: loss: 0.10236994177103043, auc: 0.9645203679369252\n",
      "Epoch 702/2000: loss: 0.10228581726551056, auc: 0.9645203679369251\n",
      "Epoch 703/2000: loss: 0.1022019311785698, auc: 0.9645203679369251\n",
      "Epoch 704/2000: loss: 0.10211829841136932, auc: 0.9645203679369252\n",
      "Epoch 705/2000: loss: 0.10203488171100616, auc: 0.9645203679369251\n",
      "Epoch 706/2000: loss: 0.10195174068212509, auc: 0.9645203679369251\n",
      "Epoch 707/2000: loss: 0.10186881572008133, auc: 0.9645203679369252\n",
      "Epoch 708/2000: loss: 0.10178612172603607, auc: 0.9645203679369251\n",
      "Epoch 709/2000: loss: 0.10170365869998932, auc: 0.9645203679369251\n",
      "Epoch 710/2000: loss: 0.10162143409252167, auc: 0.9645203679369251\n",
      "Epoch 711/2000: loss: 0.10153945535421371, auc: 0.9645203679369251\n",
      "Epoch 712/2000: loss: 0.10145770758390427, auc: 0.9645203679369252\n",
      "Epoch 713/2000: loss: 0.10137618333101273, auc: 0.9645203679369251\n",
      "Epoch 714/2000: loss: 0.10129489004611969, auc: 0.9645203679369252\n",
      "Epoch 715/2000: loss: 0.10121382027864456, auc: 0.9645203679369252\n",
      "Epoch 716/2000: loss: 0.10113299638032913, auc: 0.9645203679369251\n",
      "Epoch 717/2000: loss: 0.10105238854885101, auc: 0.9645203679369251\n",
      "Epoch 718/2000: loss: 0.10097202658653259, auc: 0.9645203679369251\n",
      "Epoch 719/2000: loss: 0.10089188069105148, auc: 0.9645203679369252\n",
      "Epoch 720/2000: loss: 0.10081194341182709, auc: 0.9645203679369252\n",
      "Epoch 721/2000: loss: 0.10073226690292358, auc: 0.9645203679369252\n",
      "Epoch 722/2000: loss: 0.100652776658535, auc: 0.9645203679369252\n",
      "Epoch 723/2000: loss: 0.10057353228330612, auc: 0.9645203679369252\n",
      "Epoch 724/2000: loss: 0.10049450397491455, auc: 0.9645203679369251\n",
      "Epoch 725/2000: loss: 0.10041569173336029, auc: 0.9645203679369252\n",
      "Epoch 726/2000: loss: 0.10033711045980453, auc: 0.9645203679369251\n",
      "Epoch 727/2000: loss: 0.1002587303519249, auc: 0.9645203679369251\n",
      "Epoch 728/2000: loss: 0.10018058866262436, auc: 0.9645203679369252\n",
      "Epoch 729/2000: loss: 0.10010266304016113, auc: 0.9645203679369251\n",
      "Epoch 730/2000: loss: 0.10002494603395462, auc: 0.9645203679369251\n",
      "Epoch 731/2000: loss: 0.09994744509458542, auc: 0.9645203679369252\n",
      "Epoch 732/2000: loss: 0.09987018257379532, auc: 0.9645203679369252\n",
      "Epoch 733/2000: loss: 0.09979309141635895, auc: 0.9645203679369252\n",
      "Epoch 734/2000: loss: 0.09971623867750168, auc: 0.9645203679369251\n",
      "Epoch 735/2000: loss: 0.09963961690664291, auc: 0.9645203679369251\n",
      "Epoch 736/2000: loss: 0.09956317394971848, auc: 0.9645203679369251\n",
      "Epoch 737/2000: loss: 0.09948695451021194, auc: 0.9645203679369252\n",
      "Epoch 738/2000: loss: 0.09941094368696213, auc: 0.9645203679369252\n",
      "Epoch 739/2000: loss: 0.09933514147996902, auc: 0.9645203679369252\n",
      "Epoch 740/2000: loss: 0.09925954788923264, auc: 0.9645203679369252\n",
      "Epoch 741/2000: loss: 0.09918417781591415, auc: 0.9645203679369251\n",
      "Epoch 742/2000: loss: 0.09910900145769119, auc: 0.9645203679369251\n",
      "Epoch 743/2000: loss: 0.09903403371572495, auc: 0.9645203679369251\n",
      "Epoch 744/2000: loss: 0.09895928204059601, auc: 0.9645203679369251\n",
      "Epoch 745/2000: loss: 0.09888472408056259, auc: 0.9645203679369252\n",
      "Epoch 746/2000: loss: 0.0988103598356247, auc: 0.9645203679369252\n",
      "Epoch 747/2000: loss: 0.0987362191081047, auc: 0.9645203679369251\n",
      "Epoch 748/2000: loss: 0.09866227209568024, auc: 0.9645203679369252\n",
      "Epoch 749/2000: loss: 0.09858854115009308, auc: 0.9645203679369251\n",
      "Epoch 750/2000: loss: 0.09851498156785965, auc: 0.9645203679369252\n",
      "Epoch 751/2000: loss: 0.09844163805246353, auc: 0.9645203679369252\n",
      "Epoch 752/2000: loss: 0.09836848825216293, auc: 0.9645203679369251\n",
      "Epoch 753/2000: loss: 0.09829553961753845, auc: 0.9645203679369252\n",
      "Epoch 754/2000: loss: 0.09822279214859009, auc: 0.9645203679369251\n",
      "Epoch 755/2000: loss: 0.09815025329589844, auc: 0.9645203679369252\n",
      "Epoch 756/2000: loss: 0.09807788580656052, auc: 0.9645203679369252\n",
      "Epoch 757/2000: loss: 0.0980057492852211, auc: 0.9645203679369252\n",
      "Epoch 758/2000: loss: 0.09793376177549362, auc: 0.9645203679369251\n",
      "Epoch 759/2000: loss: 0.09786199778318405, auc: 0.9645203679369252\n",
      "Epoch 760/2000: loss: 0.09779042750597, auc: 0.9645203679369251\n",
      "Epoch 761/2000: loss: 0.09771902859210968, auc: 0.9645203679369251\n",
      "Epoch 762/2000: loss: 0.09764785319566727, auc: 0.9645203679369251\n",
      "Epoch 763/2000: loss: 0.09757685661315918, auc: 0.9645203679369252\n",
      "Epoch 764/2000: loss: 0.09750603139400482, auc: 0.9645203679369251\n",
      "Epoch 765/2000: loss: 0.09743542224168777, auc: 0.9645203679369251\n",
      "Epoch 766/2000: loss: 0.09736499935388565, auc: 0.9645203679369252\n",
      "Epoch 767/2000: loss: 0.09729474037885666, auc: 0.9645203679369252\n",
      "Epoch 768/2000: loss: 0.09722468256950378, auc: 0.9645203679369251\n",
      "Epoch 769/2000: loss: 0.09715483337640762, auc: 0.9645203679369251\n",
      "Epoch 770/2000: loss: 0.09708516299724579, auc: 0.9645203679369252\n",
      "Epoch 771/2000: loss: 0.09701566398143768, auc: 0.9645203679369252\n",
      "Epoch 772/2000: loss: 0.0969463512301445, auc: 0.9645203679369251\n",
      "Epoch 773/2000: loss: 0.09687723219394684, auc: 0.9645203679369251\n",
      "Epoch 774/2000: loss: 0.0968082919716835, auc: 0.9645203679369252\n",
      "Epoch 775/2000: loss: 0.09673954546451569, auc: 0.9645203679369252\n",
      "Epoch 776/2000: loss: 0.09667094796895981, auc: 0.9645203679369251\n",
      "Epoch 777/2000: loss: 0.09660256654024124, auc: 0.9645203679369251\n",
      "Epoch 778/2000: loss: 0.0965343564748764, auc: 0.9645203679369251\n",
      "Epoch 779/2000: loss: 0.0964663177728653, auc: 0.9645203679369251\n",
      "Epoch 780/2000: loss: 0.09639845788478851, auc: 0.9645203679369251\n",
      "Epoch 781/2000: loss: 0.09633079171180725, auc: 0.9645203679369252\n",
      "Epoch 782/2000: loss: 0.09626331180334091, auc: 0.9645203679369251\n",
      "Epoch 783/2000: loss: 0.09619598090648651, auc: 0.9645203679369251\n",
      "Epoch 784/2000: loss: 0.09612884372472763, auc: 0.9645203679369251\n",
      "Epoch 785/2000: loss: 0.09606187790632248, auc: 0.9645203679369252\n",
      "Epoch 786/2000: loss: 0.09599509090185165, auc: 0.9645203679369252\n",
      "Epoch 787/2000: loss: 0.09592846781015396, auc: 0.9645203679369252\n",
      "Epoch 788/2000: loss: 0.09586204588413239, auc: 0.9645203679369252\n",
      "Epoch 789/2000: loss: 0.09579578042030334, auc: 0.9645203679369251\n",
      "Epoch 790/2000: loss: 0.09572967886924744, auc: 0.9645203679369251\n",
      "Epoch 791/2000: loss: 0.09566375613212585, auc: 0.9645203679369251\n",
      "Epoch 792/2000: loss: 0.0955980196595192, auc: 0.9645203679369252\n",
      "Epoch 793/2000: loss: 0.09553243964910507, auc: 0.9645203679369251\n",
      "Epoch 794/2000: loss: 0.09546703845262527, auc: 0.9645203679369251\n",
      "Epoch 795/2000: loss: 0.09540180116891861, auc: 0.9645203679369252\n",
      "Epoch 796/2000: loss: 0.09533673524856567, auc: 0.9645203679369252\n",
      "Epoch 797/2000: loss: 0.09527184069156647, auc: 0.9645203679369252\n",
      "Epoch 798/2000: loss: 0.0952071100473404, auc: 0.9645203679369251\n",
      "Epoch 799/2000: loss: 0.09514255076646805, auc: 0.9645203679369252\n",
      "Epoch 800/2000: loss: 0.09507815539836884, auc: 0.9645203679369251\n",
      "Epoch 801/2000: loss: 0.09501393139362335, auc: 0.9645203679369251\n",
      "Epoch 802/2000: loss: 0.094949871301651, auc: 0.9645203679369251\n",
      "Epoch 803/2000: loss: 0.09488597512245178, auc: 0.9645203679369251\n",
      "Epoch 804/2000: loss: 0.09482225030660629, auc: 0.9645203679369252\n",
      "Epoch 805/2000: loss: 0.09475867450237274, auc: 0.9645203679369251\n",
      "Epoch 806/2000: loss: 0.09469527006149292, auc: 0.9645203679369251\n",
      "Epoch 807/2000: loss: 0.09463202208280563, auc: 0.9645203679369252\n",
      "Epoch 808/2000: loss: 0.09456896781921387, auc: 0.9645203679369251\n",
      "Epoch 809/2000: loss: 0.09450604766607285, auc: 0.9645203679369251\n",
      "Epoch 810/2000: loss: 0.09444328397512436, auc: 0.9645203679369251\n",
      "Epoch 811/2000: loss: 0.0943806990981102, auc: 0.9645203679369252\n",
      "Epoch 812/2000: loss: 0.09431826323270798, auc: 0.9645203679369251\n",
      "Epoch 813/2000: loss: 0.09425599128007889, auc: 0.9645203679369252\n",
      "Epoch 814/2000: loss: 0.09419386833906174, auc: 0.9645203679369251\n",
      "Epoch 815/2000: loss: 0.09413192421197891, auc: 0.9645203679369252\n",
      "Epoch 816/2000: loss: 0.09407011419534683, auc: 0.9645203679369251\n",
      "Epoch 817/2000: loss: 0.09400846064090729, auc: 0.9645203679369251\n",
      "Epoch 818/2000: loss: 0.09394698590040207, auc: 0.9645203679369252\n",
      "Epoch 819/2000: loss: 0.09388566017150879, auc: 0.9645203679369252\n",
      "Epoch 820/2000: loss: 0.09382449090480804, auc: 0.9645203679369251\n",
      "Epoch 821/2000: loss: 0.09376345574855804, auc: 0.9645203679369252\n",
      "Epoch 822/2000: loss: 0.09370259195566177, auc: 0.9645203679369251\n",
      "Epoch 823/2000: loss: 0.09364189207553864, auc: 0.9645203679369252\n",
      "Epoch 824/2000: loss: 0.09358133375644684, auc: 0.9645203679369252\n",
      "Epoch 825/2000: loss: 0.09352093935012817, auc: 0.9645203679369252\n",
      "Epoch 826/2000: loss: 0.09346067905426025, auc: 0.9645203679369252\n",
      "Epoch 827/2000: loss: 0.09340057522058487, auc: 0.9645203679369252\n",
      "Epoch 828/2000: loss: 0.09334063529968262, auc: 0.9645203679369252\n",
      "Epoch 829/2000: loss: 0.09328082948923111, auc: 0.9645203679369251\n",
      "Epoch 830/2000: loss: 0.09322119504213333, auc: 0.9645203679369251\n",
      "Epoch 831/2000: loss: 0.0931616872549057, auc: 0.9645203679369252\n",
      "Epoch 832/2000: loss: 0.0931023433804512, auc: 0.9645203679369251\n",
      "Epoch 833/2000: loss: 0.09304313361644745, auc: 0.9645203679369251\n",
      "Epoch 834/2000: loss: 0.09298408031463623, auc: 0.9645203679369252\n",
      "Epoch 835/2000: loss: 0.09292518347501755, auc: 0.9645203679369251\n",
      "Epoch 836/2000: loss: 0.09286642074584961, auc: 0.9645203679369251\n",
      "Epoch 837/2000: loss: 0.09280780702829361, auc: 0.9645203679369252\n",
      "Epoch 838/2000: loss: 0.09274934977293015, auc: 0.9645203679369251\n",
      "Epoch 839/2000: loss: 0.09269103407859802, auc: 0.9645203679369252\n",
      "Epoch 840/2000: loss: 0.09263285994529724, auc: 0.9645203679369251\n",
      "Epoch 841/2000: loss: 0.0925748273730278, auc: 0.9645203679369251\n",
      "Epoch 842/2000: loss: 0.0925169438123703, auc: 0.9645203679369251\n",
      "Epoch 843/2000: loss: 0.09245919436216354, auc: 0.9645203679369252\n",
      "Epoch 844/2000: loss: 0.09240160882472992, auc: 0.9645203679369251\n",
      "Epoch 845/2000: loss: 0.09234414249658585, auc: 0.9645203679369251\n",
      "Epoch 846/2000: loss: 0.09228682518005371, auc: 0.9645203679369252\n",
      "Epoch 847/2000: loss: 0.09222966432571411, auc: 0.9645203679369251\n",
      "Epoch 848/2000: loss: 0.09217263013124466, auc: 0.9645203679369252\n",
      "Epoch 849/2000: loss: 0.09211573749780655, auc: 0.9645203679369252\n",
      "Epoch 850/2000: loss: 0.09205897897481918, auc: 0.9645203679369251\n",
      "Epoch 851/2000: loss: 0.09200237691402435, auc: 0.9645203679369252\n",
      "Epoch 852/2000: loss: 0.09194590151309967, auc: 0.9645203679369252\n",
      "Epoch 853/2000: loss: 0.09188956767320633, auc: 0.9645203679369252\n",
      "Epoch 854/2000: loss: 0.09183337539434433, auc: 0.9645203679369251\n",
      "Epoch 855/2000: loss: 0.09177731722593307, auc: 0.9645203679369251\n",
      "Epoch 856/2000: loss: 0.09172140061855316, auc: 0.9645203679369251\n",
      "Epoch 857/2000: loss: 0.0916656106710434, auc: 0.9645203679369251\n",
      "Epoch 858/2000: loss: 0.09160996973514557, auc: 0.9645203679369252\n",
      "Epoch 859/2000: loss: 0.09155447036027908, auc: 0.9645203679369251\n",
      "Epoch 860/2000: loss: 0.09149909764528275, auc: 0.9645203679369252\n",
      "Epoch 861/2000: loss: 0.09144385904073715, auc: 0.9645203679369251\n",
      "Epoch 862/2000: loss: 0.09138873964548111, auc: 0.9645203679369251\n",
      "Epoch 863/2000: loss: 0.0913337841629982, auc: 0.9645203679369252\n",
      "Epoch 864/2000: loss: 0.09127894043922424, auc: 0.9645203679369252\n",
      "Epoch 865/2000: loss: 0.09122423827648163, auc: 0.9645203679369252\n",
      "Epoch 866/2000: loss: 0.09116967767477036, auc: 0.9645203679369252\n",
      "Epoch 867/2000: loss: 0.09111522138118744, auc: 0.9645203679369251\n",
      "Epoch 868/2000: loss: 0.09106092154979706, auc: 0.9645203679369251\n",
      "Epoch 869/2000: loss: 0.09100674837827682, auc: 0.9645203679369252\n",
      "Epoch 870/2000: loss: 0.09095270186662674, auc: 0.9645203679369251\n",
      "Epoch 871/2000: loss: 0.0908987820148468, auc: 0.9645203679369251\n",
      "Epoch 872/2000: loss: 0.09084498882293701, auc: 0.9645461338280384\n",
      "Epoch 873/2000: loss: 0.09079133719205856, auc: 0.9645461338280383\n",
      "Epoch 874/2000: loss: 0.09073782712221146, auc: 0.9645461338280383\n",
      "Epoch 875/2000: loss: 0.0906844213604927, auc: 0.9645461338280383\n",
      "Epoch 876/2000: loss: 0.0906311571598053, auc: 0.9645461338280383\n",
      "Epoch 877/2000: loss: 0.09057800471782684, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 878/2000: loss: 0.09052499383687973, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 879/2000: loss: 0.09047213196754456, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 880/2000: loss: 0.09041935950517654, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 881/2000: loss: 0.09036672115325928, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 882/2000: loss: 0.09031420946121216, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 883/2000: loss: 0.09026183933019638, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 884/2000: loss: 0.09020958095788956, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 885/2000: loss: 0.09015745669603348, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 886/2000: loss: 0.09010544419288635, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 887/2000: loss: 0.09005356580018997, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 888/2000: loss: 0.09000180661678314, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 889/2000: loss: 0.08995017409324646, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 890/2000: loss: 0.08989864587783813, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 891/2000: loss: 0.08984725922346115, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 892/2000: loss: 0.08979598432779312, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 893/2000: loss: 0.08974483609199524, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 894/2000: loss: 0.08969380706548691, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 895/2000: loss: 0.08964288234710693, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 896/2000: loss: 0.0895921066403389, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Auc: 0.9768659236744344\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "# from dgl.nn import GraphConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from early_stop import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "number_class = 2\n",
    "linear_model = GCNConv(hiddle.shape[1], number_class) \n",
    "optimizer = Adam(linear_model.parameters(),lr = 1e-3)\n",
    "epochs = 2000\n",
    "early_stop = EarlyStopping(patience=20)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    linear_model.train()\n",
    "    logits = linear_model(hiddle, data.edge_index)\n",
    "    train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, 10]))\n",
    "    linear_model.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    linear_model.eval()\n",
    "    logits = linear_model(hiddle, data.edge_index)\n",
    "    val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, 10]))\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "    \n",
    "    early_stop(val_loss, linear_model)\n",
    "    if early_stop.early_stop == True:\n",
    "        print (\"Early stopping\")\n",
    "        break\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: loss: {train_loss}, auc: {auc}\")\n",
    "\n",
    "linear_model.eval()\n",
    "logits = linear_model(hiddle, data.edge_index)\n",
    "probs = logits.softmax(1)\n",
    "auc = roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "print (f\"Test Auc: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重构损失导向，检测融合嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "# from dgl.nn import GraphConv\n",
    "# from torch_geometric.nn import GCNConv\n",
    "from early_stop import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pygod.models.basic_nn import GCN\n",
    "\n",
    "\n",
    "class DOMINANT_recon(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 hid_dim,\n",
    "                 out_dim,\n",
    "                 decoder_layers,\n",
    "                 dropout,\n",
    "                 act):\n",
    "        super(DOMINANT_recon, self).__init__()\n",
    "\n",
    "        # split the number of layers for the encoder and decoders\n",
    "\n",
    "        self.attr_decoder = GCN(in_channels=in_dim,\n",
    "                                hidden_channels=hid_dim,\n",
    "                                num_layers=decoder_layers,\n",
    "                                out_channels=out_dim,\n",
    "                                dropout=dropout,\n",
    "                                act=act)\n",
    "\n",
    "        self.struct_decoder = GCN(in_channels=in_dim,\n",
    "                                  hidden_channels=hid_dim,\n",
    "                                  num_layers=decoder_layers,\n",
    "                                  out_channels=out_dim,\n",
    "                                  dropout=dropout,\n",
    "                                  act=act)\n",
    "\n",
    "    def forward(self, h, edge_index):\n",
    "        # decode feature matrix\n",
    "        x_ = self.attr_decoder(h, edge_index)\n",
    "        # print (x_.shape)\n",
    "        # decode adjacency matrix\n",
    "        h_ = self.struct_decoder(h, edge_index)\n",
    "        # print (h_.shape)\n",
    "        s_ = h_ @ h_.T\n",
    "        # return reconstructed matrices\n",
    "        return x_, s_, h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiddle = hid_dominate.detach()\n",
    "# hiddle = BW_hid.detach()\n",
    "# hiddle = GAT_hid.detach()\n",
    "# 融合的特征未曾归一化\n",
    "\n",
    "# 补充信息最好策略\n",
    "hiddle = torch.concat((1 * feature_normalize(GAT_hid.detach(),axis=0), 1 * feature_normalize(BW_hid.detach(),axis=0), 0.2 * feature_normalize(hid_dominate.detach(),axis=0)), axis=1)\n",
    "\n",
    "# 可学习化的参数\n",
    "# l_weight = [nn.Parameter(torch.randn([hiddle.shape[-1]], dtype=torch.float32, requires_grad=True))]\n",
    "# hiddle = torch.mul(hiddle, torch.softmax(*l_weight, dim = 0))\n",
    "# optimizer_ = Adam(l_weight, lr = 1e-3)\n",
    "\n",
    "# 计算hiddle特征之间的相似度，放缩到0-1之间 （专家系统）\n",
    "# hiddle = zero2one((1-cosine_distance(hiddle.T)).mean(axis=0))*hiddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.8370133638381958\n",
      "[Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)]\n",
      "Epoch 1/100: loss: 561.194091796875, test_auc: 0.6760683760683761\n",
      "Epoch 2/100: loss: 220.81646728515625, test_auc: 0.6513071895424838\n",
      "Epoch 3/100: loss: 150.86837768554688, test_auc: 0.605580693815988\n",
      "Epoch 4/100: loss: 133.05902099609375, test_auc: 0.6114379084967321\n",
      "Epoch 5/100: loss: 112.91568756103516, test_auc: 0.6219708396178985\n",
      "Epoch 6/100: loss: 84.1954345703125, test_auc: 0.6246354952237305\n",
      "Epoch 7/100: loss: 56.54951095581055, test_auc: 0.6240573152337858\n",
      "Epoch 8/100: loss: 35.511192321777344, test_auc: 0.6214429361488185\n",
      "Epoch 9/100: loss: 21.326702117919922, test_auc: 0.6216440422322775\n",
      "Epoch 10/100: loss: 12.360699653625488, test_auc: 0.6313976872800403\n",
      "Epoch 11/100: loss: 7.181057929992676, test_auc: 0.6531422825540473\n",
      "Epoch 12/100: loss: 4.632518768310547, test_auc: 0.664756158873806\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 13/100: loss: 4.845073223114014, test_auc: 0.5477878330819508\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 14/100: loss: 8.13667106628418, test_auc: 0.4071895424836601\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 15/100: loss: 11.9131498336792, test_auc: 0.3551030668677727\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 16/100: loss: 14.027332305908203, test_auc: 0.32745098039215687\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 17/100: loss: 14.007186889648438, test_auc: 0.3131724484665661\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 18/100: loss: 12.25311279296875, test_auc: 0.3149069884364002\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 19/100: loss: 9.520956993103027, test_auc: 0.3359477124183007\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 20/100: loss: 6.680136203765869, test_auc: 0.3838863750628456\n",
      "Epoch 21/100: loss: 4.457671642303467, test_auc: 0.4780794369029664\n",
      "Epoch 22/100: loss: 3.25471830368042, test_auc: 0.6104826546003017\n",
      "Epoch 23/100: loss: 2.9678807258605957, test_auc: 0.6886877828054299\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 24/100: loss: 3.114941120147705, test_auc: 0.6856963298139769\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 25/100: loss: 3.4064104557037354, test_auc: 0.6760180995475114\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 26/100: loss: 3.7867443561553955, test_auc: 0.6681246857717447\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 27/100: loss: 4.208916187286377, test_auc: 0.659703368526898\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 28/100: loss: 4.5728654861450195, test_auc: 0.6519859225741579\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 29/100: loss: 4.781983852386475, test_auc: 0.646480643539467\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 30/100: loss: 4.783099174499512, test_auc: 0.6464303670186023\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 31/100: loss: 4.579795837402344, test_auc: 0.6506535947712418\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 32/100: loss: 4.2261786460876465, test_auc: 0.6587732528909\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 33/100: loss: 3.8038699626922607, test_auc: 0.6694570135746607\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 34/100: loss: 3.3920018672943115, test_auc: 0.681649069884364\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 35/100: loss: 3.0435047149658203, test_auc: 0.6941930618401206\n",
      "Epoch 36/100: loss: 2.7736949920654297, test_auc: 0.7076671694318754\n",
      "Epoch 37/100: loss: 2.5630264282226562, test_auc: 0.7199346405228759\n",
      "Epoch 38/100: loss: 2.3759894371032715, test_auc: 0.7325289089994973\n",
      "Epoch 39/100: loss: 2.184481143951416, test_auc: 0.744544997486174\n",
      "Epoch 40/100: loss: 1.9785746335983276, test_auc: 0.7576420311714429\n",
      "Epoch 41/100: loss: 1.7636374235153198, test_auc: 0.7710155857214681\n",
      "Epoch 42/100: loss: 1.5525120496749878, test_auc: 0.7835595776772247\n",
      "Epoch 43/100: loss: 1.35836923122406, test_auc: 0.7967320261437909\n",
      "Epoch 44/100: loss: 1.1902904510498047, test_auc: 0.811035696329814\n",
      "Epoch 45/100: loss: 1.0515704154968262, test_auc: 0.8264705882352941\n",
      "Epoch 46/100: loss: 0.9404371976852417, test_auc: 0.8435897435897436\n",
      "Epoch 47/100: loss: 0.8525855541229248, test_auc: 0.8621417797888387\n",
      "Epoch 48/100: loss: 0.7837663888931274, test_auc: 0.8798391151332328\n",
      "Epoch 49/100: loss: 0.7307880520820618, test_auc: 0.8968828557063852\n",
      "Epoch 50/100: loss: 0.6910065412521362, test_auc: 0.9142282554047261\n",
      "Epoch 51/100: loss: 0.6614850759506226, test_auc: 0.9267722473604827\n",
      "Epoch 52/100: loss: 0.6388868093490601, test_auc: 0.937833081950729\n",
      "Epoch 53/100: loss: 0.6202107071876526, test_auc: 0.9458019105077928\n",
      "Epoch 54/100: loss: 0.6036096811294556, test_auc: 0.9478129713423831\n",
      "Epoch 55/100: loss: 0.5885815024375916, test_auc: 0.9479135243841126\n",
      "Epoch 56/100: loss: 0.5754797458648682, test_auc: 0.9465309200603318\n",
      "Epoch 57/100: loss: 0.5647830963134766, test_auc: 0.9453242835595776\n",
      "Epoch 58/100: loss: 0.5566300749778748, test_auc: 0.9441176470588235\n",
      "Epoch 59/100: loss: 0.5507808327674866, test_auc: 0.9432378079436904\n",
      "Epoch 60/100: loss: 0.546791672706604, test_auc: 0.9429361488185017\n",
      "Epoch 61/100: loss: 0.5441704988479614, test_auc: 0.9427853192559075\n",
      "Epoch 62/100: loss: 0.5424805283546448, test_auc: 0.9427350427350427\n",
      "Epoch 63/100: loss: 0.5413853526115417, test_auc: 0.9427853192559075\n",
      "Epoch 64/100: loss: 0.5406411290168762, test_auc: 0.9425842131724484\n",
      "Epoch 65/100: loss: 0.5400766730308533, test_auc: 0.9426596279537456\n",
      "Epoch 66/100: loss: 0.5395782589912415, test_auc: 0.9427853192559075\n",
      "Epoch 67/100: loss: 0.5390836596488953, test_auc: 0.9428607340372047\n",
      "Epoch 68/100: loss: 0.538572371006012, test_auc: 0.9427350427350428\n",
      "Epoch 69/100: loss: 0.5380402207374573, test_auc: 0.9426093514328809\n",
      "Epoch 70/100: loss: 0.5374903082847595, test_auc: 0.942684766214178\n",
      "Epoch 71/100: loss: 0.53694087266922, test_auc: 0.9426596279537457\n",
      "Epoch 72/100: loss: 0.5364173650741577, test_auc: 0.9425339366515838\n",
      "Epoch 73/100: loss: 0.5359358191490173, test_auc: 0.9426093514328809\n",
      "Epoch 74/100: loss: 0.5354984402656555, test_auc: 0.9425087983911513\n",
      "Epoch 75/100: loss: 0.5351008772850037, test_auc: 0.9424585218702866\n",
      "Epoch 76/100: loss: 0.5347444415092468, test_auc: 0.9423831070889895\n",
      "Epoch 77/100: loss: 0.5344340801239014, test_auc: 0.9423579688285572\n",
      "Epoch 78/100: loss: 0.5341665148735046, test_auc: 0.9423579688285572\n",
      "Epoch 79/100: loss: 0.5339295268058777, test_auc: 0.9424585218702867\n",
      "Epoch 80/100: loss: 0.5337148904800415, test_auc: 0.9425842131724486\n",
      "Epoch 81/100: loss: 0.5335219502449036, test_auc: 0.9425590749120162\n",
      "Epoch 82/100: loss: 0.5333504676818848, test_auc: 0.9425842131724486\n",
      "Epoch 83/100: loss: 0.5331980586051941, test_auc: 0.9426093514328809\n",
      "Epoch 84/100: loss: 0.5330615043640137, test_auc: 0.9425842131724486\n",
      "Epoch 85/100: loss: 0.5329375863075256, test_auc: 0.9426093514328809\n",
      "Epoch 86/100: loss: 0.5328232645988464, test_auc: 0.9426847662141781\n",
      "Epoch 87/100: loss: 0.5327173471450806, test_auc: 0.9426596279537457\n",
      "Epoch 88/100: loss: 0.5326202511787415, test_auc: 0.9426596279537457\n",
      "Epoch 89/100: loss: 0.5325313806533813, test_auc: 0.9426847662141781\n",
      "Epoch 90/100: loss: 0.532448410987854, test_auc: 0.9427099044746103\n",
      "Epoch 91/100: loss: 0.532368540763855, test_auc: 0.9426847662141781\n",
      "Epoch 92/100: loss: 0.5322906970977783, test_auc: 0.9426847662141781\n",
      "Epoch 93/100: loss: 0.5322163105010986, test_auc: 0.9426847662141781\n",
      "Epoch 94/100: loss: 0.5321455597877502, test_auc: 0.9426847662141781\n",
      "Epoch 95/100: loss: 0.5320768356323242, test_auc: 0.9426847662141781\n",
      "Epoch 96/100: loss: 0.532008171081543, test_auc: 0.9426847662141781\n",
      "Epoch 97/100: loss: 0.5319388508796692, test_auc: 0.9427099044746104\n",
      "Epoch 98/100: loss: 0.5318701863288879, test_auc: 0.9427099044746104\n",
      "Epoch 99/100: loss: 0.5318027138710022, test_auc: 0.9427099044746104\n",
      "Epoch 100/100: loss: 0.5317358374595642, test_auc: 0.9427099044746104\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "hid_dim = 32\n",
    "decode_layer = 1\n",
    "# domin_recon_model = DOMINANT_recon(hiddle.shape[1], hid_dim, data.x.shape[1], decode_layer, dropout=0.3, act= F.relu)\n",
    "domin_recon_model = DOMINANT_recon(hiddle.shape[1], data.x.shape[1], data.x.shape[1], decode_layer, dropout=0.3, act= F.relu)\n",
    "\n",
    "optimizer = Adam(domin_recon_model.parameters(), lr = 5e-3, weight_decay=5e-4)\n",
    "epochs = 100\n",
    "early_stop = EarlyStopping(patience=20)\n",
    "\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "s = to_dense_adj(data.edge_index)[0]\n",
    "alpha = torch.std(s).detach() / (torch.std(data.x).detach() + torch.std(s).detach())\n",
    "print (f\"Alpha: {alpha}\")\n",
    "\n",
    "# l_weight = [nn.Parameter(torch.tensor(zero2one((1-cosine_distance(hiddle.T)).sum(axis=0))*20, dtype=torch.float32, requires_grad=True))]\n",
    "# l_weight = [nn.Parameter(torch.randn([hiddle.shape[-1]], dtype=torch.float32, requires_grad=True))]\n",
    "l_weight = [nn.Parameter(torch.ones([hiddle.shape[-1]], dtype=torch.float32, requires_grad=True))]\n",
    "b_weight = [nn.Parameter(torch.ones([3], dtype=torch.float32, requires_grad=True))]\n",
    "\n",
    "optimizer_ = Adam(l_weight, lr = 5e-2, weight_decay=5e-2)\n",
    "optimizer_b = Adam(b_weight, lr = 1e-1, weight_decay=5e-2)\n",
    "print (l_weight)\n",
    "def reco_loss_func(x, x_, s, s_):\n",
    "    # attribute reconstruction loss\n",
    "    diff_attribute = torch.pow(x - x_, 2)\n",
    "    attribute_errors = torch.sqrt(torch.sum(diff_attribute, 1))\n",
    "\n",
    "    # structure reconstruction loss\n",
    "    diff_structure = torch.pow(s - s_, 2)\n",
    "    structure_errors = torch.sqrt(torch.sum(diff_structure, 1))\n",
    "\n",
    "    score = alpha * attribute_errors \\\n",
    "            + (1 - alpha) * structure_errors\n",
    "    return score\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hiddle = torch.concat((b_weight[0][0] * feature_normalize(GAT_hid.detach(),axis=0), b_weight[0][1] * feature_normalize(BW_hid.detach(),axis=0), b_weight[0][2] * feature_normalize(hid_dominate.detach(),axis=0)), axis=1)\n",
    "    # hiddle = zero2one((1-cosine_distance(hiddle.T)).mean(axis=0))*hiddle\n",
    "    hiddle_ = torch.mul(hiddle, hiddle.shape[1]*torch.softmax(*l_weight, dim = 0))\n",
    "    # hiddle_ = torch.mul(hiddle, *l_weight)\n",
    "\n",
    "    domin_recon_model.train()\n",
    "    x_, s_, hid  = domin_recon_model(hiddle_, data.edge_index)\n",
    "    \n",
    "    nodes_loss = reco_loss_func(data.x, x_, s, s_)\n",
    "    train_loss = nodes_loss.mean()\n",
    "    nodes_loss_numpy = nodes_loss.detach().numpy()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_.zero_grad()\n",
    "    optimizer_b.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer_.step()\n",
    "    optimizer_b.step()\n",
    "    dropout_auc = roc_auc_score(data.y[data.test_mask].numpy(), nodes_loss_numpy[data.test_mask])\n",
    "    \n",
    "    early_stop(train_loss, domin_recon_model)\n",
    "    if early_stop.early_stop == True:\n",
    "        print (\"Early stopping\")\n",
    "        break\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: loss: {train_loss}, test_auc: {dropout_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([ 0.0080, -0.0008,  0.0118], requires_grad=True)]\n",
      "tensor([0.3339, 0.3310, 0.3351], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print (hiddle.shape[1]*torch.softmax(*l_weight, dim = 0))\n",
    "# hiddle = hid_dominate.detach()\n",
    "# hiddle = BW_hid.detach()\n",
    "print (b_weight)\n",
    "print (torch.softmax(*b_weight, dim = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 标签导向与重构导向，先完成重构导向普遍性的实验\n",
    "\n",
    "### 2. 特征包含无用，冗余以及互补信息。冗余与互补信息通过计算相似度矩阵完成；仍需完善对无用信息针对任务导向的舍弃（考虑通过网络自己学习）\n",
    "\n",
    "### 3. 一个案例实现，完成三个视角的信息融合，实现在生成，最小类以及原生异常数据集上的通用，且性能不差于任何一个已经存在的SOTA\n",
    "\n",
    "### 4. 一般化框架的构建，做出多组数据融合对比实验，实验做到这一步，基本就杀青了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **考虑到，表征学习与重构损失是独立训练的。表征的获取必须完成其他方法的完整运行，如此进行融合学习未免南辕北辙了**\n",
    "\n",
    "* **可以将表征学习与重构损失联合训练训练的。不能，一旦联合就变成 DOMINANTE 模型，任务上就不通用了**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 0.3043\n",
      "Final Test AUC: 0.4690079151617613\n",
      "--------------------- The Embedding of Dominate have done!!! ------------------\n",
      "Epoch 1/100: val_loss: 0.6960328221321106, val_auc: 0.48437074829931975\n",
      "Epoch 2/100: val_loss: 0.6958662867546082, val_auc: 0.4855952380952381\n",
      "Epoch 3/100: val_loss: 0.6957038044929504, val_auc: 0.4876020408163265\n",
      "Epoch 4/100: val_loss: 0.6955432891845703, val_auc: 0.500297619047619\n",
      "Epoch 5/100: val_loss: 0.6953902840614319, val_auc: 0.5039625850340136\n",
      "Epoch 6/100: val_loss: 0.6952409744262695, val_auc: 0.5020918367346939\n",
      "Epoch 7/100: val_loss: 0.6950961947441101, val_auc: 0.5043537414965987\n",
      "Epoch 8/100: val_loss: 0.6949560046195984, val_auc: 0.5217687074829932\n",
      "Epoch 9/100: val_loss: 0.6948179602622986, val_auc: 0.5336904761904762\n",
      "Epoch 10/100: val_loss: 0.6946843862533569, val_auc: 0.5298554421768708\n",
      "Epoch 11/100: val_loss: 0.6945534944534302, val_auc: 0.5292857142857144\n",
      "Epoch 12/100: val_loss: 0.6944274306297302, val_auc: 0.5294387755102041\n",
      "Epoch 13/100: val_loss: 0.6943038105964661, val_auc: 0.5297874149659864\n",
      "Epoch 14/100: val_loss: 0.6941824555397034, val_auc: 0.5301700680272108\n",
      "Epoch 15/100: val_loss: 0.6940671801567078, val_auc: 0.5305612244897958\n",
      "Epoch 16/100: val_loss: 0.6939562559127808, val_auc: 0.5311054421768708\n",
      "Epoch 17/100: val_loss: 0.6938485503196716, val_auc: 0.5312755102040817\n",
      "Epoch 18/100: val_loss: 0.6937434673309326, val_auc: 0.5315136054421769\n",
      "Epoch 19/100: val_loss: 0.6936408281326294, val_auc: 0.5316156462585034\n",
      "Epoch 20/100: val_loss: 0.6935412883758545, val_auc: 0.5320408163265307\n",
      "Epoch 21/100: val_loss: 0.6934409737586975, val_auc: 0.5323639455782314\n",
      "Epoch 22/100: val_loss: 0.693342924118042, val_auc: 0.5328061224489797\n",
      "Epoch 23/100: val_loss: 0.6932480931282043, val_auc: 0.5332993197278912\n",
      "Epoch 24/100: val_loss: 0.6931563019752502, val_auc: 0.5336224489795918\n",
      "Epoch 25/100: val_loss: 0.6930674910545349, val_auc: 0.5342857142857143\n",
      "Epoch 26/100: val_loss: 0.6929823160171509, val_auc: 0.5346598639455783\n",
      "Epoch 27/100: val_loss: 0.6929002404212952, val_auc: 0.5350170068027211\n",
      "Epoch 28/100: val_loss: 0.6928216814994812, val_auc: 0.5351020408163265\n",
      "Epoch 29/100: val_loss: 0.6927487254142761, val_auc: 0.5351190476190477\n",
      "Epoch 30/100: val_loss: 0.6926795840263367, val_auc: 0.535\n",
      "Epoch 31/100: val_loss: 0.6926127672195435, val_auc: 0.5352721088435374\n",
      "Epoch 32/100: val_loss: 0.6925469040870667, val_auc: 0.5357482993197278\n",
      "Epoch 33/100: val_loss: 0.692482590675354, val_auc: 0.5362414965986395\n",
      "Epoch 34/100: val_loss: 0.6924203634262085, val_auc: 0.5365816326530612\n",
      "Epoch 35/100: val_loss: 0.6923603415489197, val_auc: 0.5370493197278912\n",
      "Epoch 36/100: val_loss: 0.6923019289970398, val_auc: 0.5375\n",
      "Epoch 37/100: val_loss: 0.6922439336776733, val_auc: 0.5379336734693877\n",
      "Epoch 38/100: val_loss: 0.6921858787536621, val_auc: 0.5383503401360544\n",
      "Epoch 39/100: val_loss: 0.6921276450157166, val_auc: 0.5387414965986395\n",
      "Epoch 40/100: val_loss: 0.6920680403709412, val_auc: 0.5393197278911565\n",
      "Epoch 41/100: val_loss: 0.6920077800750732, val_auc: 0.5397619047619047\n",
      "Epoch 42/100: val_loss: 0.6919475793838501, val_auc: 0.5403061224489797\n",
      "Epoch 43/100: val_loss: 0.6918874979019165, val_auc: 0.5409098639455783\n",
      "Epoch 44/100: val_loss: 0.6918268203735352, val_auc: 0.5413945578231293\n",
      "Epoch 45/100: val_loss: 0.6917657256126404, val_auc: 0.5423299319727891\n",
      "Epoch 46/100: val_loss: 0.6917039752006531, val_auc: 0.5430612244897959\n",
      "Epoch 47/100: val_loss: 0.6916416883468628, val_auc: 0.5436224489795919\n",
      "Epoch 48/100: val_loss: 0.6915795803070068, val_auc: 0.5444472789115646\n",
      "Epoch 49/100: val_loss: 0.6915172338485718, val_auc: 0.5451700680272109\n",
      "Epoch 50/100: val_loss: 0.6914547681808472, val_auc: 0.5457482993197279\n",
      "Epoch 51/100: val_loss: 0.6913926601409912, val_auc: 0.5462414965986394\n",
      "Epoch 52/100: val_loss: 0.69133061170578, val_auc: 0.546734693877551\n",
      "Epoch 53/100: val_loss: 0.6912687420845032, val_auc: 0.547219387755102\n",
      "Epoch 54/100: val_loss: 0.6912065744400024, val_auc: 0.547704081632653\n",
      "Epoch 55/100: val_loss: 0.6911438703536987, val_auc: 0.5482312925170069\n",
      "Epoch 56/100: val_loss: 0.691080629825592, val_auc: 0.5490136054421768\n",
      "Epoch 57/100: val_loss: 0.6910160779953003, val_auc: 0.5494557823129252\n",
      "Epoch 58/100: val_loss: 0.6909501552581787, val_auc: 0.55\n",
      "Epoch 59/100: val_loss: 0.69088214635849, val_auc: 0.5508163265306123\n",
      "Epoch 60/100: val_loss: 0.6908125281333923, val_auc: 0.5514625850340137\n",
      "Epoch 61/100: val_loss: 0.6907419562339783, val_auc: 0.5523299319727892\n",
      "Epoch 62/100: val_loss: 0.6906698346138, val_auc: 0.5530612244897959\n",
      "Epoch 63/100: val_loss: 0.6905959248542786, val_auc: 0.5538095238095239\n",
      "Epoch 64/100: val_loss: 0.6905198693275452, val_auc: 0.5545918367346939\n",
      "Epoch 65/100: val_loss: 0.6904423236846924, val_auc: 0.5552380952380953\n",
      "Epoch 66/100: val_loss: 0.6903631091117859, val_auc: 0.5561054421768707\n",
      "Epoch 67/100: val_loss: 0.6902817487716675, val_auc: 0.5567687074829931\n",
      "Epoch 68/100: val_loss: 0.6901984214782715, val_auc: 0.5579251700680272\n",
      "Epoch 69/100: val_loss: 0.6901131272315979, val_auc: 0.5589625850340136\n",
      "Epoch 70/100: val_loss: 0.6900259256362915, val_auc: 0.5596938775510204\n",
      "Epoch 71/100: val_loss: 0.689936637878418, val_auc: 0.5609353741496599\n",
      "Epoch 72/100: val_loss: 0.689845085144043, val_auc: 0.5622448979591838\n",
      "Epoch 73/100: val_loss: 0.689751386642456, val_auc: 0.5633843537414965\n",
      "Epoch 74/100: val_loss: 0.6896553635597229, val_auc: 0.5643197278911565\n",
      "Epoch 75/100: val_loss: 0.6895567178726196, val_auc: 0.5652721088435374\n",
      "Epoch 76/100: val_loss: 0.6894550919532776, val_auc: 0.5662925170068027\n",
      "Epoch 77/100: val_loss: 0.6893505454063416, val_auc: 0.5676020408163265\n",
      "Epoch 78/100: val_loss: 0.6892431974411011, val_auc: 0.5690561224489796\n",
      "Epoch 79/100: val_loss: 0.6891326308250427, val_auc: 0.5701700680272108\n",
      "Epoch 80/100: val_loss: 0.6890187859535217, val_auc: 0.5713945578231292\n",
      "Epoch 81/100: val_loss: 0.6889015436172485, val_auc: 0.5722789115646258\n",
      "Epoch 82/100: val_loss: 0.6887809634208679, val_auc: 0.573265306122449\n",
      "Epoch 83/100: val_loss: 0.6886568665504456, val_auc: 0.5747448979591837\n",
      "Epoch 84/100: val_loss: 0.688529372215271, val_auc: 0.5758163265306122\n",
      "Epoch 85/100: val_loss: 0.6883980631828308, val_auc: 0.5769557823129251\n",
      "Epoch 86/100: val_loss: 0.6882628798484802, val_auc: 0.5781632653061225\n",
      "Epoch 87/100: val_loss: 0.6881242990493774, val_auc: 0.5795918367346939\n",
      "Epoch 88/100: val_loss: 0.6879821419715881, val_auc: 0.5809013605442177\n",
      "Epoch 89/100: val_loss: 0.6878358125686646, val_auc: 0.5823299319727891\n",
      "Epoch 90/100: val_loss: 0.6876851916313171, val_auc: 0.5834268707482994\n",
      "Epoch 91/100: val_loss: 0.6875298023223877, val_auc: 0.5848809523809524\n",
      "Epoch 92/100: val_loss: 0.6873698830604553, val_auc: 0.5862925170068027\n",
      "Epoch 93/100: val_loss: 0.6872052550315857, val_auc: 0.5877040816326531\n",
      "Epoch 94/100: val_loss: 0.6870355606079102, val_auc: 0.5891326530612245\n",
      "Epoch 95/100: val_loss: 0.6868603825569153, val_auc: 0.5904591836734694\n",
      "Epoch 96/100: val_loss: 0.6866795420646667, val_auc: 0.5921428571428571\n",
      "Epoch 97/100: val_loss: 0.6864932179450989, val_auc: 0.5935544217687074\n",
      "Epoch 98/100: val_loss: 0.686301052570343, val_auc: 0.5951020408163264\n",
      "Epoch 99/100: val_loss: 0.686103343963623, val_auc: 0.5967346938775511\n",
      "Epoch 100/100: val_loss: 0.6858996152877808, val_auc: 0.598452380952381\n",
      "Final Test Auc: 0.6190501805886421\n",
      "--------------------- The Embedding of BWGNN have done!!! ------------------\n",
      "Epoch 1/100: val_loss: 0.692168116569519, val_auc: 0.8323469387755101\n",
      "Epoch 2/100: val_loss: 0.6911870837211609, val_auc: 0.9022023809523809\n",
      "Epoch 3/100: val_loss: 0.6900129318237305, val_auc: 0.9237244897959184\n",
      "Epoch 4/100: val_loss: 0.688667893409729, val_auc: 0.9310204081632654\n",
      "Epoch 5/100: val_loss: 0.6871971487998962, val_auc: 0.9351190476190476\n",
      "Epoch 6/100: val_loss: 0.6856333017349243, val_auc: 0.9375340136054422\n",
      "Epoch 7/100: val_loss: 0.6840052604675293, val_auc: 0.9389455782312924\n",
      "Epoch 8/100: val_loss: 0.6823282241821289, val_auc: 0.939812925170068\n",
      "Epoch 9/100: val_loss: 0.6805980801582336, val_auc: 0.9407823129251702\n",
      "Epoch 10/100: val_loss: 0.6788182258605957, val_auc: 0.9416496598639456\n",
      "Epoch 11/100: val_loss: 0.6769835352897644, val_auc: 0.9422278911564627\n",
      "Epoch 12/100: val_loss: 0.6750954985618591, val_auc: 0.9427891156462584\n",
      "Epoch 13/100: val_loss: 0.673163115978241, val_auc: 0.9431802721088436\n",
      "Epoch 14/100: val_loss: 0.6711905598640442, val_auc: 0.9434013605442176\n",
      "Epoch 15/100: val_loss: 0.6691762208938599, val_auc: 0.9434693877551021\n",
      "Epoch 16/100: val_loss: 0.6671236753463745, val_auc: 0.9436989795918367\n",
      "Epoch 17/100: val_loss: 0.6650308966636658, val_auc: 0.9437074829931973\n",
      "Epoch 18/100: val_loss: 0.6629031896591187, val_auc: 0.9437925170068027\n",
      "Epoch 19/100: val_loss: 0.6607430577278137, val_auc: 0.9440136054421768\n",
      "Epoch 20/100: val_loss: 0.6585524678230286, val_auc: 0.9440986394557823\n",
      "Epoch 21/100: val_loss: 0.6563297510147095, val_auc: 0.9441496598639456\n",
      "Epoch 22/100: val_loss: 0.65407794713974, val_auc: 0.944251700680272\n",
      "Epoch 23/100: val_loss: 0.6517929434776306, val_auc: 0.9444217687074831\n",
      "Epoch 24/100: val_loss: 0.6494756937026978, val_auc: 0.9443537414965987\n",
      "Epoch 25/100: val_loss: 0.647122323513031, val_auc: 0.9443707482993197\n",
      "Epoch 26/100: val_loss: 0.6447305083274841, val_auc: 0.944421768707483\n",
      "Epoch 27/100: val_loss: 0.6422964930534363, val_auc: 0.944438775510204\n",
      "Epoch 28/100: val_loss: 0.6398182511329651, val_auc: 0.9444387755102042\n",
      "Epoch 29/100: val_loss: 0.6372966766357422, val_auc: 0.9444897959183673\n",
      "Epoch 30/100: val_loss: 0.6347280144691467, val_auc: 0.9445068027210883\n",
      "Epoch 31/100: val_loss: 0.6321080923080444, val_auc: 0.9445408163265306\n",
      "Epoch 32/100: val_loss: 0.6294383406639099, val_auc: 0.9445408163265305\n",
      "Epoch 33/100: val_loss: 0.626727283000946, val_auc: 0.9445408163265306\n",
      "Epoch 34/100: val_loss: 0.6239732503890991, val_auc: 0.9445918367346938\n",
      "Epoch 35/100: val_loss: 0.6211861371994019, val_auc: 0.9446428571428571\n",
      "Epoch 36/100: val_loss: 0.6183595061302185, val_auc: 0.9447448979591837\n",
      "Epoch 37/100: val_loss: 0.6154965162277222, val_auc: 0.9447108843537415\n",
      "Epoch 38/100: val_loss: 0.6126010417938232, val_auc: 0.9447789115646259\n",
      "Epoch 39/100: val_loss: 0.6096787452697754, val_auc: 0.944795918367347\n",
      "Epoch 40/100: val_loss: 0.6067295074462891, val_auc: 0.9447789115646259\n",
      "Epoch 41/100: val_loss: 0.6037551164627075, val_auc: 0.9447959183673469\n",
      "Epoch 42/100: val_loss: 0.6007564663887024, val_auc: 0.9446768707482994\n",
      "Epoch 43/100: val_loss: 0.597726583480835, val_auc: 0.9446598639455783\n",
      "Epoch 44/100: val_loss: 0.594670295715332, val_auc: 0.9445918367346939\n",
      "Epoch 45/100: val_loss: 0.5915977954864502, val_auc: 0.9446088435374149\n",
      "Epoch 46/100: val_loss: 0.5885019898414612, val_auc: 0.9445748299319728\n",
      "Epoch 47/100: val_loss: 0.5853826999664307, val_auc: 0.9445408163265306\n",
      "Epoch 48/100: val_loss: 0.5822434425354004, val_auc: 0.9445408163265305\n",
      "Epoch 49/100: val_loss: 0.5790721774101257, val_auc: 0.9445578231292517\n",
      "Epoch 50/100: val_loss: 0.5758775472640991, val_auc: 0.944608843537415\n",
      "Epoch 51/100: val_loss: 0.5726611614227295, val_auc: 0.9445918367346938\n",
      "Epoch 52/100: val_loss: 0.5694198608398438, val_auc: 0.9446428571428572\n",
      "Epoch 53/100: val_loss: 0.5661594271659851, val_auc: 0.9446598639455782\n",
      "Epoch 54/100: val_loss: 0.5628862977027893, val_auc: 0.9446088435374149\n",
      "Epoch 55/100: val_loss: 0.5595975518226624, val_auc: 0.944608843537415\n",
      "Epoch 56/100: val_loss: 0.5562967658042908, val_auc: 0.9445748299319727\n",
      "Epoch 57/100: val_loss: 0.5529829859733582, val_auc: 0.9445578231292517\n",
      "Epoch 58/100: val_loss: 0.5496582984924316, val_auc: 0.9445748299319727\n",
      "Epoch 59/100: val_loss: 0.5463289022445679, val_auc: 0.9445748299319727\n",
      "Epoch 60/100: val_loss: 0.5429847240447998, val_auc: 0.9445748299319728\n",
      "Epoch 61/100: val_loss: 0.5396272540092468, val_auc: 0.9445408163265306\n",
      "Epoch 62/100: val_loss: 0.5362755060195923, val_auc: 0.9446088435374149\n",
      "Epoch 63/100: val_loss: 0.5329154133796692, val_auc: 0.9445408163265308\n",
      "Epoch 64/100: val_loss: 0.5295582413673401, val_auc: 0.9445578231292517\n",
      "Epoch 65/100: val_loss: 0.5262149572372437, val_auc: 0.9445408163265306\n",
      "Epoch 66/100: val_loss: 0.5228897929191589, val_auc: 0.944438775510204\n",
      "Epoch 67/100: val_loss: 0.5195786952972412, val_auc: 0.9443707482993198\n",
      "Epoch 68/100: val_loss: 0.5162680149078369, val_auc: 0.9443367346938776\n",
      "Epoch 69/100: val_loss: 0.5129619836807251, val_auc: 0.944387755102041\n",
      "Epoch 70/100: val_loss: 0.5096603035926819, val_auc: 0.9443707482993198\n",
      "Epoch 71/100: val_loss: 0.5063656568527222, val_auc: 0.9443707482993198\n",
      "Epoch 72/100: val_loss: 0.5030794143676758, val_auc: 0.9443537414965987\n",
      "Epoch 73/100: val_loss: 0.499787837266922, val_auc: 0.9442687074829932\n",
      "Epoch 74/100: val_loss: 0.49650102853775024, val_auc: 0.94421768707483\n",
      "Epoch 75/100: val_loss: 0.4932163953781128, val_auc: 0.9442687074829932\n",
      "Epoch 76/100: val_loss: 0.48994556069374084, val_auc: 0.944234693877551\n",
      "Epoch 77/100: val_loss: 0.48669707775115967, val_auc: 0.9442176870748299\n",
      "Epoch 78/100: val_loss: 0.48347944021224976, val_auc: 0.944234693877551\n",
      "Epoch 79/100: val_loss: 0.48029640316963196, val_auc: 0.9443197278911565\n",
      "Epoch 80/100: val_loss: 0.4771709740161896, val_auc: 0.9443197278911565\n",
      "Epoch 81/100: val_loss: 0.47406500577926636, val_auc: 0.9442346938775511\n",
      "Epoch 82/100: val_loss: 0.47100022435188293, val_auc: 0.944234693877551\n",
      "Epoch 83/100: val_loss: 0.4679439663887024, val_auc: 0.9442176870748299\n",
      "Epoch 84/100: val_loss: 0.46491560339927673, val_auc: 0.9442006802721088\n",
      "Epoch 85/100: val_loss: 0.4619317054748535, val_auc: 0.9442346938775511\n",
      "Epoch 86/100: val_loss: 0.45895588397979736, val_auc: 0.9442346938775511\n",
      "Epoch 87/100: val_loss: 0.4559979736804962, val_auc: 0.944234693877551\n",
      "Epoch 88/100: val_loss: 0.4530898928642273, val_auc: 0.9441666666666667\n",
      "Epoch 89/100: val_loss: 0.450229674577713, val_auc: 0.9442006802721089\n",
      "Epoch 90/100: val_loss: 0.44740623235702515, val_auc: 0.9442346938775511\n",
      "Epoch 91/100: val_loss: 0.444618284702301, val_auc: 0.9443027210884354\n",
      "Epoch 92/100: val_loss: 0.44187939167022705, val_auc: 0.9443197278911565\n",
      "Epoch 93/100: val_loss: 0.4392043948173523, val_auc: 0.9442687074829932\n",
      "Epoch 94/100: val_loss: 0.4365949332714081, val_auc: 0.9442857142857144\n",
      "Epoch 95/100: val_loss: 0.43402501940727234, val_auc: 0.9442517006802721\n",
      "Epoch 96/100: val_loss: 0.4315204918384552, val_auc: 0.9441836734693877\n",
      "Epoch 97/100: val_loss: 0.4289945662021637, val_auc: 0.9441666666666666\n",
      "Epoch 98/100: val_loss: 0.4265258312225342, val_auc: 0.9441836734693877\n",
      "Epoch 99/100: val_loss: 0.4241415858268738, val_auc: 0.9441326530612245\n",
      "Epoch 100/100: val_loss: 0.421783983707428, val_auc: 0.94406462585034\n",
      "Final Test Auc: 0.9604434027510952\n",
      "--------------------- The Embedding of GAT have done!!! ------------------\n",
      "Alpha: 0.9064301252365112\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.GAT_model import GAT\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from models.BWGNN_model import BWGNN_em\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pygod.models import DOMINANT\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from utils import anomaly_weight, train_for_GCN, feature_fusion\n",
    "\n",
    "data = pyg_dataset(dataset_name=\"citeseer\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "# data2 = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "a_weight = anomaly_weight(data)\n",
    "\n",
    "# data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.2], anomaly_type=\"min\").dataset\n",
    "model = DOMINANT(verbose=True, gpu=-1, epoch=1, lr=1e-3)\n",
    "model = model.fit(data)\n",
    "\n",
    "x_, s_, hid_dom = model.model(data.x, data.edge_index)\n",
    "\n",
    "s = to_dense_adj(data.edge_index)[0]\n",
    "score = model.loss_func(data.x,x_,s,s_)\n",
    "score = score.detach().cpu().numpy()\n",
    "outlier_scores = model.decision_function(data)\n",
    "test_auc_do = roc_auc_score(data.y[data.test_mask].numpy(), score[data.test_mask])\n",
    "print('Final Test AUC:', test_auc_do)\n",
    "print (\"--------------------- The Embedding of Dominate have done!!! ------------------\")\n",
    "\n",
    "\n",
    "number_class = 2\n",
    "hid_dim = 64\n",
    "BWGNN_model = BWGNN_em(data.x.shape[1], 64, number_class, dgl_data)\n",
    "BW_optimizer = Adam(BWGNN_model.parameters(), lr = 1e-4)\n",
    "epochs = 100\n",
    "hid_bw, test_auc_bw, best_auc_bw = train_for_GCN(BWGNN_model, BW_optimizer, data, a_weight, epochs)\n",
    "print (\"--------------------- The Embedding of BWGNN have done!!! ------------------\")\n",
    "\n",
    "\n",
    "hid_dim = 64\n",
    "edge_index = data.edge_index\n",
    "gat_model = GAT(data.x.shape[1], 64, number_class, data)\n",
    "GAT_optimizer = Adam(gat_model.parameters(), lr = 1e-3)\n",
    "epochs = 100\n",
    "hid_gat, test_auc_gat, best_auc_gat = train_for_GCN(gat_model, GAT_optimizer, data, a_weight, epochs)\n",
    "print (\"--------------------- The Embedding of GAT have done!!! ------------------\")\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pygod.models.basic_nn import GCN\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "class DOMINANT_recon(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_dim,\n",
    "                hid_dim,\n",
    "                out_dim,\n",
    "                decoder_layers,\n",
    "                dropout,\n",
    "                act):\n",
    "        super(DOMINANT_recon, self).__init__()\n",
    "\n",
    "        # split the number of layers for the encoder and decoders\n",
    "        self.attr_decoder = GCN(in_channels=in_dim,\n",
    "                                hidden_channels=hid_dim,\n",
    "                                num_layers=decoder_layers,\n",
    "                                out_channels=out_dim,\n",
    "                                dropout=dropout,\n",
    "                                act=act)\n",
    "\n",
    "        self.struct_decoder = GCN(in_channels=in_dim,\n",
    "                                hidden_channels=hid_dim,\n",
    "                                num_layers=decoder_layers,\n",
    "                                out_channels=out_dim,\n",
    "                                dropout=dropout,\n",
    "                                act=act)\n",
    "\n",
    "    def forward(self, h, edge_index):\n",
    "        # decode feature matrix\n",
    "        x_ = self.attr_decoder(h, edge_index)\n",
    "        # print (x_.shape)\n",
    "        # decode adjacency matrix\n",
    "        h_ = self.struct_decoder(h, edge_index)\n",
    "        # print (h_.shape)\n",
    "        s_ = h_ @ h_.T\n",
    "        # return reconstructed matrices\n",
    "        return x_, s_, h\n",
    "\n",
    "def reco_loss_func(x, x_, s, s_):\n",
    "    # attribute reconstruction loss\n",
    "    diff_attribute = torch.pow(x - x_, 2)\n",
    "    attribute_errors = torch.sqrt(torch.sum(diff_attribute, 1))\n",
    "\n",
    "    # structure reconstruction loss\n",
    "    diff_structure = torch.pow(s - s_, 2)\n",
    "    structure_errors = torch.sqrt(torch.sum(diff_structure, 1))\n",
    "\n",
    "    score = alpha * attribute_errors \\\n",
    "            + (1 - alpha) * structure_errors\n",
    "    return score\n",
    "\n",
    "s = to_dense_adj(data.edge_index)[0]\n",
    "alpha = torch.std(s).detach() / (torch.std(data.x).detach() + torch.std(s).detach())\n",
    "print (f\"Alpha: {alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 1/75: val_loss: 0.6857784986495972, val_auc: 0.875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 2/75: val_loss: 0.6829900145530701, val_auc: 0.875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 3/75: val_loss: 0.6806588768959045, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 4/75: val_loss: 0.678452730178833, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 5/75: val_loss: 0.6762498021125793, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 6/75: val_loss: 0.6741844415664673, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 7/75: val_loss: 0.6719791889190674, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 8/75: val_loss: 0.6694870591163635, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 9/75: val_loss: 0.6669013500213623, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 10/75: val_loss: 0.6642493009567261, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 11/75: val_loss: 0.6613300442695618, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 12/75: val_loss: 0.6583888530731201, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 13/75: val_loss: 0.6551105976104736, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 14/75: val_loss: 0.6511895656585693, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 15/75: val_loss: 0.6467039585113525, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 16/75: val_loss: 0.641365647315979, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 17/75: val_loss: 0.6353943347930908, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 18/75: val_loss: 0.6289992928504944, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 19/75: val_loss: 0.6222814321517944, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 20/75: val_loss: 0.6146273016929626, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 21/75: val_loss: 0.6060010194778442, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 22/75: val_loss: 0.5970242619514465, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 23/75: val_loss: 0.5879506468772888, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 24/75: val_loss: 0.578117847442627, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 25/75: val_loss: 0.5672570466995239, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 26/75: val_loss: 0.5554264187812805, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 27/75: val_loss: 0.5424264669418335, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 28/75: val_loss: 0.5287145376205444, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 29/75: val_loss: 0.5135330557823181, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 30/75: val_loss: 0.4969485402107239, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 31/75: val_loss: 0.4793747067451477, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 32/75: val_loss: 0.46064677834510803, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 33/75: val_loss: 0.44076257944107056, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 34/75: val_loss: 0.4196135401725769, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 35/75: val_loss: 0.39724892377853394, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 36/75: val_loss: 0.3736407160758972, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 37/75: val_loss: 0.3491073548793793, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 38/75: val_loss: 0.32398903369903564, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 39/75: val_loss: 0.2986918091773987, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 40/75: val_loss: 0.27328312397003174, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 41/75: val_loss: 0.24796760082244873, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 42/75: val_loss: 0.22337795794010162, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 43/75: val_loss: 0.19973556697368622, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 44/75: val_loss: 0.17733685672283173, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 45/75: val_loss: 0.15615537762641907, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 46/75: val_loss: 0.13631021976470947, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 47/75: val_loss: 0.1178898960351944, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 48/75: val_loss: 0.10113397240638733, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 49/75: val_loss: 0.08592981845140457, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 50/75: val_loss: 0.07229869812726974, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 51/75: val_loss: 0.060298461467027664, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 52/75: val_loss: 0.050015583634376526, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 53/75: val_loss: 0.041257016360759735, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 54/75: val_loss: 0.03385934978723526, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 55/75: val_loss: 0.02767307311296463, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 56/75: val_loss: 0.022595839574933052, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 57/75: val_loss: 0.01841610111296177, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 58/75: val_loss: 0.014980202540755272, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 59/75: val_loss: 0.012195074930787086, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 60/75: val_loss: 0.00990462675690651, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 61/75: val_loss: 0.008043401874601841, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 62/75: val_loss: 0.00656207837164402, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 63/75: val_loss: 0.005379087291657925, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 64/75: val_loss: 0.004427372943609953, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 65/75: val_loss: 0.0036650807596743107, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 66/75: val_loss: 0.0030434629879891872, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 67/75: val_loss: 0.0025341741275042295, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 68/75: val_loss: 0.0021247081458568573, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 69/75: val_loss: 0.0017950329929590225, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 70/75: val_loss: 0.0015278473729267716, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 71/75: val_loss: 0.0013077643234282732, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 72/75: val_loss: 0.0011257383739575744, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 73/75: val_loss: 0.0009738308726809919, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 74/75: val_loss: 0.000846907205414027, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 75/75: val_loss: 0.0007412417908199131, val_auc: 1.0\n",
      "Final Test Auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 1/75: val_loss: 0.7173371911048889, val_auc: 0.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 2/75: val_loss: 0.7196869254112244, val_auc: 0.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 3/75: val_loss: 0.7223286628723145, val_auc: 0.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 4/75: val_loss: 0.7254649996757507, val_auc: 0.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 5/75: val_loss: 0.7292212247848511, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 6/75: val_loss: 0.73365318775177, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 7/75: val_loss: 0.738821268081665, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 8/75: val_loss: 0.7445828914642334, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 9/75: val_loss: 0.7511301636695862, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 10/75: val_loss: 0.7586314678192139, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 11/75: val_loss: 0.7667751312255859, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 12/75: val_loss: 0.7750863432884216, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 13/75: val_loss: 0.7837927937507629, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 14/75: val_loss: 0.7931740283966064, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 15/75: val_loss: 0.8036245703697205, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 16/75: val_loss: 0.814794659614563, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 17/75: val_loss: 0.8274908661842346, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 18/75: val_loss: 0.8417166471481323, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 19/75: val_loss: 0.8577435612678528, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 20/75: val_loss: 0.8757601380348206, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Final Test Auc: 0.5\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 1/75: val_loss: 0.7023745775222778, val_auc: 0.1875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 2/75: val_loss: 0.7007881999015808, val_auc: 0.1875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 3/75: val_loss: 0.6989425420761108, val_auc: 0.1875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 4/75: val_loss: 0.6968678832054138, val_auc: 0.1875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 5/75: val_loss: 0.6946882009506226, val_auc: 0.25\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 6/75: val_loss: 0.6925795078277588, val_auc: 0.3125\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 7/75: val_loss: 0.6902116537094116, val_auc: 0.375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 8/75: val_loss: 0.6878561973571777, val_auc: 0.375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 9/75: val_loss: 0.6854346394538879, val_auc: 0.4375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 10/75: val_loss: 0.6824696063995361, val_auc: 0.625\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 11/75: val_loss: 0.6790603995323181, val_auc: 0.6875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 12/75: val_loss: 0.675287127494812, val_auc: 0.6875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 13/75: val_loss: 0.6712332367897034, val_auc: 0.6875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 14/75: val_loss: 0.6673741936683655, val_auc: 0.8125\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 15/75: val_loss: 0.6632860898971558, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 16/75: val_loss: 0.6587267518043518, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 17/75: val_loss: 0.6535986065864563, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 18/75: val_loss: 0.6481794714927673, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 19/75: val_loss: 0.6424219608306885, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 20/75: val_loss: 0.6360820531845093, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 21/75: val_loss: 0.6290805339813232, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 22/75: val_loss: 0.6213464736938477, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 23/75: val_loss: 0.6128398180007935, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 24/75: val_loss: 0.6037960648536682, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 25/75: val_loss: 0.5937603712081909, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 26/75: val_loss: 0.5826250314712524, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 27/75: val_loss: 0.5705031156539917, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 28/75: val_loss: 0.5573471784591675, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 29/75: val_loss: 0.5434029698371887, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 30/75: val_loss: 0.5283665657043457, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 31/75: val_loss: 0.5123530626296997, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 32/75: val_loss: 0.49492430686950684, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 33/75: val_loss: 0.47622767090797424, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 34/75: val_loss: 0.4563230872154236, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 35/75: val_loss: 0.43561896681785583, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 36/75: val_loss: 0.41390612721443176, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 37/75: val_loss: 0.39119112491607666, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 38/75: val_loss: 0.367916464805603, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 39/75: val_loss: 0.34443676471710205, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 40/75: val_loss: 0.32005441188812256, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 41/75: val_loss: 0.29703187942504883, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 42/75: val_loss: 0.27342790365219116, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 43/75: val_loss: 0.2497953176498413, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 44/75: val_loss: 0.22680240869522095, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 45/75: val_loss: 0.20450234413146973, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 46/75: val_loss: 0.18298734724521637, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 47/75: val_loss: 0.16196772456169128, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 48/75: val_loss: 0.14192678034305573, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 49/75: val_loss: 0.12329747527837753, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 50/75: val_loss: 0.1080273985862732, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 51/75: val_loss: 0.09374086558818817, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 52/75: val_loss: 0.08048445731401443, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 53/75: val_loss: 0.06852786242961884, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 54/75: val_loss: 0.0578998401761055, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 55/75: val_loss: 0.04854195937514305, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 56/75: val_loss: 0.04042051360011101, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 57/75: val_loss: 0.03345540910959244, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 58/75: val_loss: 0.027509324252605438, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 59/75: val_loss: 0.022510051727294922, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 60/75: val_loss: 0.0183608066290617, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 61/75: val_loss: 0.014940930530428886, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 62/75: val_loss: 0.012127122841775417, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 63/75: val_loss: 0.009823773987591267, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 64/75: val_loss: 0.007958712056279182, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 65/75: val_loss: 0.00645794253796339, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 66/75: val_loss: 0.0052513256669044495, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 67/75: val_loss: 0.0042822095565497875, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 68/75: val_loss: 0.0035041666124016047, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 69/75: val_loss: 0.002878662897273898, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 70/75: val_loss: 0.0023743947967886925, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 71/75: val_loss: 0.0019659569952636957, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 72/75: val_loss: 0.0016354667022824287, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 73/75: val_loss: 0.0013676712987944484, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 74/75: val_loss: 0.0011509560281410813, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 75/75: val_loss: 0.0009732411708682775, val_auc: 1.0\n",
      "Final Test Auc: 1.0\n",
      "[1.0, 0.5, 1.0] \n",
      " Test mean: 0.8333333333333334 \n",
      " Test std: 0.23570226039551584\n",
      "[1.0, 0.09523809523809523, 1.0] \n",
      " Val best: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from models.GAT_model import GAT\n",
    "from models.GCN_model import GCN\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from models.BWGNN_model import BWGNN_em\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pygod.models import DOMINANT\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from utils import *\n",
    "\n",
    "torch.manual_seed(21)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def train_for_mul_model(model_list, optimizer_list, linear_model, linear_optimizer, data, a_weight, epochs, b_weight, b_optimizer=None):\n",
    "    \"\"\"Train for multi model togeother. Like GAT, GCN, BWGNN or anyelse. \n",
    "    Args: \n",
    "        b_weight: option for numbers list or learnable parameters\n",
    "        b_optimizer: None if b_weight is numbers list, real torch optimizer if b_weight belongs to the learnable parameters.\n",
    "    \"\"\"\n",
    "    best_val_auc = 0\n",
    "    early_stop = EarlyStopping(patience=20)\n",
    "    for epoch in range(epochs):\n",
    "        print (b_weight)\n",
    "        hiddle_list = []\n",
    "        for pos, model in enumerate(model_list):\n",
    "            model.train()\n",
    "            logits, hid = model(data.x)\n",
    "            hid = hid * b_weight[0][pos]\n",
    "            # if pos != 2:\n",
    "                # hid = hid * 0\n",
    "            hiddle_list.append(hid)\n",
    "        # 特征融合以及特征学习\n",
    "        hiddle = torch.concat(hiddle_list, axis=1)\n",
    "        hiddle = zero2one((1-cosine_distance(hiddle.T)).mean(axis=0))*hiddle\n",
    "        \n",
    "        logits = linear_model(hiddle, data.edge_index)\n",
    "        train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "        \n",
    "        for optimizer in optimizer_list:\n",
    "            optimizer.zero_grad()\n",
    "        linear_optimizer.zero_grad()\n",
    "        if b_optimizer != None:\n",
    "            b_optimizer.zero_grad()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        \n",
    "        for optimizer in optimizer_list:\n",
    "            optimizer.step()\n",
    "        linear_optimizer.step()\n",
    "        if b_optimizer != None:\n",
    "            b_optimizer.step()\n",
    "\n",
    "        hiddle_list = []\n",
    "        for model in model_list:\n",
    "            model.eval()\n",
    "            logits, hid = model(data.x)\n",
    "            hiddle_list.append(hid)\n",
    "        # 特征融合以及特征学习\n",
    "        hiddle = torch.concat(hiddle_list, axis=1)\n",
    "        logits = linear_model(hiddle, data.edge_index)\n",
    "\n",
    "        val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "        probs = logits.softmax(1)\n",
    "        \n",
    "        auc = roc_auc_score(data.y[data.val_mask].cpu().numpy(), probs[data.val_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "             roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "        \n",
    "        if auc >= best_val_auc:\n",
    "            best_val_auc = auc\n",
    "\n",
    "        early_stop(val_loss, model)\n",
    "        if early_stop.early_stop == True:\n",
    "            print (\"Early stopping\")\n",
    "            break\n",
    "        print (f\"Epoch {epoch+1}/{epochs}: val_loss: {val_loss}, val_auc: {auc}\")\n",
    "    hiddle_list = []\n",
    "    for model in model_list:\n",
    "        model.eval()\n",
    "        logits, hid = model(data.x)\n",
    "        hiddle_list.append(hid)\n",
    "    # 特征融合以及特征学习\n",
    "    hiddle = torch.concat(hiddle_list, axis=1)\n",
    "    logits = linear_model(hiddle, data.edge_index)\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.test_mask].cpu().numpy(), probs[data.test_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "             roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "    print (f\"Final Test Auc: {auc}\")\n",
    "    return hid, auc, best_val_auc\n",
    "\n",
    "def train_for_param(data_name):\n",
    "    parameter_list = np.linspace(start=0,stop=1,num=1)\n",
    "    param2performance_list = []\n",
    "    for param1 in parameter_list:\n",
    "        param2 = 1 - param1\n",
    "\n",
    "        # run five times to get mean and std for test. best performance for val.\n",
    "        run_times = 3\n",
    "        test_auc_mean = []\n",
    "        test_auc_std = []\n",
    "        val_auc_best = []\n",
    "        test_list = []\n",
    "        val_list = []\n",
    "        for i in range(run_times):\n",
    "            np.random.seed(i*2)\n",
    "            # data = pyg_dataset(dataset_name=data_name, dataset_spilt=[0.4,0.29,0.3]).dataset\n",
    "            data = pyg_dataset(dataset_name=data_name, dataset_spilt=[0.4,0.29,0.3], anomaly_type=\"min\").dataset\n",
    "            # data2 = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "            dgl_data = pyg_to_dgl(data)\n",
    "            data = data.to(device)\n",
    "            dgl_data = dgl_data.to(device)\n",
    "\n",
    "            a_weight = anomaly_weight(data)\n",
    "\n",
    "            epochs = 75\n",
    "            hid_dim = 64\n",
    "            number_class = 2\n",
    "            gcn_model = GCN(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "            gat_model = GAT(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "            bw_model = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data).to(device)\n",
    "            model_list = [gcn_model, gat_model, bw_model]\n",
    "            cla_model = GCNConv(hid_dim*len(model_list), number_class).to(device)\n",
    "\n",
    "            # l_weight = [nn.Parameter(torch.ones([hid_dom.shape[-1] * feature_length], dtype=torch.float32, requires_grad=True))]\n",
    "            # b_weight = [nn.Parameter(torch.ones([len(model_list)], dtype=torch.float32, requires_grad=True))]\n",
    "            b_weight = [[0.0, param1, param2]]\n",
    "            # b_optimizer = Adam(b_weight, lr = 1e-2, weight_decay=5e-2)\n",
    "            # l_optimizer_ = Adam(l_weight, lr = 5e-2, weight_decay=5e-2)\n",
    "\n",
    "            gcn_optimizer = Adam(gcn_model.parameters(), lr = 1e-3)\n",
    "            gat_optimizer = Adam(gat_model.parameters(), lr = 1e-3)\n",
    "            bw_optimizer = Adam(bw_model.parameters(), lr = 1e-3)\n",
    "            cla_optimizer = Adam(cla_model.parameters(), lr = 1e-3)\n",
    "\n",
    "            optimizer_list = [gcn_optimizer,gat_optimizer,bw_optimizer]    \n",
    "            _, auc, best_val_auc = train_for_mul_model(model_list, optimizer_list, cla_model, cla_optimizer, data, a_weight, epochs, b_weight)\n",
    "            \n",
    "            test_list.append(auc)\n",
    "            val_list.append(best_val_auc)\n",
    "        \n",
    "        test_auc_mean.append(np.array(test_list).mean())\n",
    "        test_auc_std.append(np.array(test_list).std())\n",
    "        val_auc_best.append(np.array(val_list).max())\n",
    "\n",
    "        param2performance_list.append([param1, param2, test_auc_mean[-1], test_auc_std[-1], val_auc_best[-1]])\n",
    "        print (test_list,f\"\\n Test mean: {np.array(test_list).mean()}\",f\"\\n Test std: {np.array(test_list).std()}\")\n",
    "        print (val_list, f\"\\n Val best: {np.array(val_list).max()}\")\n",
    "        \n",
    "    np.savetxt(f\"./result/param2performance_{data_name}_gatbw.txt\", np.array(param2performance_list))\n",
    "\n",
    "dataset_ava_list = [\"karate\"]\n",
    "for data_name in dataset_ava_list:\n",
    "    train_for_param(data_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如上是遍历参数空间\n",
    "### 如下是使用自学习参数，对表征数据进行融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)]\n",
      "Epoch 1/10: val_loss: 11.458383560180664, val_auc: 0.29654899244159727\n",
      "[Parameter containing:\n",
      "tensor([1.0100, 0.9900, 1.0100], requires_grad=True)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9444\\2017241959.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mcla_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcla_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0moptimizer_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgcn_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgat_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_optimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_val_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_for_mul_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcla_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcla_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mtest_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9444\\2513425722.py\u001b[0m in \u001b[0;36mtrain_for_mul_model\u001b[1;34m(model_list, optimizer_list, linear_model, linear_optimizer, data, a_weight, epochs, b_weight, b_optimizer)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mb_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moptimizer_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pygod2\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pygod2\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pygod2\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mBackwardCFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FunctionBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFunctionCtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_HookMixin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[1;31m# _forward_cls is defined by derived class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;31m# The user should define either backward or vjp but never both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_times = 1\n",
    "test_auc_mean = []\n",
    "test_auc_std = []\n",
    "val_auc_best = []\n",
    "test_list = []\n",
    "val_list = []\n",
    "device = torch.device(\"cuda\")\n",
    "# param2performance_list = []\n",
    "for i in range(run_times):\n",
    "    np.random.seed(3)\n",
    "    data = pyg_dataset(dataset_name=\"weibo\", dataset_spilt=[0.4,0.29,0.3], anomaly_type=\"min\").dataset\n",
    "    # data2 = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "    dgl_data = pyg_to_dgl(data)\n",
    "    a_weight = anomaly_weight(data)\n",
    "\n",
    "    epochs = 10\n",
    "    hid_dim = 64\n",
    "    number_class = 2\n",
    "    edge_index = data.edge_index\n",
    "    gcn_model = GCN(data.x.shape[1], hid_dim, number_class, data)\n",
    "    gat_model = GAT(data.x.shape[1], hid_dim, number_class, data)\n",
    "    bw_model = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data)\n",
    "    model_list = [gcn_model,gat_model,bw_model]\n",
    "    cla_model = GCNConv(hid_dim*len(model_list), number_class)\n",
    "\n",
    "    # l_weight = [nn.Parameter(torch.ones([hid_dom.shape[-1] * feature_length], dtype=torch.float32, requires_grad=True))]\n",
    "    b_weight = [nn.Parameter(torch.ones([len(model_list)], dtype=torch.float32, requires_grad=True))]\n",
    "    # b_weight = [[0.0, param1, param2]]\n",
    "    b_optimizer = Adam(b_weight, lr = 1e-2, weight_decay=5e-2)\n",
    "    \n",
    "    # l_optimizer_ = Adam(l_weight, lr = 5e-2, weight_decay=5e-2)\n",
    "\n",
    "    gcn_optimizer = Adam(gcn_model.parameters(), lr = 1e-3)\n",
    "    gat_optimizer = Adam(gat_model.parameters(), lr = 1e-3)\n",
    "    bw_optimizer = Adam(bw_model.parameters(), lr = 1e-3)\n",
    "    cla_optimizer = Adam(cla_model.parameters(), lr = 1e-3)\n",
    "    optimizer_list = [gcn_optimizer,gat_optimizer,bw_optimizer]    \n",
    "    _, auc, best_val_auc = train_for_mul_model(model_list, optimizer_list, cla_model, cla_optimizer, data, a_weight, epochs, b_weight, b_optimizer)\n",
    "    \n",
    "    test_list.append(auc)\n",
    "    val_list.append(best_val_auc)\n",
    "\n",
    "test_auc_mean.append(np.array(test_list).mean())\n",
    "test_auc_std.append(np.array(test_list).std())\n",
    "val_auc_best.append(np.array(val_list).max())\n",
    "\n",
    "# param2performance_list.append([param1, param2, test_auc_mean[-1], test_auc_std[-1], val_auc_best[-1]])\n",
    "print (test_list,f\"\\n Test mean: {np.array(test_list).mean()}\",f\"\\n Test std: {np.array(test_list).std()}\")\n",
    "print (val_list, f\"\\n Val best: {np.array(val_list).max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design for reconstrcture-oriented method, Dominant and AnomalyDae. \n",
    "\n",
    "* Fusion training\n",
    "\n",
    "* iteratering all fusion parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly syntheic is on processing\n",
      "using 0.06 seconds\n",
      "Epoch 0000: Loss 0.3074\n",
      "Epoch 0001: Loss 0.3120\n",
      "Epoch 0002: Loss 0.3053\n",
      "Epoch 0003: Loss 0.3049\n",
      "Epoch 0004: Loss 0.3071\n",
      "Epoch 0000: Loss 2.9281\n",
      "Epoch 0001: Loss 2.8720\n",
      "Epoch 0002: Loss 2.8381\n",
      "Epoch 0003: Loss 2.8277\n",
      "Epoch 0004: Loss 2.8327\n",
      "Final Test AUC: 0.8643752190676481\n",
      "--------------------- The Embedding of Dominate have done!!! ------------------\n",
      "Final Test AUC: 0.7123860848229933\n",
      "--------------------- The Embedding of Dominate have done!!! ------------------\n"
     ]
    }
   ],
   "source": [
    "# Anomaly detection using autoencoders with nonlinear dimensionality reduction\n",
    "from pygod.models import DOMINANT, AnomalyDAE\n",
    "\n",
    "data = pyg_dataset(dataset_name=\"citeseer\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"syn\").dataset\n",
    "# data2 = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "a_weight = anomaly_weight(data)\n",
    "\n",
    "# data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.2], anomaly_type=\"min\").dataset\n",
    "do_model = DOMINANT(verbose=True, gpu=-1, epoch=5, lr=1e-3)\n",
    "do_model = do_model.fit(data)\n",
    "\n",
    "dae_model = AnomalyDAE(verbose=True, gpu=-1, epoch=5, lr=1e-3, batch_size=0)\n",
    "dae_model = dae_model.fit(data)\n",
    "\n",
    "x_, s_, hid_dom = do_model.model(data.x, data.edge_index)\n",
    "s = to_dense_adj(data.edge_index)[0]\n",
    "score = do_model.loss_func(data.x,x_,s,s_)\n",
    "score = score.detach().cpu().numpy()\n",
    "outlier_scores = do_model.decision_function(data)\n",
    "test_auc_do = roc_auc_score(data.y[data.test_mask].numpy(), score[data.test_mask])\n",
    "print('Final Test AUC:', test_auc_do)\n",
    "print (\"--------------------- The Embedding of Dominate have done!!! ------------------\")\n",
    "\n",
    "x_, s_ = dae_model.model(data.x, data.edge_index, batch_size=data.x.shape[0])\n",
    "score = dae_model.loss_func(data.x,x_,s,s_)\n",
    "score = score.detach().cpu().numpy()\n",
    "outlier_scores = dae_model.decision_function(data)\n",
    "test_auc_do = roc_auc_score(data.y[data.test_mask].numpy(), score[data.test_mask])\n",
    "print('Final Test AUC:', test_auc_do)\n",
    "print (\"--------------------- The Embedding of Dominate have done!!! ------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new idea from deconpling representation learning. Test decoupling learning on GAT, BWGNN, GCN with the SSL loss function DCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\AI Learning\\Mul-Graph_baseline\\dataset.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.dataset = Data(x=temp.x, edge_index=temp.edge_index,y=torch.tensor(temp.y, dtype=torch.long),train_mask=position,val_mask=position,test_mask=position)\n",
      "g:\\AI Learning\\Mul-Graph_baseline\\dataset.py:103: UserWarning: Anomaly is min class of dataset and anomaly rate is not conformed to setting\n",
      "  warnings.warn(f\"Anomaly is min class of dataset and anomaly rate is not conformed to setting\")\n",
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\dgl\\heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/30, loss: 0.6849053502082825\n",
      "epoch: 2/30, loss: 0.5997412204742432\n",
      "epoch: 3/30, loss: 0.5254267454147339\n",
      "epoch: 4/30, loss: 0.4518432319164276\n",
      "epoch: 5/30, loss: 0.3799225687980652\n",
      "epoch: 6/30, loss: 0.31303009390830994\n",
      "epoch: 7/30, loss: 0.2539626657962799\n",
      "epoch: 8/30, loss: 0.20350222289562225\n",
      "epoch: 9/30, loss: 0.16036009788513184\n",
      "epoch: 10/30, loss: 0.12313470244407654\n",
      "epoch: 11/30, loss: 0.09116248041391373\n",
      "epoch: 12/30, loss: 0.06474782526493073\n",
      "epoch: 13/30, loss: 0.04359951987862587\n",
      "epoch: 14/30, loss: 0.027867743745446205\n",
      "epoch: 15/30, loss: 0.01692819595336914\n",
      "epoch: 16/30, loss: 0.0098188491538167\n",
      "epoch: 17/30, loss: 0.0054903775453567505\n",
      "epoch: 18/30, loss: 0.0029963827691972256\n",
      "epoch: 19/30, loss: 0.0016164698172360659\n",
      "epoch: 20/30, loss: 0.0008717927848920226\n",
      "epoch: 21/30, loss: 0.00047432718565687537\n",
      "epoch: 22/30, loss: 0.00026286402135156095\n",
      "epoch: 23/30, loss: 0.00014828865823801607\n",
      "epoch: 24/30, loss: 8.568770863348618e-05\n",
      "epoch: 25/30, loss: 5.082969437353313e-05\n",
      "epoch: 26/30, loss: 3.0990951927378774e-05\n",
      "epoch: 27/30, loss: 1.9434864952927455e-05\n",
      "epoch: 28/30, loss: 1.2535439964267425e-05\n",
      "epoch: 29/30, loss: 8.312988029501867e-06\n",
      "epoch: 30/30, loss: 5.6628696256666444e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from models.GAT_model import GAT\n",
    "from models.GCN_model import GCN\n",
    "from models.GIN_model import GIN\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from models.BWGNN_model import BWGNN_em\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from models.dci import DCI_loss\n",
    "from utils import *\n",
    "\n",
    "torch.manual_seed(21)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "data_name = \"reddit\"\n",
    "\n",
    "# run five times to get mean and std for test. best performance for val.\n",
    "test_auc_mean = []\n",
    "test_auc_std = []\n",
    "val_auc_best = []\n",
    "test_list = []\n",
    "val_list = []\n",
    "\n",
    "np.random.seed(2)\n",
    "# dataset preparing\n",
    "data = pyg_dataset(dataset_name=data_name, dataset_spilt=[0.4,0.29,0.3], anomaly_type=\"min\").dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "data = data.to(device)\n",
    "dgl_data = dgl_data.to(device)\n",
    "a_weight = anomaly_weight(data)\n",
    "\n",
    "epochs = 30\n",
    "hid_dim = 64\n",
    "number_class = 2\n",
    "recluster_interval = 10\n",
    "kmeans = KMeans(n_clusters=number_class, random_state=0).fit(data.x)\n",
    "ss_label = kmeans.labels_\n",
    "cluster_info = [list(np.where(ss_label==i)[0]) for i in range(number_class)]\n",
    "idx = np.random.permutation(data.x.shape[0])\n",
    "shuf_feats = data.x[idx, :]\n",
    "\n",
    "gcn_model1 = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data).to(device)\n",
    "gcn_model2 = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data).to(device)\n",
    "# GCN(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "loss_dci = DCI_loss(hid_dim, device)\n",
    "gcn_optimizer = Adam([{\"params\": gcn_model1.parameters(), \"lr\": 5e-3},\\\n",
    "                        {\"params\": gcn_model2.parameters(), \"lr\": 5e-3}])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    gcn_model1.train()\n",
    "    gcn_model2.train()\n",
    "    _, hid1 = gcn_model1(data.x)\n",
    "    _, hid2 = gcn_model2(shuf_feats)\n",
    "    train_loss = loss_dci(hid1, hid2, None, None, None, cluster_info, number_class)\n",
    "\n",
    "    gcn_optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    gcn_optimizer.step()\n",
    "    print (f\"epoch: {epoch + 1}/{epochs}, loss: {train_loss}\")\n",
    "    # re-clustering\n",
    "    if epoch % recluster_interval == 0:\n",
    "        gcn_model1.eval()\n",
    "        _, emb = gcn_model1(data.x)\n",
    "        kmeans = KMeans(n_clusters=number_class, random_state=0).fit(emb.detach().cpu().numpy())\n",
    "        ss_label = kmeans.labels_\n",
    "        cluster_info = [list(np.where(ss_label==i)[0]) for i in range(number_class)]\n",
    "\n",
    "cls_model = Linear(in_features=hid_dim, out_features=number_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: val_loss: 0.6682317852973938, val_auc: 0.5756489563567362\n",
      "Epoch 2/50: val_loss: 0.668691098690033, val_auc: 0.5770986717267552\n",
      "Epoch 3/50: val_loss: 0.6908826231956482, val_auc: 0.5781935483870968\n",
      "Epoch 4/50: val_loss: 0.7352685928344727, val_auc: 0.5794402277039848\n",
      "Epoch 5/50: val_loss: 0.7459756731987, val_auc: 0.5786356736242884\n",
      "Epoch 6/50: val_loss: 0.7277727723121643, val_auc: 0.579314990512334\n",
      "Epoch 7/50: val_loss: 0.7037367224693298, val_auc: 0.5785901328273244\n",
      "Epoch 8/50: val_loss: 0.6855451464653015, val_auc: 0.5789506641366224\n",
      "Epoch 9/50: val_loss: 0.6747745275497437, val_auc: 0.5784800759013283\n",
      "Epoch 10/50: val_loss: 0.6692759990692139, val_auc: 0.5781043643263757\n",
      "Epoch 11/50: val_loss: 0.666764497756958, val_auc: 0.5780702087286528\n",
      "Epoch 12/50: val_loss: 0.6657822132110596, val_auc: 0.5782296015180266\n",
      "Epoch 13/50: val_loss: 0.6656140685081482, val_auc: 0.5787836812144213\n",
      "Epoch 14/50: val_loss: 0.6660711169242859, val_auc: 0.5794364326375712\n",
      "Epoch 15/50: val_loss: 0.667267382144928, val_auc: 0.5791100569259962\n",
      "Epoch 16/50: val_loss: 0.6694085001945496, val_auc: 0.5799146110056925\n",
      "Epoch 17/50: val_loss: 0.6726173758506775, val_auc: 0.5787039848197344\n",
      "Epoch 18/50: val_loss: 0.6768320798873901, val_auc: 0.5788216318785578\n",
      "Epoch 19/50: val_loss: 0.6817041635513306, val_auc: 0.5795806451612903\n",
      "Epoch 20/50: val_loss: 0.6865946650505066, val_auc: 0.5799146110056926\n",
      "Epoch 21/50: val_loss: 0.690647304058075, val_auc: 0.579011385199241\n",
      "Epoch 22/50: val_loss: 0.6929910778999329, val_auc: 0.578988614800759\n",
      "Epoch 23/50: val_loss: 0.6930824518203735, val_auc: 0.5790493358633776\n",
      "Epoch 24/50: val_loss: 0.6909826397895813, val_auc: 0.5790645161290323\n",
      "Epoch 25/50: val_loss: 0.6873656511306763, val_auc: 0.5789848197343453\n",
      "Epoch 26/50: val_loss: 0.6832177639007568, val_auc: 0.5800398481973434\n",
      "Epoch 27/50: val_loss: 0.6794247031211853, val_auc: 0.5799753320683112\n",
      "Epoch 28/50: val_loss: 0.6765096187591553, val_auc: 0.5798500948766603\n",
      "Epoch 29/50: val_loss: 0.6746499538421631, val_auc: 0.5794478178368121\n",
      "Epoch 30/50: val_loss: 0.673764705657959, val_auc: 0.5791290322580646\n",
      "Epoch 31/50: val_loss: 0.6737037897109985, val_auc: 0.5787229601518027\n",
      "Epoch 32/50: val_loss: 0.6743978261947632, val_auc: 0.5785294117647058\n",
      "Epoch 33/50: val_loss: 0.6757981777191162, val_auc: 0.5783472485768502\n",
      "Epoch 34/50: val_loss: 0.6777611970901489, val_auc: 0.5784345351043644\n",
      "Epoch 35/50: val_loss: 0.6799795031547546, val_auc: 0.5792314990512334\n",
      "Epoch 36/50: val_loss: 0.6820343732833862, val_auc: 0.5800132827324478\n",
      "Epoch 37/50: val_loss: 0.6834811568260193, val_auc: 0.5796166982922201\n",
      "Epoch 38/50: val_loss: 0.6840260624885559, val_auc: 0.5792504743833017\n",
      "Epoch 39/50: val_loss: 0.6835966110229492, val_auc: 0.5788368121442126\n",
      "Epoch 40/50: val_loss: 0.6823440790176392, val_auc: 0.5785370018975332\n",
      "Epoch 41/50: val_loss: 0.6805754899978638, val_auc: 0.5791593927893739\n",
      "Epoch 42/50: val_loss: 0.6786361932754517, val_auc: 0.5788709677419355\n",
      "Epoch 43/50: val_loss: 0.6768417358398438, val_auc: 0.5787001897533207\n",
      "Epoch 44/50: val_loss: 0.6754214763641357, val_auc: 0.5784573055028464\n",
      "Epoch 45/50: val_loss: 0.6745177507400513, val_auc: 0.5783130929791271\n",
      "Epoch 46/50: val_loss: 0.6741840243339539, val_auc: 0.5781688804554079\n",
      "Epoch 47/50: val_loss: 0.6744164228439331, val_auc: 0.57819165085389\n",
      "Epoch 48/50: val_loss: 0.6751627922058105, val_auc: 0.5781878557874763\n",
      "Epoch 49/50: val_loss: 0.6763153672218323, val_auc: 0.5782979127134724\n",
      "Epoch 50/50: val_loss: 0.6776974201202393, val_auc: 0.578415559772296\n",
      "Epoch 50/50: test_auc: 0.6135972861192435\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "cls_optimizer = Adam(cls_model.parameters(), lr = 5e-3)\n",
    "gcn_optimizer = Adam([{\"params\": gcn_model1.parameters(), \"lr\": 5e-3},\\\n",
    "                        {\"params\": gcn_model2.parameters(), \"lr\": 5e-3}])\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cls_model.train()\n",
    "    gcn_model1.eval()\n",
    "\n",
    "    _, hid = gcn_model1(data.x)\n",
    "    logits = cls_model(hid)\n",
    "    train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "    # train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    cls_model.zero_grad()\n",
    "    gcn_model1.zero_grad()\n",
    "    train_loss.backward()\n",
    "    cls_optimizer.step()\n",
    "    gcn_optimizer.step()\n",
    "\n",
    "    cls_model.eval()\n",
    "    logits = cls_model(hid)\n",
    "    val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "    # val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask])\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.val_mask].cpu().numpy(), probs[data.val_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "            roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: val_loss: {val_loss}, val_auc: {auc}\")\n",
    "\n",
    "cls_model.eval()\n",
    "logits = cls_model(hid)\n",
    "# val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask])\n",
    "probs = logits.softmax(1)\n",
    "auc = roc_auc_score(data.y[data.test_mask].cpu().numpy(), probs[data.test_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "        roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "print (f\"Epoch {epoch+1}/{epochs}: test_auc: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dci 联合学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)]\n",
      "epoch: 1/30, loss: 0.6948443651199341\n",
      "[Parameter containing:\n",
      "tensor([0.9900, 0.9900, 0.9900], requires_grad=True)]\n",
      "epoch: 2/30, loss: 0.26396119594573975\n",
      "[Parameter containing:\n",
      "tensor([0.9825, 0.9801, 0.9803], requires_grad=True)]\n",
      "epoch: 3/30, loss: 0.13121739029884338\n",
      "[Parameter containing:\n",
      "tensor([0.9805, 0.9705, 0.9714], requires_grad=True)]\n",
      "epoch: 4/30, loss: 0.05474701523780823\n",
      "[Parameter containing:\n",
      "tensor([0.9787, 0.9610, 0.9632], requires_grad=True)]\n",
      "epoch: 5/30, loss: 0.015590299852192402\n",
      "[Parameter containing:\n",
      "tensor([0.9748, 0.9513, 0.9548], requires_grad=True)]\n",
      "epoch: 6/30, loss: 0.0030336028430610895\n",
      "[Parameter containing:\n",
      "tensor([0.9691, 0.9415, 0.9459], requires_grad=True)]\n",
      "epoch: 7/30, loss: 0.0006729051820002496\n",
      "[Parameter containing:\n",
      "tensor([0.9622, 0.9316, 0.9367], requires_grad=True)]\n",
      "epoch: 8/30, loss: 0.00019654440984595567\n",
      "[Parameter containing:\n",
      "tensor([0.9546, 0.9216, 0.9272], requires_grad=True)]\n",
      "epoch: 9/30, loss: 6.018045314704068e-05\n",
      "[Parameter containing:\n",
      "tensor([0.9464, 0.9116, 0.9175], requires_grad=True)]\n",
      "epoch: 10/30, loss: 1.4303236639534589e-05\n",
      "[Parameter containing:\n",
      "tensor([0.9377, 0.9015, 0.9076], requires_grad=True)]\n",
      "epoch: 11/30, loss: 3.6126973554928554e-06\n",
      "[Parameter containing:\n",
      "tensor([0.9288, 0.8914, 0.8977], requires_grad=True)]\n",
      "epoch: 12/30, loss: 1.1041773859687964e-06\n",
      "[Parameter containing:\n",
      "tensor([0.9196, 0.8813, 0.8876], requires_grad=True)]\n",
      "epoch: 13/30, loss: 3.7577973444058443e-07\n",
      "[Parameter containing:\n",
      "tensor([0.9102, 0.8712, 0.8775], requires_grad=True)]\n",
      "epoch: 14/30, loss: 1.4234559841952432e-07\n",
      "[Parameter containing:\n",
      "tensor([0.9007, 0.8611, 0.8674], requires_grad=True)]\n",
      "epoch: 15/30, loss: 5.519149226529407e-08\n",
      "[Parameter containing:\n",
      "tensor([0.8911, 0.8510, 0.8572], requires_grad=True)]\n",
      "epoch: 16/30, loss: 2.3247819669336423e-08\n",
      "[Parameter containing:\n",
      "tensor([0.8813, 0.8409, 0.8471], requires_grad=True)]\n",
      "epoch: 17/30, loss: 1.028012963644187e-08\n",
      "[Parameter containing:\n",
      "tensor([0.8715, 0.8308, 0.8369], requires_grad=True)]\n",
      "epoch: 18/30, loss: 4.535431141761137e-09\n",
      "[Parameter containing:\n",
      "tensor([0.8617, 0.8208, 0.8267], requires_grad=True)]\n",
      "epoch: 19/30, loss: 7.03820612901751e-10\n",
      "[Parameter containing:\n",
      "tensor([0.8518, 0.8108, 0.8165], requires_grad=True)]\n",
      "epoch: 20/30, loss: 5.978399025829972e-11\n",
      "[Parameter containing:\n",
      "tensor([0.8419, 0.8008, 0.8064], requires_grad=True)]\n",
      "epoch: 21/30, loss: 8.152362465652097e-12\n",
      "[Parameter containing:\n",
      "tensor([0.8320, 0.7909, 0.7963], requires_grad=True)]\n",
      "epoch: 22/30, loss: 2.717454299777655e-12\n",
      "[Parameter containing:\n",
      "tensor([0.8221, 0.7810, 0.7862], requires_grad=True)]\n",
      "epoch: 23/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.8122, 0.7711, 0.7761], requires_grad=True)]\n",
      "epoch: 24/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.8023, 0.7613, 0.7661], requires_grad=True)]\n",
      "epoch: 25/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.7924, 0.7515, 0.7562], requires_grad=True)]\n",
      "epoch: 26/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.7826, 0.7418, 0.7462], requires_grad=True)]\n",
      "epoch: 27/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.7727, 0.7321, 0.7363], requires_grad=True)]\n",
      "epoch: 28/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.7629, 0.7225, 0.7265], requires_grad=True)]\n",
      "epoch: 29/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.7532, 0.7129, 0.7167], requires_grad=True)]\n",
      "epoch: 30/30, loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "from models.GAT_model import GAT\n",
    "from models.GCN_model import GCN\n",
    "from models.GIN_model import GIN\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from models.BWGNN_model import BWGNN_em\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pygod.models import DOMINANT\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from utils import *\n",
    "from sklearn.cluster import KMeans\n",
    "from models.ssl_loss import SSL_loss\n",
    "\n",
    "torch.manual_seed(21)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def train_for_mul_model_dci(model_list, optimizer_list, loss_dci, data, shuf_feat, epochs, b_weight, recluster_interval, cluster_info, number_class, b_optimizer=None):\n",
    "    \"\"\"Train for multi model togeother. Like GAT, GCN, BWGNN or anyelse. \n",
    "    Args: \n",
    "        b_weight: option for numbers list or learnable parameters\n",
    "        b_optimizer: None if b_weight is numbers list, real torch optimizer if b_weight belongs to the learnable parameters.\n",
    "    \"\"\"\n",
    "    best_val_auc = 0\n",
    "    for epoch in range(epochs):\n",
    "        print (b_weight)\n",
    "        hiddle_list1 = []\n",
    "        hiddle_list2 = []\n",
    "        for pos, model in enumerate(model_list):\n",
    "            model.train()\n",
    "            if pos % 2 == 0:\n",
    "                _, hid1 = model(data.x)\n",
    "                hid1 = hid1 * b_weight[0][int(pos/2)]\n",
    "                hiddle_list1.append(hid1)\n",
    "            else:\n",
    "                _, hid2 = model(shuf_feat)\n",
    "                hid2 = hid2 * b_weight[0][int(pos/2)]\n",
    "                hiddle_list2.append(hid2)\n",
    "            # if pos != 2:\n",
    "                # hid = hid * 0\n",
    "        # 特征融合以及特征学习\n",
    "        hiddle1 = torch.concat(hiddle_list1, axis=1)\n",
    "        hiddle1 = zero2one((1-cosine_distance(hiddle1.T)).mean(axis=0))*hiddle1\n",
    "        hiddle2 = torch.concat(hiddle_list2, axis=1)\n",
    "        hiddle2 = zero2one((1-cosine_distance(hiddle2.T)).mean(axis=0))*hiddle2\n",
    "\n",
    "        train_loss = loss_dci(hiddle1, hiddle2, None, None, None, cluster_info, number_class)\n",
    "        \n",
    "        optimizer_list.zero_grad()\n",
    "        b_optimizer.zero_grad()\n",
    "        optimizer_loss.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer_list.step()\n",
    "        b_optimizer.step()\n",
    "        optimizer_loss.step()\n",
    "        print (f\"epoch: {epoch + 1}/{epochs}, loss: {train_loss}\")\n",
    "\n",
    "\n",
    "        # re-clustering\n",
    "        if epoch % recluster_interval == 0:\n",
    "            hiddle_list1 = []\n",
    "            for pos, model in enumerate(model_list):\n",
    "                model.eval()\n",
    "                if pos % 2 == 0:\n",
    "                    _, hid1 = model(data.x)\n",
    "                    hid1 = hid1 * b_weight[0][int(pos/2)]\n",
    "                    hiddle_list1.append(hid1)\n",
    "            hiddle1 = torch.concat(hiddle_list1, axis=1)\n",
    "            hiddle1 = zero2one((1-cosine_distance(hiddle1.T)).mean(axis=0))*hiddle1\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=number_class, random_state=0).fit(hiddle1.detach().cpu().numpy())\n",
    "            ss_label = kmeans.labels_\n",
    "            cluster_info = [list(np.where(ss_label==i)[0]) for i in range(number_class)]\n",
    "\n",
    "data_name = \"reddit\"\n",
    "parameter_list = np.linspace(start=0,stop=1,num=1)\n",
    "np.random.seed(2)\n",
    "# data = pyg_dataset(dataset_name=data_name, dataset_spilt=[0.4,0.29,0.3]).dataset\n",
    "data = pyg_dataset(dataset_name=data_name, dataset_spilt=[0.4,0.29,0.3], anomaly_type=\"min\").dataset\n",
    "# data2 = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "data = data.to(device)\n",
    "dgl_data = dgl_data.to(device)\n",
    "a_weight = anomaly_weight(data)\n",
    "\n",
    "epochs = 30\n",
    "hid_dim = 64\n",
    "number_class = 2\n",
    "recluster_interval = 10\n",
    "kmeans = KMeans(n_clusters=number_class, random_state=0).fit(data.x)\n",
    "ss_label = kmeans.labels_\n",
    "cluster_info = [list(np.where(ss_label==i)[0]) for i in range(number_class)]\n",
    "idx = np.random.permutation(data.x.shape[0])\n",
    "shuf_feats = data.x[idx, :]\n",
    "\n",
    "gcn_model1 = GIN(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "gcn_model2 = GIN(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "gat_model1 = GAT(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "gat_model2 = GAT(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "bw_model1 = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data).to(device)\n",
    "bw_model2 = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data).to(device)\n",
    "model_list = [gcn_model1, gcn_model2, gat_model1, gat_model2, bw_model1, bw_model2]\n",
    "loss_dci = SSL_loss(hid_dim * int(len(model_list)/2), device)\n",
    "optimizer_loss = Adam(loss_dci.parameters(), lr = 5e-3)\n",
    "optimizer_list = Adam([{\"params\": model.parameters(), \"lr\": 5e-3} for model in model_list])\n",
    "\n",
    "b_weight = [nn.Parameter(torch.ones([int(len(model_list)/2)], dtype=torch.float32, requires_grad=True))]\n",
    "# b_weight = [[0.0, param1, param2]]\n",
    "b_optimizer = Adam(b_weight, lr = 1e-2, weight_decay=5e-2)    \n",
    "train_for_mul_model_dci(model_list, optimizer_list, loss_dci, data, shuf_feats, epochs, b_weight, recluster_interval, cluster_info, number_class, b_optimizer)\n",
    "cls_model = Linear(hid_dim*int(len(model_list)/2), number_class).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([0.7435, 0.7034, 0.7070], requires_grad=True)]\n",
      "Epoch 1/50: val_loss: 0.6922867894172668, val_auc: 0.49755028462998097\n",
      "Epoch 1/50: test_auc: 0.4683506436172684\n",
      "[Parameter containing:\n",
      "tensor([0.7338, 0.6940, 0.6972], requires_grad=True)]\n",
      "Epoch 2/50: val_loss: 0.6964511275291443, val_auc: 0.47800569259962045\n",
      "Epoch 2/50: test_auc: 0.4461159324318045\n",
      "[Parameter containing:\n",
      "tensor([0.7242, 0.6846, 0.6875], requires_grad=True)]\n",
      "Epoch 3/50: val_loss: 0.701653003692627, val_auc: 0.4929222011385199\n",
      "Epoch 3/50: test_auc: 0.493555114085227\n",
      "[Parameter containing:\n",
      "tensor([0.7146, 0.6752, 0.6780], requires_grad=True)]\n",
      "Epoch 4/50: val_loss: 0.706293523311615, val_auc: 0.4906129032258064\n",
      "Epoch 4/50: test_auc: 0.4938252242204563\n",
      "[Parameter containing:\n",
      "tensor([0.7051, 0.6660, 0.6686], requires_grad=True)]\n",
      "Epoch 5/50: val_loss: 0.7078270316123962, val_auc: 0.49049905123339665\n",
      "Epoch 5/50: test_auc: 0.47810655699614296\n",
      "[Parameter containing:\n",
      "tensor([0.6956, 0.6567, 0.6594], requires_grad=True)]\n",
      "Epoch 6/50: val_loss: 0.7064949870109558, val_auc: 0.4879753320683112\n",
      "Epoch 6/50: test_auc: 0.47362360007435295\n",
      "[Parameter containing:\n",
      "tensor([0.6863, 0.6476, 0.6501], requires_grad=True)]\n",
      "Epoch 7/50: val_loss: 0.7044622302055359, val_auc: 0.49507969639468696\n",
      "Epoch 7/50: test_auc: 0.4940517682048422\n",
      "[Parameter containing:\n",
      "tensor([0.6769, 0.6384, 0.6410], requires_grad=True)]\n",
      "Epoch 8/50: val_loss: 0.7026996612548828, val_auc: 0.5108557874762809\n",
      "Epoch 8/50: test_auc: 0.513788686741949\n",
      "[Parameter containing:\n",
      "tensor([0.6676, 0.6293, 0.6321], requires_grad=True)]\n",
      "Epoch 9/50: val_loss: 0.6999057531356812, val_auc: 0.503438330170778\n",
      "Epoch 9/50: test_auc: 0.5198835912449463\n",
      "[Parameter containing:\n",
      "tensor([0.6584, 0.6203, 0.6233], requires_grad=True)]\n",
      "Epoch 10/50: val_loss: 0.6992735862731934, val_auc: 0.5131537001897534\n",
      "Epoch 10/50: test_auc: 0.5478879130071099\n",
      "[Parameter containing:\n",
      "tensor([0.6491, 0.6113, 0.6149], requires_grad=True)]\n",
      "Epoch 11/50: val_loss: 0.6965336799621582, val_auc: 0.5357210626185958\n",
      "Epoch 11/50: test_auc: 0.5659330126864631\n",
      "[Parameter containing:\n",
      "tensor([0.6400, 0.6024, 0.6067], requires_grad=True)]\n",
      "Epoch 12/50: val_loss: 0.6937981843948364, val_auc: 0.5452182163187855\n",
      "Epoch 12/50: test_auc: 0.5736238905153584\n",
      "[Parameter containing:\n",
      "tensor([0.6308, 0.5935, 0.5990], requires_grad=True)]\n",
      "Epoch 13/50: val_loss: 0.6920123100280762, val_auc: 0.563753320683112\n",
      "Epoch 13/50: test_auc: 0.5889707932524746\n",
      "[Parameter containing:\n",
      "tensor([0.6217, 0.5847, 0.5917], requires_grad=True)]\n",
      "Epoch 14/50: val_loss: 0.6885631680488586, val_auc: 0.5723187855787476\n",
      "Epoch 14/50: test_auc: 0.602563432315628\n",
      "[Parameter containing:\n",
      "tensor([0.6127, 0.5760, 0.5848], requires_grad=True)]\n",
      "Epoch 15/50: val_loss: 0.6843901872634888, val_auc: 0.5754421252371917\n",
      "Epoch 15/50: test_auc: 0.6081515172638133\n",
      "[Parameter containing:\n",
      "tensor([0.6039, 0.5674, 0.5783], requires_grad=True)]\n",
      "Epoch 16/50: val_loss: 0.6814025044441223, val_auc: 0.5778937381404174\n",
      "Epoch 16/50: test_auc: 0.6132923230633394\n",
      "[Parameter containing:\n",
      "tensor([0.5951, 0.5588, 0.5723], requires_grad=True)]\n",
      "Epoch 17/50: val_loss: 0.681117594242096, val_auc: 0.5796925996204934\n",
      "Epoch 17/50: test_auc: 0.6139777638366095\n",
      "[Parameter containing:\n",
      "tensor([0.5864, 0.5503, 0.5663], requires_grad=True)]\n",
      "Epoch 18/50: val_loss: 0.6852396130561829, val_auc: 0.5803946869070208\n",
      "Epoch 18/50: test_auc: 0.6161125052279381\n",
      "[Parameter containing:\n",
      "tensor([0.5777, 0.5419, 0.5602], requires_grad=True)]\n",
      "Epoch 19/50: val_loss: 0.6884681582450867, val_auc: 0.5803624288425048\n",
      "Epoch 19/50: test_auc: 0.6176866954784144\n",
      "[Parameter containing:\n",
      "tensor([0.5693, 0.5336, 0.5532], requires_grad=True)]\n",
      "Epoch 20/50: val_loss: 0.6882035732269287, val_auc: 0.5801555977229602\n",
      "Epoch 20/50: test_auc: 0.6186596728472513\n",
      "[Parameter containing:\n",
      "tensor([0.5609, 0.5253, 0.5454], requires_grad=True)]\n",
      "Epoch 21/50: val_loss: 0.6850111484527588, val_auc: 0.581404174573055\n",
      "Epoch 21/50: test_auc: 0.6186669338723919\n",
      "[Parameter containing:\n",
      "tensor([0.5527, 0.5172, 0.5367], requires_grad=True)]\n",
      "Epoch 22/50: val_loss: 0.6791086792945862, val_auc: 0.5809563567362429\n",
      "Epoch 22/50: test_auc: 0.6176721734281332\n",
      "[Parameter containing:\n",
      "tensor([0.5447, 0.5091, 0.5276], requires_grad=True)]\n",
      "Epoch 23/50: val_loss: 0.6752520203590393, val_auc: 0.579404174573055\n",
      "Epoch 23/50: test_auc: 0.6170477252660439\n",
      "[Parameter containing:\n",
      "tensor([0.5367, 0.5011, 0.5185], requires_grad=True)]\n",
      "Epoch 24/50: val_loss: 0.6820053458213806, val_auc: 0.5803301707779885\n",
      "Epoch 24/50: test_auc: 0.6185783493656769\n",
      "[Parameter containing:\n",
      "tensor([0.5287, 0.4932, 0.5097], requires_grad=True)]\n",
      "Epoch 25/50: val_loss: 0.675193190574646, val_auc: 0.5780151802656547\n",
      "Epoch 25/50: test_auc: 0.6150959617082579\n",
      "[Parameter containing:\n",
      "tensor([0.5209, 0.4853, 0.5012], requires_grad=True)]\n",
      "Epoch 26/50: val_loss: 0.6764442920684814, val_auc: 0.5783301707779887\n",
      "Epoch 26/50: test_auc: 0.6162141595799061\n",
      "[Parameter containing:\n",
      "tensor([0.5131, 0.4776, 0.4932], requires_grad=True)]\n",
      "Epoch 27/50: val_loss: 0.6708601117134094, val_auc: 0.5756774193548386\n",
      "Epoch 27/50: test_auc: 0.6126620660811376\n",
      "[Parameter containing:\n",
      "tensor([0.5052, 0.4699, 0.4859], requires_grad=True)]\n",
      "Epoch 28/50: val_loss: 0.6754679083824158, val_auc: 0.578201138519924\n",
      "Epoch 28/50: test_auc: 0.6160166596960825\n",
      "[Parameter containing:\n",
      "tensor([0.4973, 0.4623, 0.4795], requires_grad=True)]\n",
      "Epoch 29/50: val_loss: 0.6775140166282654, val_auc: 0.5797722960151803\n",
      "Epoch 29/50: test_auc: 0.6161212184581069\n",
      "[Parameter containing:\n",
      "tensor([0.4894, 0.4548, 0.4735], requires_grad=True)]\n",
      "Epoch 30/50: val_loss: 0.6753793954849243, val_auc: 0.5804629981024668\n",
      "Epoch 30/50: test_auc: 0.6175821367163902\n",
      "[Parameter containing:\n",
      "tensor([0.4816, 0.4473, 0.4677], requires_grad=True)]\n",
      "Epoch 31/50: val_loss: 0.6706953644752502, val_auc: 0.5797874762808349\n",
      "Epoch 31/50: test_auc: 0.6175153352850968\n",
      "[Parameter containing:\n",
      "tensor([0.4739, 0.4400, 0.4621], requires_grad=True)]\n",
      "Epoch 32/50: val_loss: 0.6737623810768127, val_auc: 0.5822884250474384\n",
      "Epoch 32/50: test_auc: 0.6196036061155258\n",
      "[Parameter containing:\n",
      "tensor([0.4662, 0.4327, 0.4568], requires_grad=True)]\n",
      "Epoch 33/50: val_loss: 0.677339494228363, val_auc: 0.5836166982922201\n",
      "Epoch 33/50: test_auc: 0.6217339908917701\n",
      "[Parameter containing:\n",
      "tensor([0.4585, 0.4255, 0.4515], requires_grad=True)]\n",
      "Epoch 34/50: val_loss: 0.6739054322242737, val_auc: 0.5835142314990512\n",
      "Epoch 34/50: test_auc: 0.6224208838700683\n",
      "[Parameter containing:\n",
      "tensor([0.4510, 0.4183, 0.4457], requires_grad=True)]\n",
      "Epoch 35/50: val_loss: 0.6719355583190918, val_auc: 0.5838709677419354\n",
      "Epoch 35/50: test_auc: 0.6239703866350667\n",
      "[Parameter containing:\n",
      "tensor([0.4436, 0.4113, 0.4397], requires_grad=True)]\n",
      "Epoch 36/50: val_loss: 0.6833173036575317, val_auc: 0.5859430740037951\n",
      "Epoch 36/50: test_auc: 0.6255024629397277\n",
      "[Parameter containing:\n",
      "tensor([0.4363, 0.4043, 0.4335], requires_grad=True)]\n",
      "Epoch 37/50: val_loss: 0.6654371023178101, val_auc: 0.5835407969639469\n",
      "Epoch 37/50: test_auc: 0.6245527208513407\n",
      "[Parameter containing:\n",
      "tensor([0.4292, 0.3974, 0.4267], requires_grad=True)]\n",
      "Epoch 38/50: val_loss: 0.6706783771514893, val_auc: 0.5845199240986718\n",
      "Epoch 38/50: test_auc: 0.6255866908313583\n",
      "[Parameter containing:\n",
      "tensor([0.4221, 0.3906, 0.4204], requires_grad=True)]\n",
      "Epoch 39/50: val_loss: 0.6762397885322571, val_auc: 0.5856622390891841\n",
      "Epoch 39/50: test_auc: 0.6265858078907013\n",
      "[Parameter containing:\n",
      "tensor([0.4150, 0.3838, 0.4143], requires_grad=True)]\n",
      "Epoch 40/50: val_loss: 0.6759722232818604, val_auc: 0.5865768500948767\n",
      "Epoch 40/50: test_auc: 0.6265625726102513\n",
      "[Parameter containing:\n",
      "tensor([0.4081, 0.3772, 0.4082], requires_grad=True)]\n",
      "Epoch 41/50: val_loss: 0.6695778965950012, val_auc: 0.5875597722960152\n",
      "Epoch 41/50: test_auc: 0.6274745573679074\n",
      "[Parameter containing:\n",
      "tensor([0.4013, 0.3706, 0.4022], requires_grad=True)]\n",
      "Epoch 42/50: val_loss: 0.6655855774879456, val_auc: 0.5897760910815939\n",
      "Epoch 42/50: test_auc: 0.6272886751243089\n",
      "[Parameter containing:\n",
      "tensor([0.3946, 0.3641, 0.3967], requires_grad=True)]\n",
      "Epoch 43/50: val_loss: 0.6646092534065247, val_auc: 0.5888804554079696\n",
      "Epoch 43/50: test_auc: 0.6283516892048888\n",
      "[Parameter containing:\n",
      "tensor([0.3879, 0.3576, 0.3921], requires_grad=True)]\n",
      "Epoch 44/50: val_loss: 0.6707510352134705, val_auc: 0.5901897533206831\n",
      "Epoch 44/50: test_auc: 0.6284213950462383\n",
      "[Parameter containing:\n",
      "tensor([0.3812, 0.3513, 0.3882], requires_grad=True)]\n",
      "Epoch 45/50: val_loss: 0.6715099215507507, val_auc: 0.5899089184060721\n",
      "Epoch 45/50: test_auc: 0.6286305125702868\n",
      "[Parameter containing:\n",
      "tensor([0.3745, 0.3450, 0.3845], requires_grad=True)]\n",
      "Epoch 46/50: val_loss: 0.6698566675186157, val_auc: 0.5918861480075901\n",
      "Epoch 46/50: test_auc: 0.6294408429759747\n",
      "[Parameter containing:\n",
      "tensor([0.3680, 0.3388, 0.3808], requires_grad=True)]\n",
      "Epoch 47/50: val_loss: 0.6691707372665405, val_auc: 0.5920379506641367\n",
      "Epoch 47/50: test_auc: 0.6301117616989638\n",
      "[Parameter containing:\n",
      "tensor([0.3616, 0.3326, 0.3771], requires_grad=True)]\n",
      "Epoch 48/50: val_loss: 0.6684011816978455, val_auc: 0.5932903225806451\n",
      "Epoch 48/50: test_auc: 0.6302366513313816\n",
      "[Parameter containing:\n",
      "tensor([0.3553, 0.3266, 0.3733], requires_grad=True)]\n",
      "Epoch 49/50: val_loss: 0.6680331230163574, val_auc: 0.5932144212523718\n",
      "Epoch 49/50: test_auc: 0.6304254379850365\n",
      "[Parameter containing:\n",
      "tensor([0.3491, 0.3206, 0.3694], requires_grad=True)]\n",
      "Epoch 50/50: val_loss: 0.6684432625770569, val_auc: 0.5932941176470587\n",
      "Epoch 50/50: test_auc: 0.6309540406152703\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "optimizer_list = Adam([{\"params\": model.parameters(), \"lr\": 5e-3} for model in model_list])\n",
    "cls_optimizer = Adam(cls_model.parameters(), lr = 5e-3)\n",
    "# fine-tuning\n",
    "for epoch in range(epochs):\n",
    "    print (b_weight)\n",
    "    hiddle_list1 = []\n",
    "    cls_model.train()\n",
    "    for pos, model in enumerate(model_list):\n",
    "        model.train()\n",
    "        if pos % 2 == 0:\n",
    "            _, hid1 = model(data.x)\n",
    "            hid1 = hid1 * b_weight[0][int(pos/2)]\n",
    "            hiddle_list1.append(hid1)\n",
    "        # if pos != 2:\n",
    "            # hid = hid * 0\n",
    "    # 特征融合以及特征学习\n",
    "    hiddle1 = torch.concat(hiddle_list1, axis=1)\n",
    "    hiddle1 = zero2one((1-cosine_distance(hiddle1.T)).mean(axis=0))*hiddle1\n",
    "\n",
    "    logits = cls_model(hiddle1)\n",
    "    train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "    cls_optimizer.zero_grad()\n",
    "    optimizer_list.zero_grad()\n",
    "    b_optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    cls_optimizer.step()\n",
    "    optimizer_list.step()\n",
    "    b_optimizer.step()\n",
    "\n",
    "    cls_model.eval()\n",
    "    hiddle_list1 = []\n",
    "    for pos, model in enumerate(model_list):\n",
    "        model.train()\n",
    "        if pos % 2 == 0:\n",
    "            _, hid1 = model(data.x)\n",
    "            hid1 = hid1 * b_weight[0][int(pos/2)]\n",
    "            hiddle_list1.append(hid1)\n",
    "    hiddle1 = torch.concat(hiddle_list1, axis=1)\n",
    "    hiddle1 = zero2one((1-cosine_distance(hiddle1.T)).mean(axis=0))*hiddle1\n",
    "\n",
    "    logits = cls_model(hiddle1)\n",
    "    val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "    # val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask])\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.val_mask].cpu().numpy(), probs[data.val_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "            roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: val_loss: {val_loss}, val_auc: {auc}\")\n",
    "    \n",
    "    \n",
    "    auc = roc_auc_score(data.y[data.test_mask].cpu().numpy(), probs[data.test_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "            roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: test_auc: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('pygod2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f987d60b78ec8bc0fb235c2085f536aa1d712f65118694d4c0ee12c7b7a8940"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
