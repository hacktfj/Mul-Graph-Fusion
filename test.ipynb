{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from early_stop import EarlyStopping\n",
    "from torch.optim import Adam\n",
    "from models.SVDD_model import SVDD\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def anomaly_score(node_embedding, c):\n",
    "    # anomaly score of an instance is calculated by \n",
    "    # square Euclidean distance between the node embedding and the center c\n",
    "    return torch.sum((node_embedding - c) ** 2)\n",
    "\n",
    "def nor_loss(node_embedding_list, c):\n",
    "    # normal loss is calculated by mean squared Euclidian distance of \n",
    "    # the normal node embeddings to hypersphere center c \n",
    "    s = 0\n",
    "    num_node = node_embedding_list.size()[0]\n",
    "    for i in range(num_node):\n",
    "        s = s + anomaly_score(node_embedding_list[i], c)\n",
    "    return s/num_node\n",
    "\n",
    "# def AUC_loss(anomaly_node_emb, normal_node_emb, c):\n",
    "#     # AUC_loss encourages the score of anomaly instances to be higher than those of normal instances\n",
    "#     s = 0\n",
    "#     num_anomaly_node = anomaly_node_emb.size()[0]\n",
    "#     num_normal_node = normal_node_emb.size()[0]\n",
    "#     for i in range(num_anomaly_node):\n",
    "#         for j in range(num_normal_node):\n",
    "#             s1 = anomaly_score(anomaly_node_emb[i], c)\n",
    "#             s2 = anomaly_score(normal_node_emb[j], c)\n",
    "#             s = s + torch.sigmoid(s1 - s2)\n",
    "#     return s/(num_anomaly_node * num_normal_node) # check devide by zero\n",
    "\n",
    "def AUC_loss(anomaly_node_emb, normal_node_emb, c):\n",
    "    # AUC_loss encourages the score of anomaly instances to be higher than those of normal instances\n",
    "    s = 0\n",
    "    num_anomaly_node = anomaly_node_emb.size()[0]\n",
    "    num_normal_node = normal_node_emb.size()[0]\n",
    "    s2_list = []\n",
    "    for j in range(num_normal_node):\n",
    "        s2_list.append(anomaly_score(normal_node_emb[j], c))\n",
    "    for i in range(num_anomaly_node):\n",
    "            s1 = anomaly_score(anomaly_node_emb[i], c)\n",
    "            s = s + torch.sigmoid(s1 - torch.tensor(s2_list)).sum()\n",
    "    return s/(num_anomaly_node * num_normal_node) # check devide by zero\n",
    "\n",
    "def objecttive_loss(anomaly_node_emb, normal_node_emb, c, regularizer=1):\n",
    "    Nloss = nor_loss(normal_node_emb, c)\n",
    "    AUCloss = AUC_loss(anomaly_node_emb, normal_node_emb, c)\n",
    "    loss = Nloss - regularizer * AUCloss\n",
    "    return loss\n",
    "\n",
    "def normalize(feature):\n",
    "    \"\"\"Input: feature must be a 1d numpy array\n",
    "    \"\"\"\n",
    "    feature = np.array(feature)\n",
    "    mean = feature.mean()\n",
    "    std = feature.std()\n",
    "    return (feature - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.0 ('pygod2')' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n pygod2 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "\n",
    "# load the dataset\n",
    "graph = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.064,0.3,0.3], anomaly_type=\"min\").dataset\n",
    "dgl_graph = pyg_to_dgl(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\AI Learning\\Mul-Graph_baseline\\dataset.py:100: UserWarning: Anomaly is min class of dataset and anomaly rate is not conformed to setting\n",
      "  warnings.warn(f\"Anomaly is min class of dataset and anomaly rate is not conformed to setting\")\n",
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\dgl\\heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "graph = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.064,0.3,0.3], anomaly_type=\"min\").dataset\n",
    "dgl_graph = pyg_to_dgl(graph)\n",
    "\n",
    "# train_normal mask and train_anomaly mask\n",
    "train_anomaly = [bool(graph.y[i] & graph.train_mask[i]) for i in range(len(graph.train_mask))] \n",
    "train_normal = [bool((~graph.y[i]) & graph.train_mask[i]) for i in range(len(graph.train_mask))]\n",
    "val_anomaly = [bool(graph.y[i] & graph.val_mask[i]) for i in range(len(graph.val_mask))]\n",
    "val_normal = [bool((~graph.y[i]) & graph.val_mask[i]) for i in range(len(graph.val_mask))]\n",
    "test_anomaly = [bool(graph.y[i] & graph.test_mask[i]) for i in range(len(graph.test_mask))]\n",
    "test_normal = [bool((~graph.y[i]) & graph.test_mask[i]) for i in range(len(graph.test_mask))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = graph.x\n",
    "edge_index = graph.edge_index\n",
    "true_label = graph.y\n",
    "node_features = graph.num_node_features\n",
    "AUC_regularizer = 1\n",
    "n_embedding = 32\n",
    "hiddle_dim = 32\n",
    "hiddle_layer = 2\n",
    "\n",
    "model = SVDD(input_dim = node_features, hiddle_dim=hiddle_dim, number_class= n_embedding, hiddle_layer= hiddle_layer, dropout=0.5)\n",
    "lr = 1e-2\n",
    "epochs = 50\n",
    "weight_decay = 5e-4\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "early_stopping = EarlyStopping(patience = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: -0.4991134703159332 loss_val: -0.49394768476486206 roc_auc: 0.5978 time: 0.2763s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0002 loss_train: -0.49533024430274963 loss_val: -0.4989123046398163 roc_auc: 0.6643 time: 0.5230s\n",
      "Epoch: 0003 loss_train: -0.5002880096435547 loss_val: -0.49801501631736755 roc_auc: 0.4931 time: 0.1616s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0004 loss_train: -0.5000959038734436 loss_val: -0.49885308742523193 roc_auc: 0.7487 time: 0.1822s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0005 loss_train: -0.5035908818244934 loss_val: -0.4991309940814972 roc_auc: 0.9104 time: 0.1583s\n",
      "Epoch: 0006 loss_train: -0.5061190724372864 loss_val: -0.498595267534256 roc_auc: 0.9063 time: 0.1645s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0007 loss_train: -0.5086079835891724 loss_val: -0.4986422657966614 roc_auc: 0.9176 time: 0.1633s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0008 loss_train: -0.5116948485374451 loss_val: -0.49955451488494873 roc_auc: 0.9324 time: 0.2254s\n",
      "Epoch: 0009 loss_train: -0.5255290269851685 loss_val: -0.5006705522537231 roc_auc: 0.9396 time: 0.2052s\n",
      "Epoch: 0010 loss_train: -0.5256335139274597 loss_val: -0.501409649848938 roc_auc: 0.9391 time: 0.1852s\n",
      "Epoch: 0011 loss_train: -0.5264497399330139 loss_val: -0.50298011302948 roc_auc: 0.9291 time: 0.2220s\n",
      "Epoch: 0012 loss_train: -0.5569727420806885 loss_val: -0.5007383227348328 roc_auc: 0.9167 time: 0.1827s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0013 loss_train: -0.5735666751861572 loss_val: -0.5024852156639099 roc_auc: 0.9229 time: 0.1881s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0014 loss_train: -0.5658811926841736 loss_val: -0.5046675801277161 roc_auc: 0.9264 time: 0.2843s\n",
      "Epoch: 0015 loss_train: -0.6222572922706604 loss_val: -0.5114119648933411 roc_auc: 0.9325 time: 0.1636s\n",
      "Epoch: 0016 loss_train: -0.670291006565094 loss_val: -0.5221772789955139 roc_auc: 0.9392 time: 0.2108s\n",
      "Epoch: 0017 loss_train: -0.6974895596504211 loss_val: -0.5340816974639893 roc_auc: 0.9436 time: 0.1969s\n",
      "Epoch: 0018 loss_train: -0.7222279906272888 loss_val: -0.5457738041877747 roc_auc: 0.9441 time: 0.1963s\n",
      "Epoch: 0019 loss_train: -0.741157054901123 loss_val: -0.5570805668830872 roc_auc: 0.9428 time: 0.1614s\n",
      "Epoch: 0020 loss_train: -0.814022421836853 loss_val: -0.5715610384941101 roc_auc: 0.9424 time: 0.1634s\n",
      "Epoch: 0021 loss_train: -0.8372465372085571 loss_val: -0.5758395195007324 roc_auc: 0.9300 time: 0.1673s\n",
      "Epoch: 0022 loss_train: -0.8618900179862976 loss_val: -0.5886828899383545 roc_auc: 0.9318 time: 0.1589s\n",
      "Epoch: 0023 loss_train: -0.8887550234794617 loss_val: -0.5995365381240845 roc_auc: 0.9265 time: 0.1455s\n",
      "Epoch: 0024 loss_train: -0.9135417342185974 loss_val: -0.6061921715736389 roc_auc: 0.9234 time: 0.1738s\n",
      "Epoch: 0025 loss_train: -0.9189140796661377 loss_val: -0.6082810163497925 roc_auc: 0.9236 time: 0.1403s\n",
      "Epoch: 0026 loss_train: -0.9442622065544128 loss_val: -0.6095720529556274 roc_auc: 0.9178 time: 0.1494s\n",
      "Epoch: 0027 loss_train: -0.9193253517150879 loss_val: -0.6119514107704163 roc_auc: 0.9213 time: 0.1435s\n",
      "Epoch: 0028 loss_train: -0.959021806716919 loss_val: -0.6128370761871338 roc_auc: 0.9218 time: 0.1492s\n",
      "Epoch: 0029 loss_train: -0.9607048630714417 loss_val: -0.6072400808334351 roc_auc: 0.9223 time: 0.1415s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0030 loss_train: -0.9593890309333801 loss_val: -0.6084549427032471 roc_auc: 0.9202 time: 0.1530s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0031 loss_train: -0.9681957960128784 loss_val: -0.6047887802124023 roc_auc: 0.9175 time: 0.1606s\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch: 0032 loss_train: -0.9782869219779968 loss_val: -0.603501558303833 roc_auc: 0.9151 time: 0.1540s\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch: 0033 loss_train: -0.9691705703735352 loss_val: -0.605461835861206 roc_auc: 0.9089 time: 0.1479s\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch: 0034 loss_train: -0.9792960286140442 loss_val: -0.6040345430374146 roc_auc: 0.9089 time: 0.1519s\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch: 0035 loss_train: -0.9780930876731873 loss_val: -0.5937054753303528 roc_auc: 0.9137 time: 0.1440s\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch: 0036 loss_train: -0.9800262451171875 loss_val: -0.6041312217712402 roc_auc: 0.9032 time: 0.1786s\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch: 0037 loss_train: -0.9838575720787048 loss_val: -0.6036945581436157 roc_auc: 0.9013 time: 0.2244s\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch: 0038 loss_train: -0.982297956943512 loss_val: -0.591678261756897 roc_auc: 0.9070 time: 0.1530s\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch: 0039 loss_train: -0.9808268547058105 loss_val: -0.6032336950302124 roc_auc: 0.8983 time: 0.1529s\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch: 0040 loss_train: -0.9892123937606812 loss_val: -0.6039518117904663 roc_auc: 0.8914 time: 0.1469s\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch: 0041 loss_train: -0.9787673354148865 loss_val: -0.6046081185340881 roc_auc: 0.8937 time: 0.1697s\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch: 0042 loss_train: -0.9852832555770874 loss_val: -0.6019249558448792 roc_auc: 0.8952 time: 0.1469s\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch: 0043 loss_train: -0.9846121668815613 loss_val: -0.6009445786476135 roc_auc: 0.8923 time: 0.1529s\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch: 0044 loss_train: -0.9842661619186401 loss_val: -0.6042711734771729 roc_auc: 0.8813 time: 0.1457s\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch: 0045 loss_train: -0.9865118265151978 loss_val: -0.6046160459518433 roc_auc: 0.8795 time: 0.1545s\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch: 0046 loss_train: -0.9886618256568909 loss_val: -0.6009578704833984 roc_auc: 0.8790 time: 0.1475s\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch: 0047 loss_train: -0.9862549304962158 loss_val: -0.6069980263710022 roc_auc: 0.8714 time: 0.1530s\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch: 0048 loss_train: -0.9775724411010742 loss_val: -0.6062687635421753 roc_auc: 0.8640 time: 0.1460s\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# trainning\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    node_embedding = model(dgl_graph,features)\n",
    "    if epoch % 10 == 0:\n",
    "        center = model(dgl_graph, features)[train_normal].detach().mean(0)\n",
    "    \n",
    "    loss_train = objecttive_loss(node_embedding[train_anomaly], node_embedding[train_normal], center, AUC_regularizer)\n",
    "    model.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    node_embedding = model(dgl_graph, features)\n",
    "\n",
    "    loss_val = objecttive_loss(node_embedding[val_anomaly], node_embedding[val_normal], center, AUC_regularizer)\n",
    "    val_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "    auc = roc_auc_score(graph.y[graph.val_mask], val_anomaly_score)\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.item()),\n",
    "          'loss_val: {}'.format(loss_val.item()),\n",
    "          'roc_auc: {:.4f}'.format(auc),\n",
    "          'time: {:.4f}s'.format(time.time() - start))    \n",
    "    early_stopping(loss_val, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Auc: 0.9038879440665154\n"
     ]
    }
   ],
   "source": [
    "test_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.test_mask].cpu().detach()]\n",
    "auc = roc_auc_score(graph.y[graph.test_mask], test_anomaly_score)\n",
    "\n",
    "print(f\"Test Auc: {auc}\")\n",
    "\n",
    "test_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "auc = roc_auc_score(graph.y[graph.val_mask], test_anomaly_score)\n",
    "\n",
    "print(f\"Val Auc: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\AI Learning\\Mul-Graph_baseline\\dataset.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.dataset = Data(x=temp.x, edge_index=temp.edge_index,y=torch.tensor(temp.y, dtype=torch.long),train_mask=position,val_mask=position,test_mask=position)\n",
      "g:\\AI Learning\\Mul-Graph_baseline\\dataset.py:108: UserWarning: Anomaly is organic and anomaly rate is not conformed to setting\n",
      "  warnings.warn(f\"Anomaly is organic and anomaly rate is not conformed to setting\")\n",
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\dgl\\heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    }
   ],
   "source": [
    "from label_SVDD_model import label_SVDD\n",
    "\n",
    "# load the dataset\n",
    "# graph = pyg_dataset(dataset_name=\"citeseer\", dataset_spilt=[0.1,0.2,0.2], anomaly_type=\"min\").dataset\n",
    "graph = pyg_dataset(dataset_name=\"reddit\", dataset_spilt=[0.2,0.2,0.2]).dataset\n",
    "dgl_graph = pyg_to_dgl(graph)\n",
    "\n",
    "# train_normal mask and train_anomaly mask\n",
    "train_anomaly = [bool(graph.y[i] & graph.train_mask[i]) for i in range(len(graph.train_mask))] \n",
    "train_normal = [bool((~graph.y[i]) & graph.train_mask[i]) for i in range(len(graph.train_mask))]\n",
    "val_anomaly = [bool(graph.y[i] & graph.val_mask[i]) for i in range(len(graph.val_mask))]\n",
    "val_normal = [bool((~graph.y[i]) & graph.val_mask[i]) for i in range(len(graph.val_mask))]\n",
    "test_anomaly = [bool(graph.y[i] & graph.test_mask[i]) for i in range(len(graph.test_mask))]\n",
    "test_normal = [bool((~graph.y[i]) & graph.test_mask[i]) for i in range(len(graph.test_mask))]\n",
    "\n",
    "features = graph.x\n",
    "edge_index = graph.edge_index\n",
    "true_label = graph.y\n",
    "node_features = graph.num_node_features\n",
    "AUC_regularizer = 1\n",
    "n_embedding = 32\n",
    "hiddle_dim = 32\n",
    "hiddle_layer = 2\n",
    "number_class = 2\n",
    "label_svdd_balance = 1\n",
    "\n",
    "model = label_SVDD(input_dim = node_features, hiddle_dim=hiddle_dim, embedding_dim=n_embedding, number_class=number_class, hiddle_layer= hiddle_layer, dropout=0.5)\n",
    "lr = 1e-2\n",
    "epochs = 50\n",
    "weight_decay = 5e-4\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "early_stopping1 = EarlyStopping(patience = 20)\n",
    "early_stopping2 = EarlyStopping(patience = 20)\n",
    "early_stopping3 = EarlyStopping(patience = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: -0.49779295921325684 loss_val: -0.4875163435935974 roc_auc: 0.4837 time: 0.9552s\n",
      "Epoch: 0002 loss_train: -0.48815345764160156 loss_val: -0.49781185388565063 roc_auc: 0.5813 time: 1.0550s\n",
      "Epoch: 0003 loss_train: -0.49753686785697937 loss_val: -0.496715247631073 roc_auc: 0.5293 time: 1.0915s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0004 loss_train: -0.4964378774166107 loss_val: -0.4958604872226715 roc_auc: 0.5133 time: 0.9278s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0005 loss_train: -0.49571889638900757 loss_val: -0.4973612427711487 roc_auc: 0.5083 time: 0.9538s\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch: 0006 loss_train: -0.49690282344818115 loss_val: -0.4983936846256256 roc_auc: 0.5010 time: 0.9993s\n",
      "Epoch: 0007 loss_train: -0.49823835492134094 loss_val: -0.49883249402046204 roc_auc: 0.5543 time: 0.8781s\n",
      "Epoch: 0008 loss_train: -0.49871698021888733 loss_val: -0.4991220533847809 roc_auc: 0.5455 time: 0.8664s\n",
      "Epoch: 0009 loss_train: -0.4991081953048706 loss_val: -0.49894627928733826 roc_auc: 0.5345 time: 0.8627s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0010 loss_train: -0.49895575642585754 loss_val: -0.49878454208374023 roc_auc: 0.5405 time: 0.9598s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0011 loss_train: -0.4996812641620636 loss_val: -0.4995187520980835 roc_auc: 0.6002 time: 0.8871s\n",
      "Epoch: 0012 loss_train: -0.4995933771133423 loss_val: -0.49946075677871704 roc_auc: 0.6110 time: 0.8420s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0013 loss_train: -0.4995097815990448 loss_val: -0.49956223368644714 roc_auc: 0.6276 time: 0.8519s\n",
      "Epoch: 0014 loss_train: -0.4995690882205963 loss_val: -0.49969539046287537 roc_auc: 0.6411 time: 0.8468s\n",
      "Epoch: 0015 loss_train: -0.4996795356273651 loss_val: -0.4997853636741638 roc_auc: 0.6429 time: 0.8573s\n",
      "Epoch: 0016 loss_train: -0.4997471272945404 loss_val: -0.4998256266117096 roc_auc: 0.6253 time: 0.8506s\n",
      "Epoch: 0017 loss_train: -0.49978601932525635 loss_val: -0.49982157349586487 roc_auc: 0.5991 time: 0.9324s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0018 loss_train: -0.4997785985469818 loss_val: -0.4998113811016083 roc_auc: 0.5385 time: 0.9471s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0019 loss_train: -0.4997725486755371 loss_val: -0.4998264014720917 roc_auc: 0.5263 time: 1.0494s\n",
      "Epoch: 0020 loss_train: -0.4997921288013458 loss_val: -0.4998619556427002 roc_auc: 0.5310 time: 1.1328s\n",
      "Epoch: 0021 loss_train: -0.49989452958106995 loss_val: -0.49991676211357117 roc_auc: 0.5523 time: 1.0478s\n",
      "Epoch: 0022 loss_train: -0.49988284707069397 loss_val: -0.49990132451057434 roc_auc: 0.5384 time: 0.8811s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0023 loss_train: -0.4998747408390045 loss_val: -0.49990466237068176 roc_auc: 0.5067 time: 0.8051s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0024 loss_train: -0.49987608194351196 loss_val: -0.49992454051971436 roc_auc: 0.4770 time: 1.9047s\n",
      "Epoch: 0025 loss_train: -0.4998972415924072 loss_val: -0.49994611740112305 roc_auc: 0.4767 time: 1.1495s\n",
      "Epoch: 0026 loss_train: -0.4999251365661621 loss_val: -0.49995800852775574 roc_auc: 0.5153 time: 0.8505s\n",
      "Epoch: 0027 loss_train: -0.4999394416809082 loss_val: -0.49996212124824524 roc_auc: 0.5624 time: 0.7596s\n",
      "Epoch: 0028 loss_train: -0.49994614720344543 loss_val: -0.4999629557132721 roc_auc: 0.5333 time: 0.7638s\n",
      "Epoch: 0029 loss_train: -0.4999493956565857 loss_val: -0.49996379017829895 roc_auc: 0.5212 time: 0.7658s\n",
      "Epoch: 0030 loss_train: -0.49995291233062744 loss_val: -0.499967098236084 roc_auc: 0.5176 time: 0.7873s\n",
      "Epoch: 0031 loss_train: -0.49997565150260925 loss_val: -0.4999845623970032 roc_auc: 0.5222 time: 1.2416s\n",
      "Epoch: 0032 loss_train: -0.49997615814208984 loss_val: -0.4999832808971405 roc_auc: 0.5144 time: 2.6592s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0033 loss_train: -0.4999748170375824 loss_val: -0.4999856650829315 roc_auc: 0.4994 time: 0.9872s\n",
      "Epoch: 0034 loss_train: -0.49997952580451965 loss_val: -0.49999016523361206 roc_auc: 0.4933 time: 0.9235s\n",
      "Epoch: 0035 loss_train: -0.49998563528060913 loss_val: -0.4999921917915344 roc_auc: 0.5272 time: 0.9574s\n",
      "Epoch: 0036 loss_train: -0.49998795986175537 loss_val: -0.4999922811985016 roc_auc: 0.5347 time: 1.2607s\n",
      "Epoch: 0037 loss_train: -0.4999885857105255 loss_val: -0.49999234080314636 roc_auc: 0.5280 time: 1.7909s\n",
      "Epoch: 0038 loss_train: -0.4999890923500061 loss_val: -0.4999934732913971 roc_auc: 0.5266 time: 1.6714s\n",
      "Epoch: 0039 loss_train: -0.4999912977218628 loss_val: -0.49999478459358215 roc_auc: 0.5416 time: 1.7372s\n",
      "Epoch: 0040 loss_train: -0.49999290704727173 loss_val: -0.49999532103538513 roc_auc: 0.5551 time: 1.6360s\n",
      "Epoch: 0041 loss_train: -0.49999576807022095 loss_val: -0.49999672174453735 roc_auc: 0.5587 time: 1.6522s\n",
      "Epoch: 0042 loss_train: -0.49999579787254333 loss_val: -0.4999959468841553 roc_auc: 0.5462 time: 1.6469s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0043 loss_train: -0.4999951720237732 loss_val: -0.49999648332595825 roc_auc: 0.5518 time: 1.9692s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0044 loss_train: -0.499995619058609 loss_val: -0.499997615814209 roc_auc: 0.5585 time: 1.6853s\n",
      "Epoch: 0045 loss_train: -0.4999969005584717 loss_val: -0.4999982416629791 roc_auc: 0.5635 time: 1.6079s\n",
      "Epoch: 0046 loss_train: -0.4999978542327881 loss_val: -0.4999980628490448 roc_auc: 0.5588 time: 1.6542s\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch: 0047 loss_train: -0.4999977648258209 loss_val: -0.4999981224536896 roc_auc: 0.5479 time: 1.7750s\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch: 0048 loss_train: -0.4999978840351105 loss_val: -0.4999985098838806 roc_auc: 0.5414 time: 1.6699s\n",
      "Epoch: 0049 loss_train: -0.49999821186065674 loss_val: -0.49999886751174927 roc_auc: 0.5456 time: 1.0813s\n",
      "Epoch: 0050 loss_train: -0.49999868869781494 loss_val: -0.4999988079071045 roc_auc: 0.5340 time: 1.0011s\n",
      "EarlyStopping counter: 1 out of 20\n"
     ]
    }
   ],
   "source": [
    "from torch.functional import F\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# trainning\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    node_embedding, logits = model(dgl_graph,features)\n",
    "    if epoch % 10 == 0:\n",
    "        center = model(dgl_graph, features)[0][train_normal].detach().mean(0)\n",
    "    loss_train = objecttive_loss(node_embedding[train_anomaly], node_embedding[train_normal], center, AUC_regularizer)\n",
    "    model.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    node_embedding, logits = model(dgl_graph, features)\n",
    "\n",
    "    loss_val = objecttive_loss(node_embedding[val_anomaly], node_embedding[val_normal], center, AUC_regularizer)\n",
    "    val_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "    auc = roc_auc_score(graph.y[graph.val_mask], val_anomaly_score)\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {}'.format(loss_train.item()),\n",
    "          'loss_val: {}'.format(loss_val.item()),\n",
    "          'roc_auc: {:.4f}'.format(auc),\n",
    "          'time: {:.4f}s'.format(time.time() - start))    \n",
    "    early_stopping1(loss_val, model)\n",
    "    if early_stopping1.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     start = time.time()\n",
    "#     model.train()\n",
    "#     node_embedding, logits = model(dgl_graph,features)\n",
    "#     if epoch % 10 == 0:\n",
    "#         center = model(dgl_graph, features)[0][train_normal].detach().mean(0)\n",
    "#     # SVDD loss function\n",
    "#     loss_train = F.cross_entropy(logits[graph.train_mask],graph.y[graph.train_mask],weight=torch.tensor([1.0, 14.5]))\n",
    "#     model.zero_grad()\n",
    "#     loss_train.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     node_embedding, logits = model(dgl_graph, features)\n",
    "\n",
    "#     loss_val = F.cross_entropy(logits[graph.val_mask],graph.y[graph.val_mask],weight=torch.tensor([1.0, 14.5]))\n",
    "#     auc_label = roc_auc_score(graph.y[graph.val_mask].detach().numpy(),logits[graph.val_mask][:,1].detach().numpy())\n",
    "\n",
    "#     val_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "#     auc_svdd = roc_auc_score(graph.y[graph.val_mask], val_anomaly_score)\n",
    "    \n",
    "#     print('Epoch: {:04d}'.format(epoch+1),\n",
    "#           'loss_train: {}'.format(loss_train.item()),\n",
    "#           'loss_val: {}'.format(loss_val.item()),\n",
    "#           'roc_auc_label: {:.4f}'.format(auc_label),\n",
    "#           'roc_auc_svdd: {:.4f}'.format(auc_svdd),\n",
    "#           'time: {:.4f}s'.format(time.time() - start))    \n",
    "#     early_stopping2(loss_val, model)\n",
    "#     if early_stopping2.early_stop:\n",
    "#         print(\"Early stopping\")\n",
    "#         break\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     start = time.time()\n",
    "#     model.train()\n",
    "#     node_embedding, logits = model(dgl_graph,features)\n",
    "#     if epoch % 10 == 0:\n",
    "#         center = model(dgl_graph, features)[0][train_normal].detach().mean(0)\n",
    "\n",
    "#     # SVDD loss function\n",
    "#     loss_svdd = objecttive_loss(node_embedding[train_anomaly], node_embedding[train_normal], center, AUC_regularizer)\n",
    "#     loss_label = F.cross_entropy(logits[graph.train_mask],graph.y[graph.train_mask],weight=torch.tensor([1.0, 7]))\n",
    "#     loss_train = loss_svdd + label_svdd_balance * loss_label\n",
    "#     model.zero_grad()\n",
    "#     loss_train.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     node_embedding, logits = model(dgl_graph, features)\n",
    "\n",
    "#     loss_label = F.cross_entropy(logits[graph.val_mask],graph.y[graph.val_mask],weight=torch.tensor([1.0, 7]))\n",
    "#     loss_svdd = objecttive_loss(node_embedding[train_anomaly], node_embedding[train_normal], center, AUC_regularizer)\n",
    "#     loss_val = loss_svdd + loss_label\n",
    "    \n",
    "#     auc_label = roc_auc_score(graph.y[graph.val_mask].detach().numpy(),logits[graph.val_mask][:,1].detach().numpy())\n",
    "#     val_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "#     auc_svdd = roc_auc_score(graph.y[graph.val_mask], val_anomaly_score)\n",
    "    \n",
    "#     print('Epoch: {:04d}'.format(epoch+1),\n",
    "#           'loss_train: {}'.format(loss_train.item()),\n",
    "#           'loss_val: {}'.format(loss_val.item()),\n",
    "#           'roc_auc_label: {:.4f}'.format(auc_label),\n",
    "#           'roc_auc_svdd: {:.4f}'.format(auc_svdd),\n",
    "#           'time: {:.4f}s'.format(time.time() - start))    \n",
    "#     early_stopping3(loss_val, model)\n",
    "#     if early_stopping3.early_stop:\n",
    "#         print(\"Early stopping\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test svdd_Auc: 0.5614562396089233\n",
      "Val svdd_Auc: 0.5340128005769275\n",
      "\n",
      "Test label_Auc: 0.607921412441996\n",
      "Val label_Auc: 0.5534333941811462\n",
      "\n",
      "Test label_svdd_Auc: 0.4578122285912802\n",
      "Val label_svdd_Auc: 0.5028790029973181\n"
     ]
    }
   ],
   "source": [
    "node_embedding, logits = model(dgl_graph,features)\n",
    "\n",
    "test_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.test_mask].cpu().detach()]\n",
    "auc_svdd = roc_auc_score(graph.y[graph.test_mask], test_anomaly_score)\n",
    "\n",
    "print(f\"Test svdd_Auc: {auc_svdd}\")\n",
    "\n",
    "val_anomaly_score = [anomaly_score(embedding, center) for embedding in node_embedding[graph.val_mask].cpu().detach()]\n",
    "auc_svdd = roc_auc_score(graph.y[graph.val_mask], val_anomaly_score)\n",
    "\n",
    "print(f\"Val svdd_Auc: {auc_svdd}\\n\")\n",
    "\n",
    "auc_label = roc_auc_score(graph.y[graph.test_mask].detach().numpy(),logits[graph.test_mask][:,0].detach().numpy())\n",
    "print(f\"Test label_Auc: {auc_label}\")\n",
    "\n",
    "auc_label = roc_auc_score(graph.y[graph.val_mask].detach().numpy(),logits[graph.val_mask][:,0].detach().numpy())\n",
    "print(f\"Val label_Auc: {auc_label}\\n\")\n",
    "\n",
    "\n",
    "auc_label_svdd = roc_auc_score(graph.y[graph.test_mask].detach().numpy(), normalize(logits[graph.test_mask][:,1].detach().numpy())  + normalize(test_anomaly_score))\n",
    "print(f\"Test label_svdd_Auc: {auc_label_svdd}\")\n",
    "\n",
    "auc_label_svdd = roc_auc_score(graph.y[graph.val_mask].detach().numpy(), normalize(logits[graph.val_mask][:,1].detach().numpy()) + normalize(val_anomaly_score))\n",
    "print(f\"Val label_svdd_Auc: {auc_label_svdd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 BWGNN 与 DOMINANT 的数据融合效果\n",
    "\n",
    "Expected: 能达到BWGNN的性能，同时能利用DOMINANT的表征，对syn异常也有很好的表现\n",
    "\n",
    "1. **设计实验，判断DOMINANT的表征，训练一个二分类网络，判断效果如何**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly syntheic is on processing\n",
      "using 0.08 seconds\n",
      "Epoch 0000: Loss 0.5380\n",
      "Epoch 0001: Loss 0.5364\n",
      "Epoch 0002: Loss 0.5346\n",
      "Epoch 0003: Loss 0.5337\n",
      "Epoch 0004: Loss 0.5334\n",
      "Epoch 0005: Loss 0.5333\n",
      "Epoch 0006: Loss 0.5329\n",
      "Epoch 0007: Loss 0.5326\n",
      "Epoch 0008: Loss 0.5322\n",
      "Epoch 0009: Loss 0.5320\n",
      "Raw scores: [0.55513704 0.44958687 0.54521406 ... 0.3541517  0.54282665 0.55279076]\n",
      "AUC Score: 0.94759867558353\n",
      "AUC My Score: 0.94759867558353\n"
     ]
    }
   ],
   "source": [
    "from pygod.models import DOMINANT\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "\n",
    "data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.2], anomaly_type=\"syn\", anomaly_ratio=0.1).dataset\n",
    "# data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.2], anomaly_type=\"min\").dataset\n",
    "model = DOMINANT(verbose=True, gpu=-1, epoch=10, lr=1e-3)\n",
    "model = model.fit(data)\n",
    "\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "x_, s_, hid_dominate = model.model(data.x, data.edge_index)\n",
    "s = to_dense_adj(data.edge_index)[0]\n",
    "score = model.loss_func(data.x,x_,s,s_)\n",
    "score = score.detach().cpu().numpy()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "outlier_scores = model.decision_function(data)\n",
    "print(f'Raw scores: {outlier_scores}')\n",
    "auc_score = roc_auc_score(data.y.numpy(), outlier_scores)\n",
    "my_score = roc_auc_score(data.y.numpy(), score)\n",
    "print('AUC Score:', auc_score)\n",
    "print('AUC My Score:', my_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOMINANT 表征数据获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BWGNN 表征数据获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly syntheic is on processing\n",
      "using 0.09 seconds\n",
      "Epoch 1/300: loss: 0.6900728344917297, auc: 0.721964782205746\n",
      "Epoch 2/300: loss: 0.6899038553237915, auc: 0.7219750798064052\n",
      "Epoch 3/300: loss: 0.6897387504577637, auc: 0.7219647822057461\n",
      "Epoch 4/300: loss: 0.6895783543586731, auc: 0.7220059726083823\n",
      "Epoch 5/300: loss: 0.6894214749336243, auc: 0.7220162702090412\n",
      "Epoch 6/300: loss: 0.6892673969268799, auc: 0.7216764493872929\n",
      "Epoch 7/300: loss: 0.6891182065010071, auc: 0.721511687776748\n",
      "Epoch 8/300: loss: 0.6889744997024536, auc: 0.7212027597569767\n",
      "Epoch 9/300: loss: 0.688834011554718, auc: 0.7211409741530225\n",
      "Epoch 10/300: loss: 0.6886962056159973, auc: 0.7209762125424777\n",
      "Epoch 11/300: loss: 0.688559889793396, auc: 0.7208320461332509\n",
      "Epoch 12/300: loss: 0.6884261965751648, auc: 0.7207496653279785\n",
      "Epoch 13/300: loss: 0.6882947683334351, auc: 0.7205849037174339\n",
      "Epoch 14/300: loss: 0.6881651878356934, auc: 0.7206672845227061\n",
      "Epoch 15/300: loss: 0.688037633895874, auc: 0.7206260941200701\n",
      "Epoch 16/300: loss: 0.6879118084907532, auc: 0.7205231181134795\n",
      "Epoch 17/300: loss: 0.6877872347831726, auc: 0.7205643085161156\n",
      "Epoch 18/300: loss: 0.6876636743545532, auc: 0.7206672845227062\n",
      "Epoch 19/300: loss: 0.6875424981117249, auc: 0.7206363917207291\n",
      "Epoch 20/300: loss: 0.6874231696128845, auc: 0.7207496653279786\n",
      "Epoch 21/300: loss: 0.68730628490448, auc: 0.720832046133251\n",
      "Epoch 22/300: loss: 0.6871911883354187, auc: 0.7208938317372053\n",
      "Epoch 23/300: loss: 0.6870778203010559, auc: 0.7208526413345689\n",
      "Epoch 24/300: loss: 0.6869662404060364, auc: 0.7207290701266604\n",
      "Epoch 25/300: loss: 0.6868559718132019, auc: 0.7208114509319329\n",
      "Epoch 26/300: loss: 0.6867470145225525, auc: 0.7208114509319329\n",
      "Epoch 27/300: loss: 0.6866387724876404, auc: 0.7208938317372052\n",
      "Epoch 28/300: loss: 0.6865314841270447, auc: 0.7208732365358871\n",
      "Epoch 29/300: loss: 0.686424195766449, auc: 0.7208320461332509\n",
      "Epoch 30/300: loss: 0.686316967010498, auc: 0.7208114509319328\n",
      "Epoch 31/300: loss: 0.6862099170684814, auc: 0.7207496653279786\n",
      "Epoch 32/300: loss: 0.6861028671264648, auc: 0.720852641334569\n",
      "Epoch 33/300: loss: 0.6859956383705139, auc: 0.720852641334569\n",
      "Epoch 34/300: loss: 0.6858887672424316, auc: 0.7208320461332509\n",
      "Epoch 35/300: loss: 0.6857825517654419, auc: 0.7207084749253424\n",
      "Epoch 36/300: loss: 0.685676634311676, auc: 0.72062609412007\n",
      "Epoch 37/300: loss: 0.6855700612068176, auc: 0.7204819277108434\n",
      "Epoch 38/300: loss: 0.685463011264801, auc: 0.7204819277108434\n",
      "Epoch 39/300: loss: 0.6853549480438232, auc: 0.7204613325095252\n",
      "Epoch 40/300: loss: 0.6852456331253052, auc: 0.7205025229121614\n",
      "Epoch 41/300: loss: 0.685136079788208, auc: 0.7206054989187519\n",
      "Epoch 42/300: loss: 0.6850258111953735, auc: 0.7206878797240243\n",
      "Epoch 43/300: loss: 0.6849146485328674, auc: 0.7208114509319329\n",
      "Epoch 44/300: loss: 0.6848028302192688, auc: 0.7209144269385234\n",
      "Epoch 45/300: loss: 0.6846897602081299, auc: 0.7211409741530224\n",
      "Epoch 46/300: loss: 0.6845754384994507, auc: 0.7212027597569767\n",
      "Epoch 47/300: loss: 0.6844596266746521, auc: 0.721264545360931\n",
      "Epoch 48/300: loss: 0.684343159198761, auc: 0.7213263309648852\n",
      "Epoch 49/300: loss: 0.6842257380485535, auc: 0.7214704973741118\n",
      "Epoch 50/300: loss: 0.6841066479682922, auc: 0.721511687776748\n",
      "Epoch 51/300: loss: 0.6839859485626221, auc: 0.7216764493872928\n",
      "Epoch 52/300: loss: 0.6838635802268982, auc: 0.7217588301925651\n",
      "Epoch 53/300: loss: 0.6837401986122131, auc: 0.7218618061991557\n",
      "Epoch 54/300: loss: 0.6836150884628296, auc: 0.7220059726083824\n",
      "Epoch 55/300: loss: 0.6834882497787476, auc: 0.7221501390176088\n",
      "Epoch 56/300: loss: 0.6833592057228088, auc: 0.7222943054268357\n",
      "Epoch 57/300: loss: 0.6832279562950134, auc: 0.7224384718360622\n",
      "Epoch 58/300: loss: 0.6830941438674927, auc: 0.7226444238492431\n",
      "Epoch 59/300: loss: 0.682957649230957, auc: 0.7228709710637422\n",
      "Epoch 60/300: loss: 0.6828188300132751, auc: 0.7229121614663783\n",
      "Epoch 61/300: loss: 0.6826773285865784, auc: 0.7230975182782411\n",
      "Epoch 62/300: loss: 0.6825323700904846, auc: 0.7233652558953764\n",
      "Epoch 63/300: loss: 0.6823843121528625, auc: 0.7236741839151478\n",
      "Epoch 64/300: loss: 0.6822326183319092, auc: 0.7237565647204202\n",
      "Epoch 65/300: loss: 0.6820778846740723, auc: 0.7241066831428277\n",
      "Epoch 66/300: loss: 0.6819202303886414, auc: 0.7243126351560087\n",
      "Epoch 67/300: loss: 0.6817594170570374, auc: 0.7244156111625991\n",
      "Epoch 68/300: loss: 0.6815962791442871, auc: 0.7245803727731439\n",
      "Epoch 69/300: loss: 0.681430995464325, auc: 0.7248275151889609\n",
      "Epoch 70/300: loss: 0.6812632083892822, auc: 0.7252394192153229\n",
      "Epoch 71/300: loss: 0.6810929179191589, auc: 0.7253835856245495\n",
      "Epoch 72/300: loss: 0.6809207797050476, auc: 0.7256101328390486\n",
      "Epoch 73/300: loss: 0.6807469725608826, auc: 0.7256719184430028\n",
      "Epoch 74/300: loss: 0.6805709600448608, auc: 0.7258984656575019\n",
      "Epoch 75/300: loss: 0.6803923845291138, auc: 0.7262279888785914\n",
      "Epoch 76/300: loss: 0.6802111864089966, auc: 0.7265575120996808\n",
      "Epoch 77/300: loss: 0.6800270676612854, auc: 0.7267840593141799\n",
      "Epoch 78/300: loss: 0.6798405051231384, auc: 0.727051796931315\n",
      "Epoch 79/300: loss: 0.6796510815620422, auc: 0.7273195345484502\n",
      "Epoch 80/300: loss: 0.6794589757919312, auc: 0.7276078673669035\n",
      "Epoch 81/300: loss: 0.6792640089988708, auc: 0.7279167953866749\n",
      "Epoch 82/300: loss: 0.6790655255317688, auc: 0.7280609617959014\n",
      "Epoch 83/300: loss: 0.6788632273674011, auc: 0.7284728658222634\n",
      "Epoch 84/300: loss: 0.6786579489707947, auc: 0.7287817938420348\n",
      "Epoch 85/300: loss: 0.6784487962722778, auc: 0.7290495314591701\n",
      "Epoch 86/300: loss: 0.6782357692718506, auc: 0.7293996498815776\n",
      "Epoch 87/300: loss: 0.6780195832252502, auc: 0.7298733395118938\n",
      "Epoch 88/300: loss: 0.6777992844581604, auc: 0.7302852435382556\n",
      "Epoch 89/300: loss: 0.6775750517845154, auc: 0.7308001235712079\n",
      "Epoch 90/300: loss: 0.6773463487625122, auc: 0.7311914323962516\n",
      "Epoch 91/300: loss: 0.677112877368927, auc: 0.7316445268252497\n",
      "Epoch 92/300: loss: 0.6768742203712463, auc: 0.7321800020595202\n",
      "Epoch 93/300: loss: 0.6766306757926941, auc: 0.7326433940891772\n",
      "Epoch 94/300: loss: 0.6763818860054016, auc: 0.7332921429306972\n",
      "Epoch 95/300: loss: 0.6761279106140137, auc: 0.7338070229636495\n",
      "Epoch 96/300: loss: 0.675868570804596, auc: 0.7343836886005561\n",
      "Epoch 97/300: loss: 0.6756033897399902, auc: 0.7347749974255999\n",
      "Epoch 98/300: loss: 0.6753327250480652, auc: 0.7352280918545979\n",
      "Epoch 99/300: loss: 0.675056517124176, auc: 0.7357429718875502\n",
      "Epoch 100/300: loss: 0.6747748255729675, auc: 0.7364020183297292\n",
      "Epoch 101/300: loss: 0.6744869351387024, auc: 0.7366903511481825\n",
      "Epoch 102/300: loss: 0.6741925477981567, auc: 0.7373699927916796\n",
      "Epoch 103/300: loss: 0.6738913655281067, auc: 0.7377407064154052\n",
      "Epoch 104/300: loss: 0.6735833883285522, auc: 0.7384203480589022\n",
      "Epoch 105/300: loss: 0.6732683181762695, auc: 0.7388940376892185\n",
      "Epoch 106/300: loss: 0.672945499420166, auc: 0.7395942745340337\n",
      "Epoch 107/300: loss: 0.6726147532463074, auc: 0.7402533209762125\n",
      "Epoch 108/300: loss: 0.6722766160964966, auc: 0.7409329626197095\n",
      "Epoch 109/300: loss: 0.6719304919242859, auc: 0.7415714138605706\n",
      "Epoch 110/300: loss: 0.6715764403343201, auc: 0.742127484296159\n",
      "Epoch 111/300: loss: 0.6712136268615723, auc: 0.7428277211409742\n",
      "Epoch 112/300: loss: 0.6708422303199768, auc: 0.743466172381835\n",
      "Epoch 113/300: loss: 0.6704623103141785, auc: 0.7440016476161054\n",
      "Epoch 114/300: loss: 0.6700729131698608, auc: 0.7446812892596025\n",
      "Epoch 115/300: loss: 0.6696733832359314, auc: 0.7454227165070538\n",
      "Epoch 116/300: loss: 0.6692639589309692, auc: 0.7460817629492329\n",
      "Epoch 117/300: loss: 0.6688444018363953, auc: 0.7468437853980022\n",
      "Epoch 118/300: loss: 0.6684141755104065, auc: 0.7475028318401812\n",
      "Epoch 119/300: loss: 0.6679745316505432, auc: 0.7481824734836785\n",
      "Epoch 120/300: loss: 0.6675242781639099, auc: 0.7491092575429925\n",
      "Epoch 121/300: loss: 0.6670625805854797, auc: 0.7496241375759448\n",
      "Epoch 122/300: loss: 0.6665899157524109, auc: 0.7504067552260323\n",
      "Epoch 123/300: loss: 0.6661056280136108, auc: 0.7510863968695294\n",
      "Epoch 124/300: loss: 0.6656090617179871, auc: 0.7518896097209349\n",
      "Epoch 125/300: loss: 0.6650993824005127, auc: 0.7527134177736587\n",
      "Epoch 126/300: loss: 0.6645759344100952, auc: 0.7534136546184739\n",
      "Epoch 127/300: loss: 0.664039671421051, auc: 0.7542992482751518\n",
      "Epoch 128/300: loss: 0.6634895205497742, auc: 0.7551848419318299\n",
      "Epoch 129/300: loss: 0.66292405128479, auc: 0.7564617444135516\n",
      "Epoch 130/300: loss: 0.6623438596725464, auc: 0.7571619812583668\n",
      "Epoch 131/300: loss: 0.6617501378059387, auc: 0.7579445989084543\n",
      "Epoch 132/300: loss: 0.6611412167549133, auc: 0.7590361445783131\n",
      "Epoch 133/300: loss: 0.6605167984962463, auc: 0.7600041190402637\n",
      "Epoch 134/300: loss: 0.6598770022392273, auc: 0.7608073318916692\n",
      "Epoch 135/300: loss: 0.659220814704895, auc: 0.7618576871588919\n",
      "Epoch 136/300: loss: 0.6585476398468018, auc: 0.7631963752445681\n",
      "Epoch 137/300: loss: 0.6578571200370789, auc: 0.764287920914427\n",
      "Epoch 138/300: loss: 0.6571481227874756, auc: 0.7655236329935126\n",
      "Epoch 139/300: loss: 0.6564211845397949, auc: 0.7669241066831427\n",
      "Epoch 140/300: loss: 0.6556744575500488, auc: 0.767850890742457\n",
      "Epoch 141/300: loss: 0.6549078226089478, auc: 0.7688806508083617\n",
      "Epoch 142/300: loss: 0.6541230082511902, auc: 0.770281124497992\n",
      "Epoch 143/300: loss: 0.6533183455467224, auc: 0.7714138605704872\n",
      "Epoch 144/300: loss: 0.6524942517280579, auc: 0.7724436206363917\n",
      "Epoch 145/300: loss: 0.6516499519348145, auc: 0.7732880238904335\n",
      "Epoch 146/300: loss: 0.6507841944694519, auc: 0.7743177839563382\n",
      "Epoch 147/300: loss: 0.6498949527740479, auc: 0.7758006384512408\n",
      "Epoch 148/300: loss: 0.6489835977554321, auc: 0.7770981361342807\n",
      "Epoch 149/300: loss: 0.6480494737625122, auc: 0.7785398002265472\n",
      "Epoch 150/300: loss: 0.6470917463302612, auc: 0.7793224178766348\n",
      "Epoch 151/300: loss: 0.6461105942726135, auc: 0.7805787251570384\n",
      "Epoch 152/300: loss: 0.6451072096824646, auc: 0.7817320564308516\n",
      "Epoch 153/300: loss: 0.6440789103507996, auc: 0.7824116980743486\n",
      "Epoch 154/300: loss: 0.6430270671844482, auc: 0.7832149109257542\n",
      "Epoch 155/300: loss: 0.6419517993927002, auc: 0.7849655030377922\n",
      "Epoch 156/300: loss: 0.6408530473709106, auc: 0.7863453815261043\n",
      "Epoch 157/300: loss: 0.639729380607605, auc: 0.7875399032025537\n",
      "Epoch 158/300: loss: 0.6385812759399414, auc: 0.7888374008855936\n",
      "Epoch 159/300: loss: 0.637407660484314, auc: 0.7901554937699516\n",
      "Epoch 160/300: loss: 0.6362069845199585, auc: 0.7916795386674904\n",
      "Epoch 161/300: loss: 0.634979784488678, auc: 0.7927504891360313\n",
      "Epoch 162/300: loss: 0.63372403383255, auc: 0.7937596540006179\n",
      "Epoch 163/300: loss: 0.6324417591094971, auc: 0.7947688188652045\n",
      "Epoch 164/300: loss: 0.631132185459137, auc: 0.7956338173205644\n",
      "Epoch 165/300: loss: 0.6297944784164429, auc: 0.7963752445680157\n",
      "Epoch 166/300: loss: 0.6284269690513611, auc: 0.7975491710431469\n",
      "Epoch 167/300: loss: 0.6270301938056946, auc: 0.7981670270826898\n",
      "Epoch 168/300: loss: 0.6256037950515747, auc: 0.799320358356503\n",
      "Epoch 169/300: loss: 0.6241475939750671, auc: 0.8001235712079086\n",
      "Epoch 170/300: loss: 0.6226612329483032, auc: 0.800906188857996\n",
      "Epoch 171/300: loss: 0.6211441159248352, auc: 0.8022654721449902\n",
      "Epoch 172/300: loss: 0.6195963621139526, auc: 0.8027597569766245\n",
      "Epoch 173/300: loss: 0.6180179715156555, auc: 0.8035217794253938\n",
      "Epoch 174/300: loss: 0.6164077520370483, auc: 0.8046133250952529\n",
      "Epoch 175/300: loss: 0.6147661209106445, auc: 0.805087014725569\n",
      "Epoch 176/300: loss: 0.6130920648574829, auc: 0.8060343939862012\n",
      "Epoch 177/300: loss: 0.6113861203193665, auc: 0.8069611780455154\n",
      "Epoch 178/300: loss: 0.6096464395523071, auc: 0.807311296467923\n",
      "Epoch 179/300: loss: 0.6078740358352661, auc: 0.807743795695603\n",
      "Epoch 180/300: loss: 0.6060693860054016, auc: 0.8084234373390999\n",
      "Epoch 181/300: loss: 0.6042323708534241, auc: 0.8088765317680982\n",
      "Epoch 182/300: loss: 0.602364182472229, auc: 0.8095973638142313\n",
      "Epoch 183/300: loss: 0.6004638671875, auc: 0.8102358150550921\n",
      "Epoch 184/300: loss: 0.5985326170921326, auc: 0.8105241478735454\n",
      "Epoch 185/300: loss: 0.5965694785118103, auc: 0.8109154566985892\n",
      "Epoch 186/300: loss: 0.5945746302604675, auc: 0.8112655751209968\n",
      "Epoch 187/300: loss: 0.5925484299659729, auc: 0.8116774791473588\n",
      "Epoch 188/300: loss: 0.590490996837616, auc: 0.812192359180311\n",
      "Epoch 189/300: loss: 0.588402509689331, auc: 0.8122335495829472\n",
      "Epoch 190/300: loss: 0.5862829685211182, auc: 0.8127484296158995\n",
      "Epoch 191/300: loss: 0.5841341018676758, auc: 0.8128925960251262\n",
      "Epoch 192/300: loss: 0.5819553136825562, auc: 0.8132633096488519\n",
      "Epoch 193/300: loss: 0.5797472596168518, auc: 0.8134074760580785\n",
      "Epoch 194/300: loss: 0.5775074362754822, auc: 0.8135722376686233\n",
      "Epoch 195/300: loss: 0.5752357840538025, auc: 0.8134074760580785\n",
      "Epoch 196/300: loss: 0.5729337334632874, auc: 0.8133456904541242\n",
      "Epoch 197/300: loss: 0.5706021785736084, auc: 0.8130779528369889\n",
      "Epoch 198/300: loss: 0.5682414770126343, auc: 0.8128514056224899\n",
      "Epoch 199/300: loss: 0.5658499598503113, auc: 0.8132633096488519\n",
      "Epoch 200/300: loss: 0.5634276866912842, auc: 0.81371640407785\n",
      "Epoch 201/300: loss: 0.5609757304191589, auc: 0.8132427144475337\n",
      "Epoch 202/300: loss: 0.558495283126831, auc: 0.8132633096488519\n",
      "Epoch 203/300: loss: 0.5559860467910767, auc: 0.8133456904541242\n",
      "Epoch 204/300: loss: 0.553449273109436, auc: 0.8131397384409432\n",
      "Epoch 205/300: loss: 0.5508869290351868, auc: 0.8129749768303985\n",
      "Epoch 206/300: loss: 0.5482980608940125, auc: 0.8130367624343529\n",
      "Epoch 207/300: loss: 0.5456820726394653, auc: 0.81285140562249\n",
      "Epoch 208/300: loss: 0.5430384874343872, auc: 0.8123365255895376\n",
      "Epoch 209/300: loss: 0.5403696894645691, auc: 0.8117804551539491\n",
      "Epoch 210/300: loss: 0.5376750826835632, auc: 0.811286170322315\n",
      "Epoch 211/300: loss: 0.5349559187889099, auc: 0.811100813510452\n",
      "Epoch 212/300: loss: 0.5322121381759644, auc: 0.8108536710946349\n",
      "Epoch 213/300: loss: 0.5294446349143982, auc: 0.8102564102564103\n",
      "Epoch 214/300: loss: 0.5266532301902771, auc: 0.809741530223458\n",
      "Epoch 215/300: loss: 0.5238397717475891, auc: 0.8093708165997323\n",
      "Epoch 216/300: loss: 0.5210046768188477, auc: 0.8086293893522809\n",
      "Epoch 217/300: loss: 0.5181477069854736, auc: 0.8082380805272372\n",
      "Epoch 218/300: loss: 0.5152711272239685, auc: 0.8078055812995573\n",
      "Epoch 219/300: loss: 0.5123744010925293, auc: 0.8067140356296983\n",
      "Epoch 220/300: loss: 0.5094574093818665, auc: 0.8060961795901556\n",
      "Epoch 221/300: loss: 0.5065217614173889, auc: 0.8057254659664298\n",
      "Epoch 222/300: loss: 0.5035679936408997, auc: 0.8049428483163422\n",
      "Epoch 223/300: loss: 0.5005964636802673, auc: 0.8039542786530738\n",
      "Epoch 224/300: loss: 0.4976101517677307, auc: 0.8035217794253939\n",
      "Epoch 225/300: loss: 0.49460965394973755, auc: 0.8028833281845329\n",
      "Epoch 226/300: loss: 0.49159833788871765, auc: 0.8020801153331274\n",
      "Epoch 227/300: loss: 0.48857513070106506, auc: 0.801544640098857\n",
      "Epoch 228/300: loss: 0.4855406880378723, auc: 0.8006178560395428\n",
      "Epoch 229/300: loss: 0.4824965298175812, auc: 0.7997116671815467\n",
      "Epoch 230/300: loss: 0.4794408679008484, auc: 0.7988466687261868\n",
      "Epoch 231/300: loss: 0.4763743579387665, auc: 0.7979095870662136\n",
      "Epoch 232/300: loss: 0.47329676151275635, auc: 0.7970342910101947\n",
      "Epoch 233/300: loss: 0.47020867466926575, auc: 0.7959015549376995\n",
      "Epoch 234/300: loss: 0.467112272977829, auc: 0.7950159612810216\n",
      "Epoch 235/300: loss: 0.46400728821754456, auc: 0.794212748429616\n",
      "Epoch 236/300: loss: 0.46089574694633484, auc: 0.7932859643703016\n",
      "Epoch 237/300: loss: 0.45777764916419983, auc: 0.7921738234991247\n",
      "Epoch 238/300: loss: 0.454653799533844, auc: 0.7911646586345381\n",
      "Epoch 239/300: loss: 0.4515247344970703, auc: 0.7904026361857687\n",
      "Epoch 240/300: loss: 0.44839000701904297, auc: 0.7893316857172279\n",
      "Epoch 241/300: loss: 0.4452529549598694, auc: 0.788466687261868\n",
      "Epoch 242/300: loss: 0.44211456179618835, auc: 0.7876840696117804\n",
      "Epoch 243/300: loss: 0.43897560238838196, auc: 0.7866337143445578\n",
      "Epoch 244/300: loss: 0.4358372390270233, auc: 0.7852950262588817\n",
      "Epoch 245/300: loss: 0.4326974153518677, auc: 0.7841005045824323\n",
      "Epoch 246/300: loss: 0.4295573830604553, auc: 0.7832766965297087\n",
      "Epoch 247/300: loss: 0.42641913890838623, auc: 0.7821645556585316\n",
      "Epoch 248/300: loss: 0.42328211665153503, auc: 0.7810318195860365\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 249/300: loss: 0.42014560103416443, auc: 0.7800638451240862\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 250/300: loss: 0.41700810194015503, auc: 0.7787663474410462\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 251/300: loss: 0.41387009620666504, auc: 0.7776336113685511\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 252/300: loss: 0.4107333719730377, auc: 0.7763361136855113\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 253/300: loss: 0.4075965881347656, auc: 0.7754093296261971\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 254/300: loss: 0.40446141362190247, auc: 0.7744207599629287\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 255/300: loss: 0.4013242721557617, auc: 0.7731026670785708\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 256/300: loss: 0.3981867730617523, auc: 0.7718051693955309\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 257/300: loss: 0.39505043625831604, auc: 0.7705900525177634\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 258/300: loss: 0.39191609621047974, auc: 0.7696426732571312\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 259/300: loss: 0.3887803554534912, auc: 0.768715889197817\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 260/300: loss: 0.38564708828926086, auc: 0.7679126763464112\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 261/300: loss: 0.382525771856308, auc: 0.7670888682936876\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 262/300: loss: 0.37941455841064453, auc: 0.7659767274225106\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 263/300: loss: 0.3763170838356018, auc: 0.7646792297394707\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 264/300: loss: 0.3732345998287201, auc: 0.7634641128617032\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 265/300: loss: 0.3701647222042084, auc: 0.762537328802389\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 266/300: loss: 0.36710476875305176, auc: 0.7615899495417567\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 267/300: loss: 0.36405500769615173, auc: 0.7608485222943054\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Auc: 0.7763687600644124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from BWGNN_model import BWGNN_em\n",
    "\n",
    "data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.3,0.25], anomaly_type=\"syn\", anomaly_ratio=0.1).dataset\n",
    "# data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.3,0.25], anomaly_type=\"min\").dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "number_class = 2\n",
    "hid_dim = 64\n",
    "number_class = 2\n",
    "BWGNN_model = BWGNN_em(data.x.shape[1], 64, number_class, dgl_data)\n",
    "\n",
    "optimizer = Adam(BWGNN_model.parameters(), lr = 1e-4)\n",
    "epochs = 300\n",
    "early_stop = EarlyStopping(patience=20)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    BWGNN_model.train()\n",
    "    logits, BW_hid = BWGNN_model(data.x)\n",
    "    train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, 10.0]))\n",
    "    BWGNN_model.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    BWGNN_model.eval()\n",
    "    logits, BW_hid= BWGNN_model(data.x)\n",
    "    val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, 10.0]))\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "\n",
    "    early_stop(val_loss, BWGNN_model)\n",
    "    if early_stop.early_stop == True:\n",
    "        print (\"Early stopping\")\n",
    "        break\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: loss: {train_loss}, auc: {auc}\")\n",
    "BWGNN_model.eval()\n",
    "logits, BW_hid= BWGNN_model(data.x)\n",
    "probs = logits.softmax(1)\n",
    "auc = roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "print (f\"Auc: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两层 GAT，获取其表征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly syntheic is on processing\n",
      "using 0.08 seconds\n",
      "Epoch 1/300: loss: 0.6930190920829773, auc: 0.6255411255411256\n",
      "Epoch 2/300: loss: 0.6923465728759766, auc: 0.680952380952381\n",
      "Epoch 3/300: loss: 0.6916289329528809, auc: 0.7054951850870219\n",
      "Epoch 4/300: loss: 0.6911395788192749, auc: 0.7191801395883028\n",
      "Epoch 5/300: loss: 0.690441906452179, auc: 0.729039667815178\n",
      "Epoch 6/300: loss: 0.6896844506263733, auc: 0.73551550490326\n",
      "Epoch 7/300: loss: 0.6889178156852722, auc: 0.7387401713932324\n",
      "Epoch 8/300: loss: 0.6880950927734375, auc: 0.7414170863150455\n",
      "Epoch 9/300: loss: 0.6874065399169922, auc: 0.7438731336690521\n",
      "Epoch 10/300: loss: 0.6862128973007202, auc: 0.7452866861030126\n",
      "Epoch 11/300: loss: 0.6855784058570862, auc: 0.7462938422122096\n",
      "Epoch 12/300: loss: 0.6845324039459229, auc: 0.7469122714020675\n",
      "Epoch 13/300: loss: 0.684036910533905, auc: 0.7471419736725858\n",
      "Epoch 14/300: loss: 0.682813286781311, auc: 0.7476720558353211\n",
      "Epoch 15/300: loss: 0.6814088225364685, auc: 0.7478664192949906\n",
      "Epoch 16/300: loss: 0.6806892156600952, auc: 0.7481314603763582\n",
      "Epoch 17/300: loss: 0.6799058318138123, auc: 0.748573195511971\n",
      "Epoch 18/300: loss: 0.6783350110054016, auc: 0.748749889566216\n",
      "Epoch 19/300: loss: 0.6773656010627747, auc: 0.7491386164855552\n",
      "Epoch 20/300: loss: 0.6759372353553772, auc: 0.7490502694584329\n",
      "Epoch 21/300: loss: 0.6746343374252319, auc: 0.7485201872956975\n",
      "Epoch 22/300: loss: 0.6730208396911621, auc: 0.7485555261065464\n",
      "Epoch 23/300: loss: 0.6723797917366028, auc: 0.748467179079424\n",
      "Epoch 24/300: loss: 0.671117901802063, auc: 0.7488559059987632\n",
      "Epoch 25/300: loss: 0.6697293519973755, auc: 0.7486262037282445\n",
      "Epoch 26/300: loss: 0.6680763959884644, auc: 0.7485378567011219\n",
      "Epoch 27/300: loss: 0.6669040322303772, auc: 0.7485731955119711\n",
      "Epoch 28/300: loss: 0.6651526093482971, auc: 0.7482728156197545\n",
      "Epoch 29/300: loss: 0.6636767983436584, auc: 0.7483788320523015\n",
      "Epoch 30/300: loss: 0.6622686386108398, auc: 0.7480784521600847\n",
      "Epoch 31/300: loss: 0.6611220240592957, auc: 0.7477957416732925\n",
      "Epoch 32/300: loss: 0.6591197848320007, auc: 0.747601378213623\n",
      "Epoch 33/300: loss: 0.657880425453186, auc: 0.7477073946461701\n",
      "Epoch 34/300: loss: 0.6562241911888123, auc: 0.7476190476190475\n",
      "Epoch 35/300: loss: 0.6551492810249329, auc: 0.7473716759431045\n",
      "Epoch 36/300: loss: 0.6530749201774597, auc: 0.7468769325912183\n",
      "Epoch 37/300: loss: 0.6513475179672241, auc: 0.7467179079423978\n",
      "Epoch 38/300: loss: 0.6489876508712769, auc: 0.7468592631857938\n",
      "Epoch 39/300: loss: 0.6481548547744751, auc: 0.7466825691315486\n",
      "Epoch 40/300: loss: 0.6463913917541504, auc: 0.7465588832935771\n",
      "Epoch 41/300: loss: 0.6443049311637878, auc: 0.7462054951850872\n",
      "Epoch 42/300: loss: 0.6419411897659302, auc: 0.7460288011308419\n",
      "Epoch 43/300: loss: 0.6407523155212402, auc: 0.7457107518332009\n",
      "Epoch 44/300: loss: 0.6386325359344482, auc: 0.7452513472921636\n",
      "Epoch 45/300: loss: 0.6380699276924133, auc: 0.7450569838324941\n",
      "Epoch 46/300: loss: 0.6356416344642639, auc: 0.7445799098860324\n",
      "Epoch 47/300: loss: 0.6320264339447021, auc: 0.744155844155844\n",
      "Epoch 48/300: loss: 0.6312928795814514, auc: 0.7440498277232971\n",
      "Epoch 49/300: loss: 0.6290233731269836, auc: 0.744014488912448\n",
      "Epoch 50/300: loss: 0.6258529424667358, auc: 0.744155844155844\n",
      "Epoch 51/300: loss: 0.6253867745399475, auc: 0.7440851665341461\n",
      "Epoch 52/300: loss: 0.6233693361282349, auc: 0.7440674971287216\n",
      "Epoch 53/300: loss: 0.6206021904945374, auc: 0.7439261418853256\n",
      "Epoch 54/300: loss: 0.6201527118682861, auc: 0.7438554642636276\n",
      "Epoch 55/300: loss: 0.6171501874923706, auc: 0.7438024560473541\n",
      "Epoch 56/300: loss: 0.6143662929534912, auc: 0.7437141090202315\n",
      "Epoch 57/300: loss: 0.6140579581260681, auc: 0.7436964396148069\n",
      "Epoch 58/300: loss: 0.6102825999259949, auc: 0.7436964396148069\n",
      "Epoch 59/300: loss: 0.6065794229507446, auc: 0.743802456047354\n",
      "Epoch 60/300: loss: 0.6056587100028992, auc: 0.7436787702093824\n",
      "Epoch 61/300: loss: 0.6022721529006958, auc: 0.7437494478310804\n",
      "Epoch 62/300: loss: 0.6000188589096069, auc: 0.7437141090202315\n",
      "Epoch 63/300: loss: 0.5986719131469727, auc: 0.743837794858203\n",
      "Epoch 64/300: loss: 0.5958698391914368, auc: 0.743802456047354\n",
      "Epoch 65/300: loss: 0.5927953124046326, auc: 0.7438554642636275\n",
      "Epoch 66/300: loss: 0.5898914337158203, auc: 0.7438731336690522\n",
      "Epoch 67/300: loss: 0.5880499482154846, auc: 0.7439614806961746\n",
      "Epoch 68/300: loss: 0.587751567363739, auc: 0.7438642989663397\n",
      "Epoch 69/300: loss: 0.5839194059371948, auc: 0.7438201254527785\n",
      "Epoch 70/300: loss: 0.5820345282554626, auc: 0.7439261418853256\n",
      "Epoch 71/300: loss: 0.5793848633766174, auc: 0.7440321583178726\n",
      "Epoch 72/300: loss: 0.5776073932647705, auc: 0.7441735135612686\n",
      "Epoch 73/300: loss: 0.5750411152839661, auc: 0.7444208852372117\n",
      "Epoch 74/300: loss: 0.5730869770050049, auc: 0.7443855464263627\n",
      "Epoch 75/300: loss: 0.5708670616149902, auc: 0.7444562240480608\n",
      "Epoch 76/300: loss: 0.5682804584503174, auc: 0.7443678770209382\n",
      "Epoch 77/300: loss: 0.5648173093795776, auc: 0.7444738934534854\n",
      "Epoch 78/300: loss: 0.5621539950370789, auc: 0.7445622404806078\n",
      "Epoch 79/300: loss: 0.5603573322296143, auc: 0.7445269016697589\n",
      "Epoch 80/300: loss: 0.5561749935150146, auc: 0.7446152486968814\n",
      "Epoch 81/300: loss: 0.5559780597686768, auc: 0.7447389345348527\n",
      "Epoch 82/300: loss: 0.552431583404541, auc: 0.7448979591836735\n",
      "Epoch 83/300: loss: 0.5493655204772949, auc: 0.745021645021645\n",
      "Epoch 84/300: loss: 0.5505591630935669, auc: 0.7450039756162204\n",
      "Epoch 85/300: loss: 0.5450475215911865, auc: 0.7450216450216449\n",
      "Epoch 86/300: loss: 0.5421479344367981, auc: 0.7450039756162206\n",
      "Epoch 87/300: loss: 0.540468156337738, auc: 0.7451099920487676\n",
      "Epoch 88/300: loss: 0.5364289879798889, auc: 0.7451806696704656\n",
      "Epoch 89/300: loss: 0.534278392791748, auc: 0.7452690166975882\n",
      "Epoch 90/300: loss: 0.5302850604057312, auc: 0.7453573637247106\n",
      "Epoch 91/300: loss: 0.527696967124939, auc: 0.7454280413464086\n",
      "Epoch 92/300: loss: 0.5287835597991943, auc: 0.7457284212386254\n",
      "Epoch 93/300: loss: 0.5248222947120667, auc: 0.7458874458874457\n",
      "Epoch 94/300: loss: 0.5212804675102234, auc: 0.7460464705362665\n",
      "Epoch 95/300: loss: 0.520513117313385, auc: 0.7461348175633888\n",
      "Epoch 96/300: loss: 0.5161835551261902, auc: 0.7461524869688134\n",
      "Epoch 97/300: loss: 0.5130747556686401, auc: 0.7462231645905114\n",
      "Epoch 98/300: loss: 0.5133597254753113, auc: 0.7463821892393321\n",
      "Epoch 99/300: loss: 0.5079924464225769, auc: 0.7466118915098507\n",
      "Epoch 100/300: loss: 0.5062246322631836, auc: 0.7468062549695202\n",
      "Epoch 101/300: loss: 0.5036818981170654, auc: 0.7470182878346143\n",
      "Epoch 102/300: loss: 0.502230167388916, auc: 0.7471419736725858\n",
      "Epoch 103/300: loss: 0.496965616941452, auc: 0.7472656595105575\n",
      "Epoch 104/300: loss: 0.4964645802974701, auc: 0.7474246841593781\n",
      "Epoch 105/300: loss: 0.49120965600013733, auc: 0.7477250640515947\n",
      "Epoch 106/300: loss: 0.49400171637535095, auc: 0.7478134110787171\n",
      "Epoch 107/300: loss: 0.48847246170043945, auc: 0.7482021379980562\n",
      "Epoch 108/300: loss: 0.4854966700077057, auc: 0.7484848484848485\n",
      "Epoch 109/300: loss: 0.4878586232662201, auc: 0.7486792119445179\n",
      "Epoch 110/300: loss: 0.4809446930885315, auc: 0.7488382365933386\n",
      "Epoch 111/300: loss: 0.4764974117279053, auc: 0.7491032776747061\n",
      "Epoch 112/300: loss: 0.47741425037384033, auc: 0.7492092941072531\n",
      "Epoch 113/300: loss: 0.4736383557319641, auc: 0.7494566657831964\n",
      "Epoch 114/300: loss: 0.4729905426502228, auc: 0.7495096739994699\n",
      "Epoch 115/300: loss: 0.46711480617523193, auc: 0.749615690432017\n",
      "Epoch 116/300: loss: 0.46808168292045593, auc: 0.7497570456754131\n",
      "Epoch 117/300: loss: 0.46621719002723694, auc: 0.7501281031893277\n",
      "Epoch 118/300: loss: 0.46108314394950867, auc: 0.7503578054598462\n",
      "Epoch 119/300: loss: 0.46071213483810425, auc: 0.7506405159466383\n",
      "Epoch 120/300: loss: 0.45890986919403076, auc: 0.7510999204876756\n",
      "Epoch 121/300: loss: 0.4549638032913208, auc: 0.7511705981093737\n",
      "Epoch 122/300: loss: 0.45346909761428833, auc: 0.7515593250287128\n",
      "Epoch 123/300: loss: 0.45135223865509033, auc: 0.7516830108666842\n",
      "Epoch 124/300: loss: 0.44641295075416565, auc: 0.7517183496775334\n",
      "Epoch 125/300: loss: 0.4437958002090454, auc: 0.7519833907589009\n",
      "Epoch 126/300: loss: 0.4428689181804657, auc: 0.751983390758901\n",
      "Epoch 127/300: loss: 0.4421742856502533, auc: 0.7521777542185706\n",
      "Epoch 128/300: loss: 0.43702444434165955, auc: 0.7524251258945137\n",
      "Epoch 129/300: loss: 0.436171293258667, auc: 0.7525311423270606\n",
      "Epoch 130/300: loss: 0.4338628053665161, auc: 0.7526724975704567\n",
      "Epoch 131/300: loss: 0.4294922649860382, auc: 0.7528491916247019\n",
      "Epoch 132/300: loss: 0.43401914834976196, auc: 0.753202579733192\n",
      "Epoch 133/300: loss: 0.4236627519130707, auc: 0.7534322820037107\n",
      "Epoch 134/300: loss: 0.4274688959121704, auc: 0.7539093559501723\n",
      "Epoch 135/300: loss: 0.4213307201862335, auc: 0.7538386783284742\n",
      "Epoch 136/300: loss: 0.41757363080978394, auc: 0.7539800335718703\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 137/300: loss: 0.41638779640197754, auc: 0.7540860500044174\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 138/300: loss: 0.41540244221687317, auc: 0.7542274052478135\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 139/300: loss: 0.4153117537498474, auc: 0.7544394381129074\n",
      "Epoch 140/300: loss: 0.41311225295066833, auc: 0.7549165120593692\n",
      "Epoch 141/300: loss: 0.40860074758529663, auc: 0.7551108755190389\n",
      "Epoch 142/300: loss: 0.41042330861091614, auc: 0.7552168919515858\n",
      "Epoch 143/300: loss: 0.40436455607414246, auc: 0.7555879494655005\n",
      "Epoch 144/300: loss: 0.4038582742214203, auc: 0.755852990546868\n",
      "Epoch 145/300: loss: 0.3998177945613861, auc: 0.7561003622228112\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 146/300: loss: 0.3944350779056549, auc: 0.7564184115204523\n",
      "Epoch 147/300: loss: 0.3961695432662964, auc: 0.7565244279529995\n",
      "Epoch 148/300: loss: 0.39534878730773926, auc: 0.7564714197367258\n",
      "Epoch 149/300: loss: 0.39339688420295715, auc: 0.7567717996289425\n",
      "Epoch 150/300: loss: 0.3909951150417328, auc: 0.7571781959537062\n",
      "Epoch 151/300: loss: 0.3876781761646271, auc: 0.7574078982242247\n",
      "Epoch 152/300: loss: 0.38996121287345886, auc: 0.757831963954413\n",
      "Epoch 153/300: loss: 0.38307249546051025, auc: 0.7582736990900255\n",
      "Epoch 154/300: loss: 0.3817324936389923, auc: 0.7584857319551197\n",
      "Epoch 155/300: loss: 0.37983423471450806, auc: 0.7586977648202138\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 156/300: loss: 0.37841346859931946, auc: 0.7589097976853078\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 157/300: loss: 0.37601232528686523, auc: 0.7592455163883735\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 158/300: loss: 0.3739970624446869, auc: 0.7597225903348352\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 159/300: loss: 0.37379634380340576, auc: 0.7598639455782313\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 160/300: loss: 0.36669301986694336, auc: 0.7599346231999293\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 161/300: loss: 0.36524489521980286, auc: 0.7600406396324764\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 162/300: loss: 0.36772674322128296, auc: 0.7603586889301175\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 163/300: loss: 0.3625677824020386, auc: 0.7605530523897871\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 164/300: loss: 0.3603628873825073, auc: 0.7607650852548812\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 165/300: loss: 0.35996854305267334, auc: 0.7611361427687958\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 166/300: loss: 0.357974112033844, auc: 0.7614188532555879\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 167/300: loss: 0.35640501976013184, auc: 0.7616308861206821\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 168/300: loss: 0.3518032431602478, auc: 0.7619489354183231\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 169/300: loss: 0.35251039266586304, auc: 0.7621609682834173\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 170/300: loss: 0.3507187068462372, auc: 0.7625320257973319\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 171/300: loss: 0.352195680141449, auc: 0.7626557116353034\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 172/300: loss: 0.34976208209991455, auc: 0.7629384221220955\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 173/300: loss: 0.34349218010902405, auc: 0.7631857937980386\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 174/300: loss: 0.34406137466430664, auc: 0.7631504549871896\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Auc: 0.7712921065862243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.nn.conv import GATConv, GCNConv, MessagePassing\n",
    "\n",
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels=in_dim, out_channels=hid_dim)\n",
    "        self.conv2 = GCNConv(in_channels=hid_dim, out_channels=out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        h = F.dropout(F.relu(x), 0.3, self.training)\n",
    "        x = self.conv2(h, edge_index)\n",
    "        return x, h\n",
    "\n",
    "data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.3,0.25], anomaly_type=\"syn\", anomaly_ratio=0.1).dataset\n",
    "# data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.3,0.25], anomaly_type=\"min\").dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "number_class = 2\n",
    "hid_dim = 64\n",
    "number_class = 2\n",
    "edge_index = data.edge_index\n",
    "GAT_model = GAT(data.x.shape[1], 64, number_class)\n",
    "\n",
    "optimizer = Adam(GAT_model.parameters(), lr = 1e-3)\n",
    "epochs = 300\n",
    "early_stop = EarlyStopping(patience=20)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    GAT_model.train()\n",
    "    logits, GAT_hid = GAT_model(data.x, edge_index)\n",
    "    train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, 10.0]))\n",
    "    GAT_model.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    GAT_model.eval()\n",
    "    logits, GAT_hid= GAT_model(data.x, edge_index)\n",
    "    val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, 10.0]))\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "\n",
    "    early_stop(val_loss, GAT_model)\n",
    "    if early_stop.early_stop == True:\n",
    "        print (\"Early stopping\")\n",
    "        break\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: loss: {train_loss}, auc: {auc}\")\n",
    "GAT_model.eval()\n",
    "logits, GAT_hid= GAT_model(data.x, edge_index)\n",
    "probs = logits.softmax(1)\n",
    "auc = roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "print (f\"Auc: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 消除冗余特征函数\n",
    "\n",
    "### 消除不同任务表征量纲的影响，归一化融合的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x1, x2=None, eps=1e-8):\n",
    "    x2 = x1 if x2 is None else x2\n",
    "    w1 = x1.norm(p=2, dim=1, keepdim=True)\n",
    "    w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\n",
    "    # .clamp(min=eps)\n",
    "    # 0-1\n",
    "    return (torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)).absolute()\n",
    "\n",
    "# 越无关，权重越大\n",
    "def zero2one(feature):\n",
    "    \"\"\"Input: feature must be a 1d numpy array\n",
    "    \"\"\"\n",
    "    # feature = np.array(feature)\n",
    "    min = feature.min()\n",
    "    max = feature.max()\n",
    "    return (feature - min)/(max-min)\n",
    "\n",
    "def feature_normalize(feature, axis=1, eps=1e-10):\n",
    "    \"\"\"2D array feature to row normalize\"\"\"\n",
    "    mean = None\n",
    "    std = None\n",
    "    if axis == 1:\n",
    "        mean = feature.mean(axis=axis).reshape(-1,1)\n",
    "        std = feature.std(axis=axis).reshape(-1,1)\n",
    "    elif axis == 0:\n",
    "        mean = feature.mean(axis=axis).reshape(1,-1)\n",
    "        std = feature.std(axis=axis).reshape(1,-1)\n",
    "    return (feature - mean) / (std + eps)\n",
    "\n",
    "# hiddle = hid_dominate.detach()\n",
    "# hiddle = BW_hid.detach()\n",
    "# hiddle = GAT_hid.detach()\n",
    "# 融合的特征未曾归一化\n",
    "\n",
    "# 补充信息最好策略\n",
    "hiddle = torch.concat((1 * feature_normalize(GAT_hid.detach(),axis=0), 1 * feature_normalize(BW_hid.detach(),axis=0), 0.2 * feature_normalize(hid_dominate.detach(),axis=0)), axis=1)\n",
    "\n",
    "# 可学习化的参数\n",
    "# l_weight = [nn.Parameter(torch.randn([hiddle.shape[-1]], dtype=torch.float32, requires_grad=True))]\n",
    "# hiddle = torch.mul(hiddle, torch.softmax(*l_weight, dim = 0))\n",
    "# optimizer_ = Adam(l_weight, lr = 1e-3)\n",
    "\n",
    "# 计算hiddle特征之间的相似度，放缩到0-1之间 （专家系统）\n",
    "# hiddle = zero2one((1-cosine_distance(hiddle.T)).mean(axis=0))*hiddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签导向，检测融合的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000: loss: 0.7500657439231873, auc: 0.46432712375357504\n",
      "Epoch 2/2000: loss: 0.7401655912399292, auc: 0.5405941614490738\n",
      "Epoch 3/2000: loss: 0.7304065227508545, auc: 0.6193347246914536\n",
      "Epoch 4/2000: loss: 0.7207922339439392, auc: 0.6925098554533509\n",
      "Epoch 5/2000: loss: 0.7113261818885803, auc: 0.7547860142743037\n",
      "Epoch 6/2000: loss: 0.702012300491333, auc: 0.8108525933369405\n",
      "Epoch 7/2000: loss: 0.6928532719612122, auc: 0.8542681198629254\n",
      "Epoch 8/2000: loss: 0.6838525533676147, auc: 0.887763778310273\n",
      "Epoch 9/2000: loss: 0.6750122904777527, auc: 0.9125505655613099\n",
      "Epoch 10/2000: loss: 0.6663348078727722, auc: 0.9306897529051041\n",
      "Epoch 11/2000: loss: 0.6578222513198853, auc: 0.942207106232769\n",
      "Epoch 12/2000: loss: 0.6494759917259216, auc: 0.9509675092113061\n",
      "Epoch 13/2000: loss: 0.6412971019744873, auc: 0.9567133029295818\n",
      "Epoch 14/2000: loss: 0.6332863569259644, auc: 0.9607327819432636\n",
      "Epoch 15/2000: loss: 0.6254439353942871, auc: 0.9632836051634845\n",
      "Epoch 16/2000: loss: 0.6177700161933899, auc: 0.9650099198680786\n",
      "Epoch 17/2000: loss: 0.6102643609046936, auc: 0.9662724485326325\n",
      "Epoch 18/2000: loss: 0.6029260158538818, auc: 0.9670196593749194\n",
      "Epoch 19/2000: loss: 0.5957545638084412, auc: 0.9677411043260931\n",
      "Epoch 20/2000: loss: 0.5887486338615417, auc: 0.9680502950194532\n",
      "Epoch 21/2000: loss: 0.581906795501709, auc: 0.9683852516039266\n",
      "Epoch 22/2000: loss: 0.5752272605895996, auc: 0.96810182680168\n",
      "Epoch 23/2000: loss: 0.5687080025672913, auc: 0.96810182680168\n",
      "Epoch 24/2000: loss: 0.5623467564582825, auc: 0.9680245291283399\n",
      "Epoch 25/2000: loss: 0.5561410784721375, auc: 0.9676380407616396\n",
      "Epoch 26/2000: loss: 0.5500882267951965, auc: 0.9677153384349798\n",
      "Epoch 27/2000: loss: 0.5441855192184448, auc: 0.9677153384349798\n",
      "Epoch 28/2000: loss: 0.5384295582771301, auc: 0.9675092113060729\n",
      "Epoch 29/2000: loss: 0.5328174829483032, auc: 0.9675349771971864\n",
      "Epoch 30/2000: loss: 0.5273460745811462, auc: 0.9675349771971865\n",
      "Epoch 31/2000: loss: 0.522011935710907, auc: 0.9673803818505063\n",
      "Epoch 32/2000: loss: 0.5168116688728333, auc: 0.9674061477416198\n",
      "Epoch 33/2000: loss: 0.5117418169975281, auc: 0.9672773182860529\n",
      "Epoch 34/2000: loss: 0.506799042224884, auc: 0.9671484888304862\n",
      "Epoch 35/2000: loss: 0.5019799470901489, auc: 0.9672000206127128\n",
      "Epoch 36/2000: loss: 0.4972807765007019, auc: 0.9670454252660329\n",
      "Epoch 37/2000: loss: 0.49269843101501465, auc: 0.9670711911571461\n",
      "Epoch 38/2000: loss: 0.48822933435440063, auc: 0.9669681275926928\n",
      "Epoch 39/2000: loss: 0.48387008905410767, auc: 0.9668650640282395\n",
      "Epoch 40/2000: loss: 0.4796173572540283, auc: 0.9669165958104661\n",
      "Epoch 41/2000: loss: 0.47546783089637756, auc: 0.9668135322460126\n",
      "Epoch 42/2000: loss: 0.47141826152801514, auc: 0.9667362345726727\n",
      "Epoch 43/2000: loss: 0.4674655497074127, auc: 0.9667877663548994\n",
      "Epoch 44/2000: loss: 0.4636063873767853, auc: 0.9667877663548994\n",
      "Epoch 45/2000: loss: 0.45983779430389404, auc: 0.966762000463786\n",
      "Epoch 46/2000: loss: 0.45615679025650024, auc: 0.9667362345726727\n",
      "Epoch 47/2000: loss: 0.4525603950023651, auc: 0.9666589368993327\n",
      "Epoch 48/2000: loss: 0.4490458369255066, auc: 0.9666074051171061\n",
      "Epoch 49/2000: loss: 0.4456102252006531, auc: 0.9664270438793126\n",
      "Epoch 50/2000: loss: 0.4422508478164673, auc: 0.9664012779881992\n",
      "Epoch 51/2000: loss: 0.4389651417732239, auc: 0.9663497462059726\n",
      "Epoch 52/2000: loss: 0.4357506036758423, auc: 0.9662466826415193\n",
      "Epoch 53/2000: loss: 0.4326046407222748, auc: 0.9661693849681792\n",
      "Epoch 54/2000: loss: 0.4295249581336975, auc: 0.9661693849681791\n",
      "Epoch 55/2000: loss: 0.42650917172431946, auc: 0.9660663214037257\n",
      "Epoch 56/2000: loss: 0.4235551059246063, auc: 0.9660405555126124\n",
      "Epoch 57/2000: loss: 0.4206605553627014, auc: 0.9660663214037258\n",
      "Epoch 58/2000: loss: 0.4178233742713928, auc: 0.9659890237303858\n",
      "Epoch 59/2000: loss: 0.4150417447090149, auc: 0.965937491948159\n",
      "Epoch 60/2000: loss: 0.4123136103153229, auc: 0.9658859601659323\n",
      "Epoch 61/2000: loss: 0.40963706374168396, auc: 0.9658344283837057\n",
      "Epoch 62/2000: loss: 0.4070104658603668, auc: 0.9658086624925923\n",
      "Epoch 63/2000: loss: 0.4044319987297058, auc: 0.9657313648192523\n",
      "Epoch 64/2000: loss: 0.4019000232219696, auc: 0.9657313648192523\n",
      "Epoch 65/2000: loss: 0.3994128704071045, auc: 0.9657313648192523\n",
      "Epoch 66/2000: loss: 0.39696913957595825, auc: 0.9657313648192523\n",
      "Epoch 67/2000: loss: 0.3945674002170563, auc: 0.9656798330370255\n",
      "Epoch 68/2000: loss: 0.3922061026096344, auc: 0.9656283012547989\n",
      "Epoch 69/2000: loss: 0.38988396525382996, auc: 0.9656025353636856\n",
      "Epoch 70/2000: loss: 0.38759973645210266, auc: 0.9655767694725722\n",
      "Epoch 71/2000: loss: 0.38535213470458984, auc: 0.9656025353636857\n",
      "Epoch 72/2000: loss: 0.383139967918396, auc: 0.9655510035814588\n",
      "Epoch 73/2000: loss: 0.38096216320991516, auc: 0.9655510035814588\n",
      "Epoch 74/2000: loss: 0.378817617893219, auc: 0.9655252376903455\n",
      "Epoch 75/2000: loss: 0.37670519948005676, auc: 0.9654994717992321\n",
      "Epoch 76/2000: loss: 0.3746240735054016, auc: 0.9654479400170055\n",
      "Epoch 77/2000: loss: 0.3725730776786804, auc: 0.9654479400170055\n",
      "Epoch 78/2000: loss: 0.37055152654647827, auc: 0.9654221741258922\n",
      "Epoch 79/2000: loss: 0.3685583472251892, auc: 0.9654221741258922\n",
      "Epoch 80/2000: loss: 0.36659279465675354, auc: 0.9654221741258922\n",
      "Epoch 81/2000: loss: 0.3646540939807892, auc: 0.9654479400170055\n",
      "Epoch 82/2000: loss: 0.3627414405345917, auc: 0.9654221741258922\n",
      "Epoch 83/2000: loss: 0.3608541488647461, auc: 0.9654221741258922\n",
      "Epoch 84/2000: loss: 0.3589913547039032, auc: 0.9653706423436654\n",
      "Epoch 85/2000: loss: 0.3571526110172272, auc: 0.9653706423436654\n",
      "Epoch 86/2000: loss: 0.3553370535373688, auc: 0.9653191105614388\n",
      "Epoch 87/2000: loss: 0.3535442352294922, auc: 0.9652933446703253\n",
      "Epoch 88/2000: loss: 0.3517734706401825, auc: 0.9652675787792121\n",
      "Epoch 89/2000: loss: 0.3500242233276367, auc: 0.965190281105872\n",
      "Epoch 90/2000: loss: 0.3482958674430847, auc: 0.965190281105872\n",
      "Epoch 91/2000: loss: 0.34658804535865784, auc: 0.9651387493236453\n",
      "Epoch 92/2000: loss: 0.34490007162094116, auc: 0.965112983432532\n",
      "Epoch 93/2000: loss: 0.3432316482067108, auc: 0.9651129834325319\n",
      "Epoch 94/2000: loss: 0.34158214926719666, auc: 0.9651129834325319\n",
      "Epoch 95/2000: loss: 0.33995121717453003, auc: 0.965112983432532\n",
      "Epoch 96/2000: loss: 0.33833837509155273, auc: 0.9651129834325319\n",
      "Epoch 97/2000: loss: 0.3367433249950409, auc: 0.9651129834325319\n",
      "Epoch 98/2000: loss: 0.33516550064086914, auc: 0.9650872175414186\n",
      "Epoch 99/2000: loss: 0.3336046636104584, auc: 0.9650872175414187\n",
      "Epoch 100/2000: loss: 0.3320603668689728, auc: 0.9650872175414187\n",
      "Epoch 101/2000: loss: 0.3305323123931885, auc: 0.9650872175414186\n",
      "Epoch 102/2000: loss: 0.32902011275291443, auc: 0.9650872175414186\n",
      "Epoch 103/2000: loss: 0.32752346992492676, auc: 0.9650872175414187\n",
      "Epoch 104/2000: loss: 0.3260420858860016, auc: 0.9650872175414186\n",
      "Epoch 105/2000: loss: 0.3245756924152374, auc: 0.9650614516503052\n",
      "Epoch 106/2000: loss: 0.32312384247779846, auc: 0.9650356857591919\n",
      "Epoch 107/2000: loss: 0.32168641686439514, auc: 0.965035685759192\n",
      "Epoch 108/2000: loss: 0.3202630579471588, auc: 0.9649841539769652\n",
      "Epoch 109/2000: loss: 0.31885355710983276, auc: 0.9649841539769652\n",
      "Epoch 110/2000: loss: 0.3174575865268707, auc: 0.9649841539769652\n",
      "Epoch 111/2000: loss: 0.3160749673843384, auc: 0.9649841539769652\n",
      "Epoch 112/2000: loss: 0.314705491065979, auc: 0.964958388085852\n",
      "Epoch 113/2000: loss: 0.3133488595485687, auc: 0.9649326221947386\n",
      "Epoch 114/2000: loss: 0.3120048940181732, auc: 0.9649068563036253\n",
      "Epoch 115/2000: loss: 0.3106732964515686, auc: 0.964881090412512\n",
      "Epoch 116/2000: loss: 0.30935394763946533, auc: 0.964881090412512\n",
      "Epoch 117/2000: loss: 0.3080466091632843, auc: 0.9649068563036252\n",
      "Epoch 118/2000: loss: 0.3067510724067688, auc: 0.9649068563036253\n",
      "Epoch 119/2000: loss: 0.3054671883583069, auc: 0.964881090412512\n",
      "Epoch 120/2000: loss: 0.30419474840164185, auc: 0.9648810904125119\n",
      "Epoch 121/2000: loss: 0.30293354392051697, auc: 0.9648553245213985\n",
      "Epoch 122/2000: loss: 0.3016834855079651, auc: 0.9648553245213987\n",
      "Epoch 123/2000: loss: 0.3004443049430847, auc: 0.9648553245213985\n",
      "Epoch 124/2000: loss: 0.2992159426212311, auc: 0.9648810904125118\n",
      "Epoch 125/2000: loss: 0.2979981005191803, auc: 0.9648810904125118\n",
      "Epoch 126/2000: loss: 0.2967907786369324, auc: 0.9648810904125119\n",
      "Epoch 127/2000: loss: 0.2955936789512634, auc: 0.9648810904125119\n",
      "Epoch 128/2000: loss: 0.29440680146217346, auc: 0.9648810904125118\n",
      "Epoch 129/2000: loss: 0.293229877948761, auc: 0.9648810904125118\n",
      "Epoch 130/2000: loss: 0.29206275939941406, auc: 0.9648810904125118\n",
      "Epoch 131/2000: loss: 0.2909054458141327, auc: 0.9648810904125118\n",
      "Epoch 132/2000: loss: 0.28975772857666016, auc: 0.9648810904125118\n",
      "Epoch 133/2000: loss: 0.28861942887306213, auc: 0.9648810904125119\n",
      "Epoch 134/2000: loss: 0.28749048709869385, auc: 0.9648295586302852\n",
      "Epoch 135/2000: loss: 0.28637072443962097, auc: 0.9648295586302852\n",
      "Epoch 136/2000: loss: 0.28526002168655396, auc: 0.9648295586302851\n",
      "Epoch 137/2000: loss: 0.2841583490371704, auc: 0.9648295586302853\n",
      "Epoch 138/2000: loss: 0.2830654978752136, auc: 0.9648295586302852\n",
      "Epoch 139/2000: loss: 0.2819814085960388, auc: 0.9648295586302851\n",
      "Epoch 140/2000: loss: 0.28090593218803406, auc: 0.9648295586302851\n",
      "Epoch 141/2000: loss: 0.27983900904655457, auc: 0.9648553245213987\n",
      "Epoch 142/2000: loss: 0.27878043055534363, auc: 0.9648553245213987\n",
      "Epoch 143/2000: loss: 0.27773022651672363, auc: 0.9648553245213987\n",
      "Epoch 144/2000: loss: 0.27668821811676025, auc: 0.9648295586302853\n",
      "Epoch 145/2000: loss: 0.27565431594848633, auc: 0.9648295586302851\n",
      "Epoch 146/2000: loss: 0.2746284008026123, auc: 0.9648295586302853\n",
      "Epoch 147/2000: loss: 0.2736104428768158, auc: 0.9648295586302851\n",
      "Epoch 148/2000: loss: 0.27260029315948486, auc: 0.9648295586302853\n",
      "Epoch 149/2000: loss: 0.27159783244132996, auc: 0.9648295586302853\n",
      "Epoch 150/2000: loss: 0.2706030607223511, auc: 0.9648295586302853\n",
      "Epoch 151/2000: loss: 0.26961585879325867, auc: 0.9648295586302852\n",
      "Epoch 152/2000: loss: 0.2686360776424408, auc: 0.9648295586302853\n",
      "Epoch 153/2000: loss: 0.26766374707221985, auc: 0.9648295586302853\n",
      "Epoch 154/2000: loss: 0.2666986286640167, auc: 0.9648295586302853\n",
      "Epoch 155/2000: loss: 0.2657407522201538, auc: 0.9648295586302853\n",
      "Epoch 156/2000: loss: 0.26479002833366394, auc: 0.9648295586302853\n",
      "Epoch 157/2000: loss: 0.26384636759757996, auc: 0.9648295586302853\n",
      "Epoch 158/2000: loss: 0.2629096508026123, auc: 0.9648295586302852\n",
      "Epoch 159/2000: loss: 0.2619798481464386, auc: 0.9648295586302852\n",
      "Epoch 160/2000: loss: 0.2610568702220917, auc: 0.9648295586302853\n",
      "Epoch 161/2000: loss: 0.26014065742492676, auc: 0.9648037927391719\n",
      "Epoch 162/2000: loss: 0.2592311203479767, auc: 0.9648037927391718\n",
      "Epoch 163/2000: loss: 0.2583281993865967, auc: 0.9648037927391718\n",
      "Epoch 164/2000: loss: 0.2574317753314972, auc: 0.9648037927391718\n",
      "Epoch 165/2000: loss: 0.2565418481826782, auc: 0.9648037927391718\n",
      "Epoch 166/2000: loss: 0.2556583285331726, auc: 0.9647780268480585\n",
      "Epoch 167/2000: loss: 0.2547811269760132, auc: 0.9647780268480585\n",
      "Epoch 168/2000: loss: 0.25391021370887756, auc: 0.9647522609569451\n",
      "Epoch 169/2000: loss: 0.2530454993247986, auc: 0.9647522609569451\n",
      "Epoch 170/2000: loss: 0.2521868944168091, auc: 0.9647522609569451\n",
      "Epoch 171/2000: loss: 0.25133439898490906, auc: 0.9647522609569451\n",
      "Epoch 172/2000: loss: 0.25048789381980896, auc: 0.9647522609569451\n",
      "Epoch 173/2000: loss: 0.2496473640203476, auc: 0.9647522609569451\n",
      "Epoch 174/2000: loss: 0.2488126903772354, auc: 0.9647522609569452\n",
      "Epoch 175/2000: loss: 0.24798384308815002, auc: 0.9647264950658317\n",
      "Epoch 176/2000: loss: 0.24716080725193024, auc: 0.9647264950658319\n",
      "Epoch 177/2000: loss: 0.2463434636592865, auc: 0.9647264950658317\n",
      "Epoch 178/2000: loss: 0.24553178250789642, auc: 0.9647264950658319\n",
      "Epoch 179/2000: loss: 0.24472571909427643, auc: 0.9647264950658317\n",
      "Epoch 180/2000: loss: 0.24392522871494293, auc: 0.9647264950658319\n",
      "Epoch 181/2000: loss: 0.24313020706176758, auc: 0.9647264950658319\n",
      "Epoch 182/2000: loss: 0.24234060943126678, auc: 0.9647264950658319\n",
      "Epoch 183/2000: loss: 0.24155640602111816, auc: 0.9647007291747184\n",
      "Epoch 184/2000: loss: 0.24077758193016052, auc: 0.9647007291747185\n",
      "Epoch 185/2000: loss: 0.24000398814678192, auc: 0.9647007291747185\n",
      "Epoch 186/2000: loss: 0.23923566937446594, auc: 0.9647007291747185\n",
      "Epoch 187/2000: loss: 0.23847247660160065, auc: 0.9646749632836051\n",
      "Epoch 188/2000: loss: 0.2377144694328308, auc: 0.9646749632836052\n",
      "Epoch 189/2000: loss: 0.2369615137577057, auc: 0.9646749632836051\n",
      "Epoch 190/2000: loss: 0.23621362447738647, auc: 0.9646749632836052\n",
      "Epoch 191/2000: loss: 0.2354707270860672, auc: 0.9646749632836052\n",
      "Epoch 192/2000: loss: 0.2347327321767807, auc: 0.9646749632836051\n",
      "Epoch 193/2000: loss: 0.23399966955184937, auc: 0.9646749632836052\n",
      "Epoch 194/2000: loss: 0.23327143490314484, auc: 0.9646749632836052\n",
      "Epoch 195/2000: loss: 0.2325480580329895, auc: 0.9646749632836051\n",
      "Epoch 196/2000: loss: 0.23182938992977142, auc: 0.9646749632836052\n",
      "Epoch 197/2000: loss: 0.23111547529697418, auc: 0.9646749632836051\n",
      "Epoch 198/2000: loss: 0.23040618002414703, auc: 0.9646749632836051\n",
      "Epoch 199/2000: loss: 0.22970157861709595, auc: 0.9646749632836051\n",
      "Epoch 200/2000: loss: 0.22900152206420898, auc: 0.9646749632836051\n",
      "Epoch 201/2000: loss: 0.22830606997013092, auc: 0.9646491973924918\n",
      "Epoch 202/2000: loss: 0.22761507332324982, auc: 0.9646491973924918\n",
      "Epoch 203/2000: loss: 0.22692856192588806, auc: 0.9646491973924918\n",
      "Epoch 204/2000: loss: 0.22624646127223969, auc: 0.9646491973924918\n",
      "Epoch 205/2000: loss: 0.2255687713623047, auc: 0.9646749632836051\n",
      "Epoch 206/2000: loss: 0.2248953878879547, auc: 0.9646749632836051\n",
      "Epoch 207/2000: loss: 0.22422632575035095, auc: 0.9646749632836051\n",
      "Epoch 208/2000: loss: 0.22356155514717102, auc: 0.9646749632836051\n",
      "Epoch 209/2000: loss: 0.22290100157260895, auc: 0.9646749632836051\n",
      "Epoch 210/2000: loss: 0.22224462032318115, auc: 0.9646749632836051\n",
      "Epoch 211/2000: loss: 0.22159244120121002, auc: 0.9646749632836051\n",
      "Epoch 212/2000: loss: 0.2209443598985672, auc: 0.9646749632836051\n",
      "Epoch 213/2000: loss: 0.2203003317117691, auc: 0.9646749632836051\n",
      "Epoch 214/2000: loss: 0.21966040134429932, auc: 0.9646749632836051\n",
      "Epoch 215/2000: loss: 0.21902446448802948, auc: 0.9646749632836051\n",
      "Epoch 216/2000: loss: 0.2183925211429596, auc: 0.9646749632836051\n",
      "Epoch 217/2000: loss: 0.21776452660560608, auc: 0.9646749632836051\n",
      "Epoch 218/2000: loss: 0.21714039146900177, auc: 0.9646749632836051\n",
      "Epoch 219/2000: loss: 0.21652017533779144, auc: 0.9646491973924918\n",
      "Epoch 220/2000: loss: 0.21590375900268555, auc: 0.9646491973924919\n",
      "Epoch 221/2000: loss: 0.21529123187065125, auc: 0.9646491973924918\n",
      "Epoch 222/2000: loss: 0.2146824151277542, auc: 0.9646491973924918\n",
      "Epoch 223/2000: loss: 0.21407735347747803, auc: 0.9646491973924918\n",
      "Epoch 224/2000: loss: 0.2134760171175003, auc: 0.9646491973924918\n",
      "Epoch 225/2000: loss: 0.21287834644317627, auc: 0.9646491973924918\n",
      "Epoch 226/2000: loss: 0.21228435635566711, auc: 0.9646491973924918\n",
      "Epoch 227/2000: loss: 0.21169395744800568, auc: 0.9646234315013785\n",
      "Epoch 228/2000: loss: 0.21110716462135315, auc: 0.9646234315013784\n",
      "Epoch 229/2000: loss: 0.21052393317222595, auc: 0.9646234315013784\n",
      "Epoch 230/2000: loss: 0.2099442183971405, auc: 0.9646234315013785\n",
      "Epoch 231/2000: loss: 0.20936799049377441, auc: 0.9646234315013785\n",
      "Epoch 232/2000: loss: 0.20879526436328888, auc: 0.9646234315013784\n",
      "Epoch 233/2000: loss: 0.20822595059871674, auc: 0.9645976656102652\n",
      "Epoch 234/2000: loss: 0.20766007900238037, auc: 0.9645976656102652\n",
      "Epoch 235/2000: loss: 0.20709756016731262, auc: 0.964597665610265\n",
      "Epoch 236/2000: loss: 0.20653840899467468, auc: 0.9645976656102652\n",
      "Epoch 237/2000: loss: 0.20598261058330536, auc: 0.9645976656102652\n",
      "Epoch 238/2000: loss: 0.20543010532855988, auc: 0.9645976656102652\n",
      "Epoch 239/2000: loss: 0.20488087832927704, auc: 0.964597665610265\n",
      "Epoch 240/2000: loss: 0.20433488488197327, auc: 0.964597665610265\n",
      "Epoch 241/2000: loss: 0.20379211008548737, auc: 0.9645976656102652\n",
      "Epoch 242/2000: loss: 0.20325252413749695, auc: 0.9645976656102652\n",
      "Epoch 243/2000: loss: 0.20271612703800201, auc: 0.9645976656102652\n",
      "Epoch 244/2000: loss: 0.2021828591823578, auc: 0.964597665610265\n",
      "Epoch 245/2000: loss: 0.20165275037288666, auc: 0.9645976656102652\n",
      "Epoch 246/2000: loss: 0.20112569630146027, auc: 0.9645976656102652\n",
      "Epoch 247/2000: loss: 0.20060168206691742, auc: 0.9645976656102652\n",
      "Epoch 248/2000: loss: 0.20008079707622528, auc: 0.964597665610265\n",
      "Epoch 249/2000: loss: 0.19956284761428833, auc: 0.9645976656102652\n",
      "Epoch 250/2000: loss: 0.19904793798923492, auc: 0.9645976656102652\n",
      "Epoch 251/2000: loss: 0.1985359787940979, auc: 0.964597665610265\n",
      "Epoch 252/2000: loss: 0.19802697002887726, auc: 0.964597665610265\n",
      "Epoch 253/2000: loss: 0.197520911693573, auc: 0.9646234315013785\n",
      "Epoch 254/2000: loss: 0.19701772928237915, auc: 0.9646234315013785\n",
      "Epoch 255/2000: loss: 0.1965174376964569, auc: 0.9646234315013785\n",
      "Epoch 256/2000: loss: 0.1960199773311615, auc: 0.9646234315013785\n",
      "Epoch 257/2000: loss: 0.1955253779888153, auc: 0.9646234315013785\n",
      "Epoch 258/2000: loss: 0.19503359496593475, auc: 0.9646234315013785\n",
      "Epoch 259/2000: loss: 0.19454458355903625, auc: 0.9646234315013785\n",
      "Epoch 260/2000: loss: 0.194058358669281, auc: 0.9646234315013785\n",
      "Epoch 261/2000: loss: 0.19357486069202423, auc: 0.9646234315013785\n",
      "Epoch 262/2000: loss: 0.19309407472610474, auc: 0.9646234315013785\n",
      "Epoch 263/2000: loss: 0.1926160305738449, auc: 0.9646234315013785\n",
      "Epoch 264/2000: loss: 0.1921406388282776, auc: 0.9646234315013785\n",
      "Epoch 265/2000: loss: 0.19166792929172516, auc: 0.9646491973924919\n",
      "Epoch 266/2000: loss: 0.19119782745838165, auc: 0.9646491973924919\n",
      "Epoch 267/2000: loss: 0.19073037803173065, auc: 0.9646491973924919\n",
      "Epoch 268/2000: loss: 0.19026552140712738, auc: 0.9646491973924919\n",
      "Epoch 269/2000: loss: 0.18980325758457184, auc: 0.9646491973924919\n",
      "Epoch 270/2000: loss: 0.18934351205825806, auc: 0.9646491973924919\n",
      "Epoch 271/2000: loss: 0.188886359333992, auc: 0.9646491973924919\n",
      "Epoch 272/2000: loss: 0.1884317249059677, auc: 0.9646491973924918\n",
      "Epoch 273/2000: loss: 0.1879795789718628, auc: 0.9646491973924919\n",
      "Epoch 274/2000: loss: 0.18752989172935486, auc: 0.9646491973924919\n",
      "Epoch 275/2000: loss: 0.1870827078819275, auc: 0.9646491973924919\n",
      "Epoch 276/2000: loss: 0.18663793802261353, auc: 0.9646491973924919\n",
      "Epoch 277/2000: loss: 0.18619562685489655, auc: 0.9646491973924919\n",
      "Epoch 278/2000: loss: 0.18575572967529297, auc: 0.9646491973924919\n",
      "Epoch 279/2000: loss: 0.1853182017803192, auc: 0.9646491973924919\n",
      "Epoch 280/2000: loss: 0.18488307297229767, auc: 0.9646491973924919\n",
      "Epoch 281/2000: loss: 0.18445025384426117, auc: 0.9646491973924919\n",
      "Epoch 282/2000: loss: 0.18401983380317688, auc: 0.9646491973924919\n",
      "Epoch 283/2000: loss: 0.18359170854091644, auc: 0.9646491973924919\n",
      "Epoch 284/2000: loss: 0.18316586315631866, auc: 0.9646491973924919\n",
      "Epoch 285/2000: loss: 0.18274232745170593, auc: 0.9646491973924919\n",
      "Epoch 286/2000: loss: 0.18232107162475586, auc: 0.9646491973924919\n",
      "Epoch 287/2000: loss: 0.18190205097198486, auc: 0.9646491973924919\n",
      "Epoch 288/2000: loss: 0.18148528039455414, auc: 0.9646491973924919\n",
      "Epoch 289/2000: loss: 0.1810707449913025, auc: 0.9646491973924919\n",
      "Epoch 290/2000: loss: 0.18065835535526276, auc: 0.9646491973924919\n",
      "Epoch 291/2000: loss: 0.1802482008934021, auc: 0.9646491973924919\n",
      "Epoch 292/2000: loss: 0.17984020709991455, auc: 0.9646491973924919\n",
      "Epoch 293/2000: loss: 0.1794344037771225, auc: 0.9646491973924919\n",
      "Epoch 294/2000: loss: 0.1790306717157364, auc: 0.9646491973924919\n",
      "Epoch 295/2000: loss: 0.17862913012504578, auc: 0.9646491973924919\n",
      "Epoch 296/2000: loss: 0.1782296746969223, auc: 0.9646491973924919\n",
      "Epoch 297/2000: loss: 0.17783230543136597, auc: 0.9646491973924919\n",
      "Epoch 298/2000: loss: 0.17743700742721558, auc: 0.9646491973924919\n",
      "Epoch 299/2000: loss: 0.1770438253879547, auc: 0.9646491973924919\n",
      "Epoch 300/2000: loss: 0.17665265500545502, auc: 0.9646491973924919\n",
      "Epoch 301/2000: loss: 0.1762634962797165, auc: 0.9646491973924919\n",
      "Epoch 302/2000: loss: 0.17587639391422272, auc: 0.9646491973924919\n",
      "Epoch 303/2000: loss: 0.17549128830432892, auc: 0.9646491973924919\n",
      "Epoch 304/2000: loss: 0.1751081645488739, auc: 0.9646491973924919\n",
      "Epoch 305/2000: loss: 0.17472700774669647, auc: 0.9646491973924918\n",
      "Epoch 306/2000: loss: 0.17434783279895782, auc: 0.9646491973924919\n",
      "Epoch 307/2000: loss: 0.17397060990333557, auc: 0.9646491973924919\n",
      "Epoch 308/2000: loss: 0.17359532415866852, auc: 0.9646491973924919\n",
      "Epoch 309/2000: loss: 0.17322194576263428, auc: 0.9646491973924918\n",
      "Epoch 310/2000: loss: 0.17285048961639404, auc: 0.9646491973924918\n",
      "Epoch 311/2000: loss: 0.17248094081878662, auc: 0.9646491973924918\n",
      "Epoch 312/2000: loss: 0.17211322486400604, auc: 0.9646491973924918\n",
      "Epoch 313/2000: loss: 0.17174741625785828, auc: 0.9646491973924918\n",
      "Epoch 314/2000: loss: 0.17138344049453735, auc: 0.9646491973924918\n",
      "Epoch 315/2000: loss: 0.17102132737636566, auc: 0.9646491973924919\n",
      "Epoch 316/2000: loss: 0.17066103219985962, auc: 0.9646234315013785\n",
      "Epoch 317/2000: loss: 0.1703025847673416, auc: 0.9646234315013785\n",
      "Epoch 318/2000: loss: 0.16994589567184448, auc: 0.9646234315013784\n",
      "Epoch 319/2000: loss: 0.1695910394191742, auc: 0.9646234315013785\n",
      "Epoch 320/2000: loss: 0.1692378968000412, auc: 0.9646234315013785\n",
      "Epoch 321/2000: loss: 0.16888658702373505, auc: 0.9646491973924918\n",
      "Epoch 322/2000: loss: 0.16853702068328857, auc: 0.9646491973924919\n",
      "Epoch 323/2000: loss: 0.1681891828775406, auc: 0.9646491973924919\n",
      "Epoch 324/2000: loss: 0.1678430736064911, auc: 0.9646491973924919\n",
      "Epoch 325/2000: loss: 0.16749869287014008, auc: 0.9646491973924919\n",
      "Epoch 326/2000: loss: 0.16715602576732635, auc: 0.9646491973924918\n",
      "Epoch 327/2000: loss: 0.16681504249572754, auc: 0.9646491973924918\n",
      "Epoch 328/2000: loss: 0.16647572815418243, auc: 0.9646234315013785\n",
      "Epoch 329/2000: loss: 0.16613811254501343, auc: 0.9646234315013784\n",
      "Epoch 330/2000: loss: 0.16580212116241455, auc: 0.9646234315013784\n",
      "Epoch 331/2000: loss: 0.16546784341335297, auc: 0.9646234315013785\n",
      "Epoch 332/2000: loss: 0.16513513028621674, auc: 0.9646234315013784\n",
      "Epoch 333/2000: loss: 0.1648041009902954, auc: 0.9646234315013783\n",
      "Epoch 334/2000: loss: 0.16447466611862183, auc: 0.9646234315013784\n",
      "Epoch 335/2000: loss: 0.16414685547351837, auc: 0.9646234315013784\n",
      "Epoch 336/2000: loss: 0.16382059454917908, auc: 0.9646234315013784\n",
      "Epoch 337/2000: loss: 0.16349594295024872, auc: 0.9646234315013784\n",
      "Epoch 338/2000: loss: 0.1631728857755661, auc: 0.9646234315013784\n",
      "Epoch 339/2000: loss: 0.16285136342048645, auc: 0.9646234315013785\n",
      "Epoch 340/2000: loss: 0.16253139078617096, auc: 0.9646234315013784\n",
      "Epoch 341/2000: loss: 0.16221296787261963, auc: 0.9646491973924919\n",
      "Epoch 342/2000: loss: 0.16189609467983246, auc: 0.9646491973924919\n",
      "Epoch 343/2000: loss: 0.16158075630664825, auc: 0.9646491973924918\n",
      "Epoch 344/2000: loss: 0.16126687824726105, auc: 0.9646491973924917\n",
      "Epoch 345/2000: loss: 0.1609545201063156, auc: 0.9646491973924918\n",
      "Epoch 346/2000: loss: 0.16064368188381195, auc: 0.9646491973924918\n",
      "Epoch 347/2000: loss: 0.16033430397510529, auc: 0.9646491973924918\n",
      "Epoch 348/2000: loss: 0.16002638638019562, auc: 0.9646491973924919\n",
      "Epoch 349/2000: loss: 0.15971994400024414, auc: 0.9646491973924918\n",
      "Epoch 350/2000: loss: 0.15941496193408966, auc: 0.9646491973924918\n",
      "Epoch 351/2000: loss: 0.15911142528057098, auc: 0.9646491973924919\n",
      "Epoch 352/2000: loss: 0.1588093340396881, auc: 0.9646491973924918\n",
      "Epoch 353/2000: loss: 0.15850861370563507, auc: 0.9646234315013784\n",
      "Epoch 354/2000: loss: 0.15820932388305664, auc: 0.9646234315013784\n",
      "Epoch 355/2000: loss: 0.15791144967079163, auc: 0.9646234315013784\n",
      "Epoch 356/2000: loss: 0.15761500597000122, auc: 0.9646234315013784\n",
      "Epoch 357/2000: loss: 0.15731990337371826, auc: 0.9646234315013784\n",
      "Epoch 358/2000: loss: 0.1570262312889099, auc: 0.9646234315013785\n",
      "Epoch 359/2000: loss: 0.15673388540744781, auc: 0.964597665610265\n",
      "Epoch 360/2000: loss: 0.15644291043281555, auc: 0.9646234315013784\n",
      "Epoch 361/2000: loss: 0.15615330636501312, auc: 0.9646234315013784\n",
      "Epoch 362/2000: loss: 0.15586502850055695, auc: 0.9646234315013784\n",
      "Epoch 363/2000: loss: 0.15557807683944702, auc: 0.9646234315013784\n",
      "Epoch 364/2000: loss: 0.15529248118400574, auc: 0.9646234315013784\n",
      "Epoch 365/2000: loss: 0.15500818192958832, auc: 0.9646234315013784\n",
      "Epoch 366/2000: loss: 0.15472519397735596, auc: 0.9646234315013784\n",
      "Epoch 367/2000: loss: 0.15444353222846985, auc: 0.9646234315013784\n",
      "Epoch 368/2000: loss: 0.15416312217712402, auc: 0.9646234315013784\n",
      "Epoch 369/2000: loss: 0.15388402342796326, auc: 0.9646234315013784\n",
      "Epoch 370/2000: loss: 0.15360620617866516, auc: 0.9646234315013784\n",
      "Epoch 371/2000: loss: 0.15332964062690735, auc: 0.9646234315013784\n",
      "Epoch 372/2000: loss: 0.15305431187152863, auc: 0.9646234315013784\n",
      "Epoch 373/2000: loss: 0.15278026461601257, auc: 0.9646234315013784\n",
      "Epoch 374/2000: loss: 0.152507483959198, auc: 0.9646234315013784\n",
      "Epoch 375/2000: loss: 0.15223592519760132, auc: 0.9646234315013784\n",
      "Epoch 376/2000: loss: 0.15196560323238373, auc: 0.9646234315013784\n",
      "Epoch 377/2000: loss: 0.15169645845890045, auc: 0.9646234315013785\n",
      "Epoch 378/2000: loss: 0.15142856538295746, auc: 0.9646234315013784\n",
      "Epoch 379/2000: loss: 0.15116189420223236, auc: 0.9646234315013784\n",
      "Epoch 380/2000: loss: 0.15089640021324158, auc: 0.9646234315013784\n",
      "Epoch 381/2000: loss: 0.1506320685148239, auc: 0.9646234315013784\n",
      "Epoch 382/2000: loss: 0.15036895871162415, auc: 0.9646234315013784\n",
      "Epoch 383/2000: loss: 0.1501070111989975, auc: 0.9646234315013784\n",
      "Epoch 384/2000: loss: 0.14984624087810516, auc: 0.9646234315013784\n",
      "Epoch 385/2000: loss: 0.14958664774894714, auc: 0.9646234315013784\n",
      "Epoch 386/2000: loss: 0.14932820200920105, auc: 0.9646234315013784\n",
      "Epoch 387/2000: loss: 0.1490708887577057, auc: 0.9646234315013784\n",
      "Epoch 388/2000: loss: 0.14881472289562225, auc: 0.9646234315013784\n",
      "Epoch 389/2000: loss: 0.14855967462062836, auc: 0.9646234315013784\n",
      "Epoch 390/2000: loss: 0.1483057737350464, auc: 0.9646234315013784\n",
      "Epoch 391/2000: loss: 0.14805299043655396, auc: 0.9646234315013784\n",
      "Epoch 392/2000: loss: 0.14780132472515106, auc: 0.9646234315013784\n",
      "Epoch 393/2000: loss: 0.14755073189735413, auc: 0.9646234315013784\n",
      "Epoch 394/2000: loss: 0.14730127155780792, auc: 0.9646234315013784\n",
      "Epoch 395/2000: loss: 0.14705292880535126, auc: 0.9646234315013784\n",
      "Epoch 396/2000: loss: 0.14680562913417816, auc: 0.9646234315013784\n",
      "Epoch 397/2000: loss: 0.14655941724777222, auc: 0.9646234315013784\n",
      "Epoch 398/2000: loss: 0.14631427824497223, auc: 0.9646234315013784\n",
      "Epoch 399/2000: loss: 0.14607024192810059, auc: 0.9646234315013784\n",
      "Epoch 400/2000: loss: 0.14582720398902893, auc: 0.9646234315013784\n",
      "Epoch 401/2000: loss: 0.14558525383472443, auc: 0.9646234315013785\n",
      "Epoch 402/2000: loss: 0.14534437656402588, auc: 0.9646234315013784\n",
      "Epoch 403/2000: loss: 0.14510449767112732, auc: 0.9646234315013785\n",
      "Epoch 404/2000: loss: 0.14486566185951233, auc: 0.9646234315013785\n",
      "Epoch 405/2000: loss: 0.1446278840303421, auc: 0.9646234315013785\n",
      "Epoch 406/2000: loss: 0.14439108967781067, auc: 0.9646234315013784\n",
      "Epoch 407/2000: loss: 0.1441553384065628, auc: 0.9646234315013784\n",
      "Epoch 408/2000: loss: 0.14392058551311493, auc: 0.9646234315013784\n",
      "Epoch 409/2000: loss: 0.14368684589862823, auc: 0.9646234315013785\n",
      "Epoch 410/2000: loss: 0.14345408976078033, auc: 0.9646234315013785\n",
      "Epoch 411/2000: loss: 0.1432223618030548, auc: 0.9646234315013785\n",
      "Epoch 412/2000: loss: 0.1429915875196457, auc: 0.9646234315013784\n",
      "Epoch 413/2000: loss: 0.14276178181171417, auc: 0.9646234315013784\n",
      "Epoch 414/2000: loss: 0.14253298938274384, auc: 0.9646234315013784\n",
      "Epoch 415/2000: loss: 0.1423051655292511, auc: 0.9646234315013784\n",
      "Epoch 416/2000: loss: 0.14207828044891357, auc: 0.9646234315013783\n",
      "Epoch 417/2000: loss: 0.14185236394405365, auc: 0.9646234315013784\n",
      "Epoch 418/2000: loss: 0.14162738621234894, auc: 0.9646234315013784\n",
      "Epoch 419/2000: loss: 0.14140337705612183, auc: 0.9646234315013784\n",
      "Epoch 420/2000: loss: 0.14118032157421112, auc: 0.9646234315013785\n",
      "Epoch 421/2000: loss: 0.14095816016197205, auc: 0.9646234315013784\n",
      "Epoch 422/2000: loss: 0.14073695242404938, auc: 0.9646234315013784\n",
      "Epoch 423/2000: loss: 0.14051668345928192, auc: 0.9646234315013785\n",
      "Epoch 424/2000: loss: 0.1402973234653473, auc: 0.9645976656102652\n",
      "Epoch 425/2000: loss: 0.1400788575410843, auc: 0.964597665610265\n",
      "Epoch 426/2000: loss: 0.1398613154888153, auc: 0.964597665610265\n",
      "Epoch 427/2000: loss: 0.13964469730854034, auc: 0.964597665610265\n",
      "Epoch 428/2000: loss: 0.13942895829677582, auc: 0.964597665610265\n",
      "Epoch 429/2000: loss: 0.13921409845352173, auc: 0.964597665610265\n",
      "Epoch 430/2000: loss: 0.13900016248226166, auc: 0.9645976656102652\n",
      "Epoch 431/2000: loss: 0.13878709077835083, auc: 0.9645976656102652\n",
      "Epoch 432/2000: loss: 0.13857488334178925, auc: 0.964597665610265\n",
      "Epoch 433/2000: loss: 0.1383635699748993, auc: 0.9645976656102652\n",
      "Epoch 434/2000: loss: 0.13815315067768097, auc: 0.964597665610265\n",
      "Epoch 435/2000: loss: 0.13794353604316711, auc: 0.964597665610265\n",
      "Epoch 436/2000: loss: 0.13773483037948608, auc: 0.964597665610265\n",
      "Epoch 437/2000: loss: 0.1375269591808319, auc: 0.9645976656102652\n",
      "Epoch 438/2000: loss: 0.1373199224472046, auc: 0.9645976656102652\n",
      "Epoch 439/2000: loss: 0.13711373507976532, auc: 0.9645976656102652\n",
      "Epoch 440/2000: loss: 0.1369083821773529, auc: 0.9645718997191518\n",
      "Epoch 441/2000: loss: 0.13670389354228973, auc: 0.9645718997191518\n",
      "Epoch 442/2000: loss: 0.13650020956993103, auc: 0.9645718997191518\n",
      "Epoch 443/2000: loss: 0.13629736006259918, auc: 0.9645718997191517\n",
      "Epoch 444/2000: loss: 0.136095330119133, auc: 0.9645718997191518\n",
      "Epoch 445/2000: loss: 0.13589410483837128, auc: 0.9645718997191518\n",
      "Epoch 446/2000: loss: 0.13569369912147522, auc: 0.9645718997191518\n",
      "Epoch 447/2000: loss: 0.13549409806728363, auc: 0.9645718997191518\n",
      "Epoch 448/2000: loss: 0.1352953165769577, auc: 0.9645718997191518\n",
      "Epoch 449/2000: loss: 0.13509730994701385, auc: 0.9645718997191518\n",
      "Epoch 450/2000: loss: 0.13490012288093567, auc: 0.9645718997191518\n",
      "Epoch 451/2000: loss: 0.13470369577407837, auc: 0.9645718997191518\n",
      "Epoch 452/2000: loss: 0.13450805842876434, auc: 0.9645718997191518\n",
      "Epoch 453/2000: loss: 0.1343132108449936, auc: 0.9645718997191518\n",
      "Epoch 454/2000: loss: 0.13411913812160492, auc: 0.9645718997191518\n",
      "Epoch 455/2000: loss: 0.13392584025859833, auc: 0.9645718997191518\n",
      "Epoch 456/2000: loss: 0.13373331725597382, auc: 0.9645718997191518\n",
      "Epoch 457/2000: loss: 0.1335415244102478, auc: 0.9645718997191518\n",
      "Epoch 458/2000: loss: 0.13335052132606506, auc: 0.9645718997191518\n",
      "Epoch 459/2000: loss: 0.13316026329994202, auc: 0.9645718997191518\n",
      "Epoch 460/2000: loss: 0.13297073543071747, auc: 0.9645718997191518\n",
      "Epoch 461/2000: loss: 0.1327819973230362, auc: 0.9645718997191518\n",
      "Epoch 462/2000: loss: 0.13259395956993103, auc: 0.9645718997191518\n",
      "Epoch 463/2000: loss: 0.13240668177604675, auc: 0.9645718997191518\n",
      "Epoch 464/2000: loss: 0.13222013413906097, auc: 0.9645718997191518\n",
      "Epoch 465/2000: loss: 0.1320343315601349, auc: 0.9645718997191518\n",
      "Epoch 466/2000: loss: 0.1318492293357849, auc: 0.9645718997191518\n",
      "Epoch 467/2000: loss: 0.13166485726833344, auc: 0.9645718997191518\n",
      "Epoch 468/2000: loss: 0.13148123025894165, auc: 0.9645718997191518\n",
      "Epoch 469/2000: loss: 0.13129828870296478, auc: 0.9645718997191517\n",
      "Epoch 470/2000: loss: 0.13111606240272522, auc: 0.9645718997191518\n",
      "Epoch 471/2000: loss: 0.13093455135822296, auc: 0.9645718997191518\n",
      "Epoch 472/2000: loss: 0.130753755569458, auc: 0.9645718997191518\n",
      "Epoch 473/2000: loss: 0.13057363033294678, auc: 0.9645718997191518\n",
      "Epoch 474/2000: loss: 0.13039420545101166, auc: 0.9645718997191518\n",
      "Epoch 475/2000: loss: 0.13021548092365265, auc: 0.9645718997191518\n",
      "Epoch 476/2000: loss: 0.13003745675086975, auc: 0.9645718997191518\n",
      "Epoch 477/2000: loss: 0.1298600733280182, auc: 0.9645718997191518\n",
      "Epoch 478/2000: loss: 0.12968340516090393, auc: 0.9645718997191518\n",
      "Epoch 479/2000: loss: 0.1295074224472046, auc: 0.9645718997191518\n",
      "Epoch 480/2000: loss: 0.12933209538459778, auc: 0.9645718997191518\n",
      "Epoch 481/2000: loss: 0.12915745377540588, auc: 0.9645718997191518\n",
      "Epoch 482/2000: loss: 0.12898345291614532, auc: 0.9645718997191518\n",
      "Epoch 483/2000: loss: 0.12881013751029968, auc: 0.9645718997191518\n",
      "Epoch 484/2000: loss: 0.12863746285438538, auc: 0.9645718997191518\n",
      "Epoch 485/2000: loss: 0.128465473651886, auc: 0.9645718997191518\n",
      "Epoch 486/2000: loss: 0.12829412519931793, auc: 0.9645718997191518\n",
      "Epoch 487/2000: loss: 0.12812340259552002, auc: 0.9645718997191518\n",
      "Epoch 488/2000: loss: 0.12795335054397583, auc: 0.9645718997191518\n",
      "Epoch 489/2000: loss: 0.12778393924236298, auc: 0.9645718997191518\n",
      "Epoch 490/2000: loss: 0.12761516869068146, auc: 0.9645718997191518\n",
      "Epoch 491/2000: loss: 0.12744702398777008, auc: 0.9645718997191518\n",
      "Epoch 492/2000: loss: 0.12727953493595123, auc: 0.9645718997191518\n",
      "Epoch 493/2000: loss: 0.12711264193058014, auc: 0.9645718997191518\n",
      "Epoch 494/2000: loss: 0.1269463747739792, auc: 0.9645718997191518\n",
      "Epoch 495/2000: loss: 0.12678073346614838, auc: 0.9645718997191518\n",
      "Epoch 496/2000: loss: 0.1266157329082489, auc: 0.9645718997191518\n",
      "Epoch 497/2000: loss: 0.126451313495636, auc: 0.9645718997191517\n",
      "Epoch 498/2000: loss: 0.1262875497341156, auc: 0.9645718997191518\n",
      "Epoch 499/2000: loss: 0.12612438201904297, auc: 0.9645718997191518\n",
      "Epoch 500/2000: loss: 0.1259617954492569, auc: 0.9645718997191518\n",
      "Epoch 501/2000: loss: 0.12579983472824097, auc: 0.964597665610265\n",
      "Epoch 502/2000: loss: 0.1256384700536728, auc: 0.9645976656102652\n",
      "Epoch 503/2000: loss: 0.12547771632671356, auc: 0.964597665610265\n",
      "Epoch 504/2000: loss: 0.1253175288438797, auc: 0.9645976656102652\n",
      "Epoch 505/2000: loss: 0.1251579374074936, auc: 0.9645976656102652\n",
      "Epoch 506/2000: loss: 0.12499895691871643, auc: 0.9645976656102652\n",
      "Epoch 507/2000: loss: 0.12484055012464523, auc: 0.9645976656102652\n",
      "Epoch 508/2000: loss: 0.1246827244758606, auc: 0.9645976656102652\n",
      "Epoch 509/2000: loss: 0.12452547252178192, auc: 0.964597665610265\n",
      "Epoch 510/2000: loss: 0.12436880171298981, auc: 0.9645976656102652\n",
      "Epoch 511/2000: loss: 0.12421271204948425, auc: 0.9645976656102652\n",
      "Epoch 512/2000: loss: 0.12405716627836227, auc: 0.9645976656102652\n",
      "Epoch 513/2000: loss: 0.12390220910310745, auc: 0.9645976656102652\n",
      "Epoch 514/2000: loss: 0.123747818171978, auc: 0.9645976656102652\n",
      "Epoch 515/2000: loss: 0.12359397858381271, auc: 0.9645976656102652\n",
      "Epoch 516/2000: loss: 0.1234406977891922, auc: 0.9645976656102652\n",
      "Epoch 517/2000: loss: 0.12328798323869705, auc: 0.9645976656102652\n",
      "Epoch 518/2000: loss: 0.12313580513000488, auc: 0.9645976656102652\n",
      "Epoch 519/2000: loss: 0.12298417836427689, auc: 0.964597665610265\n",
      "Epoch 520/2000: loss: 0.12283311039209366, auc: 0.9645718997191518\n",
      "Epoch 521/2000: loss: 0.1226825937628746, auc: 0.9645718997191518\n",
      "Epoch 522/2000: loss: 0.12253260612487793, auc: 0.9645718997191518\n",
      "Epoch 523/2000: loss: 0.12238316237926483, auc: 0.9645718997191518\n",
      "Epoch 524/2000: loss: 0.12223425507545471, auc: 0.9645718997191518\n",
      "Epoch 525/2000: loss: 0.12208588421344757, auc: 0.9645718997191518\n",
      "Epoch 526/2000: loss: 0.1219380646944046, auc: 0.9645718997191518\n",
      "Epoch 527/2000: loss: 0.12179072946310043, auc: 0.9645718997191518\n",
      "Epoch 528/2000: loss: 0.12164394557476044, auc: 0.9645718997191518\n",
      "Epoch 529/2000: loss: 0.12149768322706223, auc: 0.9645718997191518\n",
      "Epoch 530/2000: loss: 0.1213519349694252, auc: 0.9645718997191518\n",
      "Epoch 531/2000: loss: 0.12120670080184937, auc: 0.9645718997191518\n",
      "Epoch 532/2000: loss: 0.12106198817491531, auc: 0.9645718997191517\n",
      "Epoch 533/2000: loss: 0.12091779708862305, auc: 0.9645718997191518\n",
      "Epoch 534/2000: loss: 0.12077413499355316, auc: 0.9645461338280384\n",
      "Epoch 535/2000: loss: 0.12063094228506088, auc: 0.9645461338280384\n",
      "Epoch 536/2000: loss: 0.12048827856779099, auc: 0.9645461338280384\n",
      "Epoch 537/2000: loss: 0.12034611403942108, auc: 0.9645461338280384\n",
      "Epoch 538/2000: loss: 0.12020443379878998, auc: 0.9645461338280384\n",
      "Epoch 539/2000: loss: 0.12006326019763947, auc: 0.9645461338280384\n",
      "Epoch 540/2000: loss: 0.11992259323596954, auc: 0.9645461338280384\n",
      "Epoch 541/2000: loss: 0.11978241056203842, auc: 0.9645461338280384\n",
      "Epoch 542/2000: loss: 0.11964274197816849, auc: 0.9645461338280384\n",
      "Epoch 543/2000: loss: 0.11950353533029556, auc: 0.9645461338280384\n",
      "Epoch 544/2000: loss: 0.11936481297016144, auc: 0.9645461338280386\n",
      "Epoch 545/2000: loss: 0.11922656744718552, auc: 0.9645461338280384\n",
      "Epoch 546/2000: loss: 0.11908882856369019, auc: 0.9645461338280384\n",
      "Epoch 547/2000: loss: 0.11895155906677246, auc: 0.9645461338280384\n",
      "Epoch 548/2000: loss: 0.11881476640701294, auc: 0.9645461338280384\n",
      "Epoch 549/2000: loss: 0.11867845058441162, auc: 0.9645461338280384\n",
      "Epoch 550/2000: loss: 0.11854259669780731, auc: 0.9645461338280384\n",
      "Epoch 551/2000: loss: 0.1184072345495224, auc: 0.9645461338280384\n",
      "Epoch 552/2000: loss: 0.1182723268866539, auc: 0.9645461338280384\n",
      "Epoch 553/2000: loss: 0.11813787370920181, auc: 0.9645461338280386\n",
      "Epoch 554/2000: loss: 0.11800388991832733, auc: 0.9645461338280384\n",
      "Epoch 555/2000: loss: 0.11787037551403046, auc: 0.9645461338280386\n",
      "Epoch 556/2000: loss: 0.11773732304573059, auc: 0.9645461338280384\n",
      "Epoch 557/2000: loss: 0.11760471761226654, auc: 0.9645461338280384\n",
      "Epoch 558/2000: loss: 0.1174725741147995, auc: 0.9645461338280384\n",
      "Epoch 559/2000: loss: 0.11734087765216827, auc: 0.9645461338280384\n",
      "Epoch 560/2000: loss: 0.11720963567495346, auc: 0.9645461338280384\n",
      "Epoch 561/2000: loss: 0.11707883328199387, auc: 0.9645461338280384\n",
      "Epoch 562/2000: loss: 0.11694847792387009, auc: 0.9645461338280384\n",
      "Epoch 563/2000: loss: 0.11681856960058212, auc: 0.9645461338280384\n",
      "Epoch 564/2000: loss: 0.11668910831212997, auc: 0.9645461338280386\n",
      "Epoch 565/2000: loss: 0.11656007915735245, auc: 0.9645461338280384\n",
      "Epoch 566/2000: loss: 0.11643150448799133, auc: 0.9645461338280384\n",
      "Epoch 567/2000: loss: 0.11630333214998245, auc: 0.9645461338280384\n",
      "Epoch 568/2000: loss: 0.11617561429738998, auc: 0.9645461338280384\n",
      "Epoch 569/2000: loss: 0.11604832112789154, auc: 0.9645461338280384\n",
      "Epoch 570/2000: loss: 0.11592147499322891, auc: 0.9645203679369251\n",
      "Epoch 571/2000: loss: 0.11579504609107971, auc: 0.9645203679369251\n",
      "Epoch 572/2000: loss: 0.11566902697086334, auc: 0.9645203679369251\n",
      "Epoch 573/2000: loss: 0.11554345488548279, auc: 0.9645203679369252\n",
      "Epoch 574/2000: loss: 0.11541827768087387, auc: 0.9645203679369251\n",
      "Epoch 575/2000: loss: 0.11529355496168137, auc: 0.9645203679369251\n",
      "Epoch 576/2000: loss: 0.1151692271232605, auc: 0.9645203679369251\n",
      "Epoch 577/2000: loss: 0.11504530906677246, auc: 0.9645203679369251\n",
      "Epoch 578/2000: loss: 0.11492182314395905, auc: 0.9645203679369251\n",
      "Epoch 579/2000: loss: 0.11479871720075607, auc: 0.9645203679369251\n",
      "Epoch 580/2000: loss: 0.11467605829238892, auc: 0.9645203679369252\n",
      "Epoch 581/2000: loss: 0.1145537868142128, auc: 0.9645203679369252\n",
      "Epoch 582/2000: loss: 0.11443192511796951, auc: 0.9645203679369251\n",
      "Epoch 583/2000: loss: 0.11431045085191727, auc: 0.9645203679369251\n",
      "Epoch 584/2000: loss: 0.11418940871953964, auc: 0.9645203679369252\n",
      "Epoch 585/2000: loss: 0.11406876891851425, auc: 0.9645203679369251\n",
      "Epoch 586/2000: loss: 0.11394850164651871, auc: 0.9645203679369252\n",
      "Epoch 587/2000: loss: 0.11382865905761719, auc: 0.9645203679369251\n",
      "Epoch 588/2000: loss: 0.11370918899774551, auc: 0.9645203679369252\n",
      "Epoch 589/2000: loss: 0.11359010636806488, auc: 0.9645203679369252\n",
      "Epoch 590/2000: loss: 0.11347144842147827, auc: 0.9645203679369251\n",
      "Epoch 591/2000: loss: 0.11335315555334091, auc: 0.9645203679369251\n",
      "Epoch 592/2000: loss: 0.1132352277636528, auc: 0.9645203679369252\n",
      "Epoch 593/2000: loss: 0.11311770975589752, auc: 0.9645203679369252\n",
      "Epoch 594/2000: loss: 0.11300058662891388, auc: 0.9645203679369252\n",
      "Epoch 595/2000: loss: 0.11288384348154068, auc: 0.9645203679369251\n",
      "Epoch 596/2000: loss: 0.11276745796203613, auc: 0.9645203679369251\n",
      "Epoch 597/2000: loss: 0.11265148222446442, auc: 0.9645203679369252\n",
      "Epoch 598/2000: loss: 0.11253586411476135, auc: 0.9645203679369251\n",
      "Epoch 599/2000: loss: 0.11242063343524933, auc: 0.9645203679369252\n",
      "Epoch 600/2000: loss: 0.11230577528476715, auc: 0.9645203679369252\n",
      "Epoch 601/2000: loss: 0.11219128221273422, auc: 0.9645203679369252\n",
      "Epoch 602/2000: loss: 0.11207716166973114, auc: 0.9645203679369251\n",
      "Epoch 603/2000: loss: 0.11196340620517731, auc: 0.9645203679369251\n",
      "Epoch 604/2000: loss: 0.11185002326965332, auc: 0.9645203679369251\n",
      "Epoch 605/2000: loss: 0.11173699796199799, auc: 0.9645203679369251\n",
      "Epoch 606/2000: loss: 0.1116243451833725, auc: 0.9645203679369251\n",
      "Epoch 607/2000: loss: 0.11151205003261566, auc: 0.9645203679369251\n",
      "Epoch 608/2000: loss: 0.11140011996030807, auc: 0.9645203679369251\n",
      "Epoch 609/2000: loss: 0.11128854751586914, auc: 0.9645203679369251\n",
      "Epoch 610/2000: loss: 0.11117733269929886, auc: 0.9645203679369252\n",
      "Epoch 611/2000: loss: 0.11106646806001663, auc: 0.9645203679369251\n",
      "Epoch 612/2000: loss: 0.11095596104860306, auc: 0.9645203679369252\n",
      "Epoch 613/2000: loss: 0.11084580421447754, auc: 0.9645203679369251\n",
      "Epoch 614/2000: loss: 0.11073601245880127, auc: 0.9645203679369251\n",
      "Epoch 615/2000: loss: 0.11062654852867126, auc: 0.9645203679369252\n",
      "Epoch 616/2000: loss: 0.11051742732524872, auc: 0.9645203679369251\n",
      "Epoch 617/2000: loss: 0.11040869355201721, auc: 0.9645203679369251\n",
      "Epoch 618/2000: loss: 0.11030027270317078, auc: 0.9645203679369251\n",
      "Epoch 619/2000: loss: 0.1101922020316124, auc: 0.9645203679369251\n",
      "Epoch 620/2000: loss: 0.11008448898792267, auc: 0.9645203679369251\n",
      "Epoch 621/2000: loss: 0.10997708886861801, auc: 0.9645203679369252\n",
      "Epoch 622/2000: loss: 0.109870046377182, auc: 0.9645203679369251\n",
      "Epoch 623/2000: loss: 0.10976334661245346, auc: 0.9645203679369251\n",
      "Epoch 624/2000: loss: 0.10965695977210999, auc: 0.9645203679369252\n",
      "Epoch 625/2000: loss: 0.10955091565847397, auc: 0.9645203679369251\n",
      "Epoch 626/2000: loss: 0.1094452291727066, auc: 0.9645203679369251\n",
      "Epoch 627/2000: loss: 0.10933984071016312, auc: 0.9645203679369252\n",
      "Epoch 628/2000: loss: 0.10923478752374649, auc: 0.9645203679369251\n",
      "Epoch 629/2000: loss: 0.10913007706403732, auc: 0.9645203679369251\n",
      "Epoch 630/2000: loss: 0.10902568697929382, auc: 0.9645203679369251\n",
      "Epoch 631/2000: loss: 0.10892161726951599, auc: 0.9645203679369252\n",
      "Epoch 632/2000: loss: 0.10881787538528442, auc: 0.9645203679369252\n",
      "Epoch 633/2000: loss: 0.10871446132659912, auc: 0.9645203679369251\n",
      "Epoch 634/2000: loss: 0.10861136764287949, auc: 0.9645203679369251\n",
      "Epoch 635/2000: loss: 0.10850860178470612, auc: 0.9645203679369251\n",
      "Epoch 636/2000: loss: 0.10840613394975662, auc: 0.9645203679369252\n",
      "Epoch 637/2000: loss: 0.1083039939403534, auc: 0.9645203679369251\n",
      "Epoch 638/2000: loss: 0.10820216685533524, auc: 0.9645203679369252\n",
      "Epoch 639/2000: loss: 0.10810066759586334, auc: 0.9645203679369252\n",
      "Epoch 640/2000: loss: 0.10799948126077652, auc: 0.9645203679369251\n",
      "Epoch 641/2000: loss: 0.10789858549833298, auc: 0.9645203679369251\n",
      "Epoch 642/2000: loss: 0.1077980175614357, auc: 0.9645203679369251\n",
      "Epoch 643/2000: loss: 0.10769776254892349, auc: 0.9645203679369251\n",
      "Epoch 644/2000: loss: 0.10759781301021576, auc: 0.9645203679369251\n",
      "Epoch 645/2000: loss: 0.1074981614947319, auc: 0.9645203679369251\n",
      "Epoch 646/2000: loss: 0.10739881545305252, auc: 0.9645203679369252\n",
      "Epoch 647/2000: loss: 0.10729978233575821, auc: 0.9645203679369251\n",
      "Epoch 648/2000: loss: 0.10720104724168777, auc: 0.9645203679369251\n",
      "Epoch 649/2000: loss: 0.10710260272026062, auc: 0.9645203679369252\n",
      "Epoch 650/2000: loss: 0.10700448602437973, auc: 0.9645203679369252\n",
      "Epoch 651/2000: loss: 0.10690664499998093, auc: 0.9645203679369252\n",
      "Epoch 652/2000: loss: 0.1068091094493866, auc: 0.9645203679369251\n",
      "Epoch 653/2000: loss: 0.10671187192201614, auc: 0.9645203679369252\n",
      "Epoch 654/2000: loss: 0.10661491751670837, auc: 0.9645203679369252\n",
      "Epoch 655/2000: loss: 0.10651828348636627, auc: 0.9645203679369252\n",
      "Epoch 656/2000: loss: 0.10642191022634506, auc: 0.9645203679369251\n",
      "Epoch 657/2000: loss: 0.10632585734128952, auc: 0.9645203679369251\n",
      "Epoch 658/2000: loss: 0.10623007267713547, auc: 0.9645203679369251\n",
      "Epoch 659/2000: loss: 0.10613460093736649, auc: 0.9645203679369252\n",
      "Epoch 660/2000: loss: 0.1060393899679184, auc: 0.9645203679369252\n",
      "Epoch 661/2000: loss: 0.10594447702169418, auc: 0.9645203679369251\n",
      "Epoch 662/2000: loss: 0.10584985464811325, auc: 0.9645203679369252\n",
      "Epoch 663/2000: loss: 0.105755515396595, auc: 0.9645203679369252\n",
      "Epoch 664/2000: loss: 0.10566143691539764, auc: 0.9645203679369252\n",
      "Epoch 665/2000: loss: 0.10556767135858536, auc: 0.9645203679369252\n",
      "Epoch 666/2000: loss: 0.10547417402267456, auc: 0.9645203679369251\n",
      "Epoch 667/2000: loss: 0.10538095235824585, auc: 0.9645203679369251\n",
      "Epoch 668/2000: loss: 0.10528802126646042, auc: 0.9645203679369251\n",
      "Epoch 669/2000: loss: 0.10519535094499588, auc: 0.9645203679369251\n",
      "Epoch 670/2000: loss: 0.10510296374559402, auc: 0.9645203679369251\n",
      "Epoch 671/2000: loss: 0.10501086711883545, auc: 0.9645203679369251\n",
      "Epoch 672/2000: loss: 0.10491902381181717, auc: 0.9645203679369251\n",
      "Epoch 673/2000: loss: 0.10482744127511978, auc: 0.9645203679369252\n",
      "Epoch 674/2000: loss: 0.10473615676164627, auc: 0.9645203679369251\n",
      "Epoch 675/2000: loss: 0.10464514046907425, auc: 0.9645203679369251\n",
      "Epoch 676/2000: loss: 0.10455439239740372, auc: 0.9645203679369251\n",
      "Epoch 677/2000: loss: 0.10446389764547348, auc: 0.9645203679369252\n",
      "Epoch 678/2000: loss: 0.10437367856502533, auc: 0.9645203679369251\n",
      "Epoch 679/2000: loss: 0.10428373515605927, auc: 0.9645203679369252\n",
      "Epoch 680/2000: loss: 0.10419405996799469, auc: 0.9645203679369251\n",
      "Epoch 681/2000: loss: 0.10410463064908981, auc: 0.9645203679369251\n",
      "Epoch 682/2000: loss: 0.10401546955108643, auc: 0.9645203679369252\n",
      "Epoch 683/2000: loss: 0.10392656177282333, auc: 0.9645203679369252\n",
      "Epoch 684/2000: loss: 0.10383792221546173, auc: 0.9645203679369251\n",
      "Epoch 685/2000: loss: 0.10374954342842102, auc: 0.9645203679369252\n",
      "Epoch 686/2000: loss: 0.1036614254117012, auc: 0.9645203679369251\n",
      "Epoch 687/2000: loss: 0.10357357561588287, auc: 0.9645203679369252\n",
      "Epoch 688/2000: loss: 0.10348595678806305, auc: 0.9645203679369251\n",
      "Epoch 689/2000: loss: 0.10339860618114471, auc: 0.9645203679369252\n",
      "Epoch 690/2000: loss: 0.10331149399280548, auc: 0.9645203679369251\n",
      "Epoch 691/2000: loss: 0.10322466492652893, auc: 0.9645203679369251\n",
      "Epoch 692/2000: loss: 0.10313808172941208, auc: 0.9645203679369252\n",
      "Epoch 693/2000: loss: 0.10305173695087433, auc: 0.9645203679369251\n",
      "Epoch 694/2000: loss: 0.10296563804149628, auc: 0.9645203679369252\n",
      "Epoch 695/2000: loss: 0.10287979990243912, auc: 0.9645203679369251\n",
      "Epoch 696/2000: loss: 0.10279422253370285, auc: 0.9645203679369251\n",
      "Epoch 697/2000: loss: 0.1027088537812233, auc: 0.9645203679369251\n",
      "Epoch 698/2000: loss: 0.10262377560138702, auc: 0.9645203679369251\n",
      "Epoch 699/2000: loss: 0.10253892093896866, auc: 0.9645203679369251\n",
      "Epoch 700/2000: loss: 0.1024543046951294, auc: 0.9645203679369251\n",
      "Epoch 701/2000: loss: 0.10236994177103043, auc: 0.9645203679369252\n",
      "Epoch 702/2000: loss: 0.10228581726551056, auc: 0.9645203679369251\n",
      "Epoch 703/2000: loss: 0.1022019311785698, auc: 0.9645203679369251\n",
      "Epoch 704/2000: loss: 0.10211829841136932, auc: 0.9645203679369252\n",
      "Epoch 705/2000: loss: 0.10203488171100616, auc: 0.9645203679369251\n",
      "Epoch 706/2000: loss: 0.10195174068212509, auc: 0.9645203679369251\n",
      "Epoch 707/2000: loss: 0.10186881572008133, auc: 0.9645203679369252\n",
      "Epoch 708/2000: loss: 0.10178612172603607, auc: 0.9645203679369251\n",
      "Epoch 709/2000: loss: 0.10170365869998932, auc: 0.9645203679369251\n",
      "Epoch 710/2000: loss: 0.10162143409252167, auc: 0.9645203679369251\n",
      "Epoch 711/2000: loss: 0.10153945535421371, auc: 0.9645203679369251\n",
      "Epoch 712/2000: loss: 0.10145770758390427, auc: 0.9645203679369252\n",
      "Epoch 713/2000: loss: 0.10137618333101273, auc: 0.9645203679369251\n",
      "Epoch 714/2000: loss: 0.10129489004611969, auc: 0.9645203679369252\n",
      "Epoch 715/2000: loss: 0.10121382027864456, auc: 0.9645203679369252\n",
      "Epoch 716/2000: loss: 0.10113299638032913, auc: 0.9645203679369251\n",
      "Epoch 717/2000: loss: 0.10105238854885101, auc: 0.9645203679369251\n",
      "Epoch 718/2000: loss: 0.10097202658653259, auc: 0.9645203679369251\n",
      "Epoch 719/2000: loss: 0.10089188069105148, auc: 0.9645203679369252\n",
      "Epoch 720/2000: loss: 0.10081194341182709, auc: 0.9645203679369252\n",
      "Epoch 721/2000: loss: 0.10073226690292358, auc: 0.9645203679369252\n",
      "Epoch 722/2000: loss: 0.100652776658535, auc: 0.9645203679369252\n",
      "Epoch 723/2000: loss: 0.10057353228330612, auc: 0.9645203679369252\n",
      "Epoch 724/2000: loss: 0.10049450397491455, auc: 0.9645203679369251\n",
      "Epoch 725/2000: loss: 0.10041569173336029, auc: 0.9645203679369252\n",
      "Epoch 726/2000: loss: 0.10033711045980453, auc: 0.9645203679369251\n",
      "Epoch 727/2000: loss: 0.1002587303519249, auc: 0.9645203679369251\n",
      "Epoch 728/2000: loss: 0.10018058866262436, auc: 0.9645203679369252\n",
      "Epoch 729/2000: loss: 0.10010266304016113, auc: 0.9645203679369251\n",
      "Epoch 730/2000: loss: 0.10002494603395462, auc: 0.9645203679369251\n",
      "Epoch 731/2000: loss: 0.09994744509458542, auc: 0.9645203679369252\n",
      "Epoch 732/2000: loss: 0.09987018257379532, auc: 0.9645203679369252\n",
      "Epoch 733/2000: loss: 0.09979309141635895, auc: 0.9645203679369252\n",
      "Epoch 734/2000: loss: 0.09971623867750168, auc: 0.9645203679369251\n",
      "Epoch 735/2000: loss: 0.09963961690664291, auc: 0.9645203679369251\n",
      "Epoch 736/2000: loss: 0.09956317394971848, auc: 0.9645203679369251\n",
      "Epoch 737/2000: loss: 0.09948695451021194, auc: 0.9645203679369252\n",
      "Epoch 738/2000: loss: 0.09941094368696213, auc: 0.9645203679369252\n",
      "Epoch 739/2000: loss: 0.09933514147996902, auc: 0.9645203679369252\n",
      "Epoch 740/2000: loss: 0.09925954788923264, auc: 0.9645203679369252\n",
      "Epoch 741/2000: loss: 0.09918417781591415, auc: 0.9645203679369251\n",
      "Epoch 742/2000: loss: 0.09910900145769119, auc: 0.9645203679369251\n",
      "Epoch 743/2000: loss: 0.09903403371572495, auc: 0.9645203679369251\n",
      "Epoch 744/2000: loss: 0.09895928204059601, auc: 0.9645203679369251\n",
      "Epoch 745/2000: loss: 0.09888472408056259, auc: 0.9645203679369252\n",
      "Epoch 746/2000: loss: 0.0988103598356247, auc: 0.9645203679369252\n",
      "Epoch 747/2000: loss: 0.0987362191081047, auc: 0.9645203679369251\n",
      "Epoch 748/2000: loss: 0.09866227209568024, auc: 0.9645203679369252\n",
      "Epoch 749/2000: loss: 0.09858854115009308, auc: 0.9645203679369251\n",
      "Epoch 750/2000: loss: 0.09851498156785965, auc: 0.9645203679369252\n",
      "Epoch 751/2000: loss: 0.09844163805246353, auc: 0.9645203679369252\n",
      "Epoch 752/2000: loss: 0.09836848825216293, auc: 0.9645203679369251\n",
      "Epoch 753/2000: loss: 0.09829553961753845, auc: 0.9645203679369252\n",
      "Epoch 754/2000: loss: 0.09822279214859009, auc: 0.9645203679369251\n",
      "Epoch 755/2000: loss: 0.09815025329589844, auc: 0.9645203679369252\n",
      "Epoch 756/2000: loss: 0.09807788580656052, auc: 0.9645203679369252\n",
      "Epoch 757/2000: loss: 0.0980057492852211, auc: 0.9645203679369252\n",
      "Epoch 758/2000: loss: 0.09793376177549362, auc: 0.9645203679369251\n",
      "Epoch 759/2000: loss: 0.09786199778318405, auc: 0.9645203679369252\n",
      "Epoch 760/2000: loss: 0.09779042750597, auc: 0.9645203679369251\n",
      "Epoch 761/2000: loss: 0.09771902859210968, auc: 0.9645203679369251\n",
      "Epoch 762/2000: loss: 0.09764785319566727, auc: 0.9645203679369251\n",
      "Epoch 763/2000: loss: 0.09757685661315918, auc: 0.9645203679369252\n",
      "Epoch 764/2000: loss: 0.09750603139400482, auc: 0.9645203679369251\n",
      "Epoch 765/2000: loss: 0.09743542224168777, auc: 0.9645203679369251\n",
      "Epoch 766/2000: loss: 0.09736499935388565, auc: 0.9645203679369252\n",
      "Epoch 767/2000: loss: 0.09729474037885666, auc: 0.9645203679369252\n",
      "Epoch 768/2000: loss: 0.09722468256950378, auc: 0.9645203679369251\n",
      "Epoch 769/2000: loss: 0.09715483337640762, auc: 0.9645203679369251\n",
      "Epoch 770/2000: loss: 0.09708516299724579, auc: 0.9645203679369252\n",
      "Epoch 771/2000: loss: 0.09701566398143768, auc: 0.9645203679369252\n",
      "Epoch 772/2000: loss: 0.0969463512301445, auc: 0.9645203679369251\n",
      "Epoch 773/2000: loss: 0.09687723219394684, auc: 0.9645203679369251\n",
      "Epoch 774/2000: loss: 0.0968082919716835, auc: 0.9645203679369252\n",
      "Epoch 775/2000: loss: 0.09673954546451569, auc: 0.9645203679369252\n",
      "Epoch 776/2000: loss: 0.09667094796895981, auc: 0.9645203679369251\n",
      "Epoch 777/2000: loss: 0.09660256654024124, auc: 0.9645203679369251\n",
      "Epoch 778/2000: loss: 0.0965343564748764, auc: 0.9645203679369251\n",
      "Epoch 779/2000: loss: 0.0964663177728653, auc: 0.9645203679369251\n",
      "Epoch 780/2000: loss: 0.09639845788478851, auc: 0.9645203679369251\n",
      "Epoch 781/2000: loss: 0.09633079171180725, auc: 0.9645203679369252\n",
      "Epoch 782/2000: loss: 0.09626331180334091, auc: 0.9645203679369251\n",
      "Epoch 783/2000: loss: 0.09619598090648651, auc: 0.9645203679369251\n",
      "Epoch 784/2000: loss: 0.09612884372472763, auc: 0.9645203679369251\n",
      "Epoch 785/2000: loss: 0.09606187790632248, auc: 0.9645203679369252\n",
      "Epoch 786/2000: loss: 0.09599509090185165, auc: 0.9645203679369252\n",
      "Epoch 787/2000: loss: 0.09592846781015396, auc: 0.9645203679369252\n",
      "Epoch 788/2000: loss: 0.09586204588413239, auc: 0.9645203679369252\n",
      "Epoch 789/2000: loss: 0.09579578042030334, auc: 0.9645203679369251\n",
      "Epoch 790/2000: loss: 0.09572967886924744, auc: 0.9645203679369251\n",
      "Epoch 791/2000: loss: 0.09566375613212585, auc: 0.9645203679369251\n",
      "Epoch 792/2000: loss: 0.0955980196595192, auc: 0.9645203679369252\n",
      "Epoch 793/2000: loss: 0.09553243964910507, auc: 0.9645203679369251\n",
      "Epoch 794/2000: loss: 0.09546703845262527, auc: 0.9645203679369251\n",
      "Epoch 795/2000: loss: 0.09540180116891861, auc: 0.9645203679369252\n",
      "Epoch 796/2000: loss: 0.09533673524856567, auc: 0.9645203679369252\n",
      "Epoch 797/2000: loss: 0.09527184069156647, auc: 0.9645203679369252\n",
      "Epoch 798/2000: loss: 0.0952071100473404, auc: 0.9645203679369251\n",
      "Epoch 799/2000: loss: 0.09514255076646805, auc: 0.9645203679369252\n",
      "Epoch 800/2000: loss: 0.09507815539836884, auc: 0.9645203679369251\n",
      "Epoch 801/2000: loss: 0.09501393139362335, auc: 0.9645203679369251\n",
      "Epoch 802/2000: loss: 0.094949871301651, auc: 0.9645203679369251\n",
      "Epoch 803/2000: loss: 0.09488597512245178, auc: 0.9645203679369251\n",
      "Epoch 804/2000: loss: 0.09482225030660629, auc: 0.9645203679369252\n",
      "Epoch 805/2000: loss: 0.09475867450237274, auc: 0.9645203679369251\n",
      "Epoch 806/2000: loss: 0.09469527006149292, auc: 0.9645203679369251\n",
      "Epoch 807/2000: loss: 0.09463202208280563, auc: 0.9645203679369252\n",
      "Epoch 808/2000: loss: 0.09456896781921387, auc: 0.9645203679369251\n",
      "Epoch 809/2000: loss: 0.09450604766607285, auc: 0.9645203679369251\n",
      "Epoch 810/2000: loss: 0.09444328397512436, auc: 0.9645203679369251\n",
      "Epoch 811/2000: loss: 0.0943806990981102, auc: 0.9645203679369252\n",
      "Epoch 812/2000: loss: 0.09431826323270798, auc: 0.9645203679369251\n",
      "Epoch 813/2000: loss: 0.09425599128007889, auc: 0.9645203679369252\n",
      "Epoch 814/2000: loss: 0.09419386833906174, auc: 0.9645203679369251\n",
      "Epoch 815/2000: loss: 0.09413192421197891, auc: 0.9645203679369252\n",
      "Epoch 816/2000: loss: 0.09407011419534683, auc: 0.9645203679369251\n",
      "Epoch 817/2000: loss: 0.09400846064090729, auc: 0.9645203679369251\n",
      "Epoch 818/2000: loss: 0.09394698590040207, auc: 0.9645203679369252\n",
      "Epoch 819/2000: loss: 0.09388566017150879, auc: 0.9645203679369252\n",
      "Epoch 820/2000: loss: 0.09382449090480804, auc: 0.9645203679369251\n",
      "Epoch 821/2000: loss: 0.09376345574855804, auc: 0.9645203679369252\n",
      "Epoch 822/2000: loss: 0.09370259195566177, auc: 0.9645203679369251\n",
      "Epoch 823/2000: loss: 0.09364189207553864, auc: 0.9645203679369252\n",
      "Epoch 824/2000: loss: 0.09358133375644684, auc: 0.9645203679369252\n",
      "Epoch 825/2000: loss: 0.09352093935012817, auc: 0.9645203679369252\n",
      "Epoch 826/2000: loss: 0.09346067905426025, auc: 0.9645203679369252\n",
      "Epoch 827/2000: loss: 0.09340057522058487, auc: 0.9645203679369252\n",
      "Epoch 828/2000: loss: 0.09334063529968262, auc: 0.9645203679369252\n",
      "Epoch 829/2000: loss: 0.09328082948923111, auc: 0.9645203679369251\n",
      "Epoch 830/2000: loss: 0.09322119504213333, auc: 0.9645203679369251\n",
      "Epoch 831/2000: loss: 0.0931616872549057, auc: 0.9645203679369252\n",
      "Epoch 832/2000: loss: 0.0931023433804512, auc: 0.9645203679369251\n",
      "Epoch 833/2000: loss: 0.09304313361644745, auc: 0.9645203679369251\n",
      "Epoch 834/2000: loss: 0.09298408031463623, auc: 0.9645203679369252\n",
      "Epoch 835/2000: loss: 0.09292518347501755, auc: 0.9645203679369251\n",
      "Epoch 836/2000: loss: 0.09286642074584961, auc: 0.9645203679369251\n",
      "Epoch 837/2000: loss: 0.09280780702829361, auc: 0.9645203679369252\n",
      "Epoch 838/2000: loss: 0.09274934977293015, auc: 0.9645203679369251\n",
      "Epoch 839/2000: loss: 0.09269103407859802, auc: 0.9645203679369252\n",
      "Epoch 840/2000: loss: 0.09263285994529724, auc: 0.9645203679369251\n",
      "Epoch 841/2000: loss: 0.0925748273730278, auc: 0.9645203679369251\n",
      "Epoch 842/2000: loss: 0.0925169438123703, auc: 0.9645203679369251\n",
      "Epoch 843/2000: loss: 0.09245919436216354, auc: 0.9645203679369252\n",
      "Epoch 844/2000: loss: 0.09240160882472992, auc: 0.9645203679369251\n",
      "Epoch 845/2000: loss: 0.09234414249658585, auc: 0.9645203679369251\n",
      "Epoch 846/2000: loss: 0.09228682518005371, auc: 0.9645203679369252\n",
      "Epoch 847/2000: loss: 0.09222966432571411, auc: 0.9645203679369251\n",
      "Epoch 848/2000: loss: 0.09217263013124466, auc: 0.9645203679369252\n",
      "Epoch 849/2000: loss: 0.09211573749780655, auc: 0.9645203679369252\n",
      "Epoch 850/2000: loss: 0.09205897897481918, auc: 0.9645203679369251\n",
      "Epoch 851/2000: loss: 0.09200237691402435, auc: 0.9645203679369252\n",
      "Epoch 852/2000: loss: 0.09194590151309967, auc: 0.9645203679369252\n",
      "Epoch 853/2000: loss: 0.09188956767320633, auc: 0.9645203679369252\n",
      "Epoch 854/2000: loss: 0.09183337539434433, auc: 0.9645203679369251\n",
      "Epoch 855/2000: loss: 0.09177731722593307, auc: 0.9645203679369251\n",
      "Epoch 856/2000: loss: 0.09172140061855316, auc: 0.9645203679369251\n",
      "Epoch 857/2000: loss: 0.0916656106710434, auc: 0.9645203679369251\n",
      "Epoch 858/2000: loss: 0.09160996973514557, auc: 0.9645203679369252\n",
      "Epoch 859/2000: loss: 0.09155447036027908, auc: 0.9645203679369251\n",
      "Epoch 860/2000: loss: 0.09149909764528275, auc: 0.9645203679369252\n",
      "Epoch 861/2000: loss: 0.09144385904073715, auc: 0.9645203679369251\n",
      "Epoch 862/2000: loss: 0.09138873964548111, auc: 0.9645203679369251\n",
      "Epoch 863/2000: loss: 0.0913337841629982, auc: 0.9645203679369252\n",
      "Epoch 864/2000: loss: 0.09127894043922424, auc: 0.9645203679369252\n",
      "Epoch 865/2000: loss: 0.09122423827648163, auc: 0.9645203679369252\n",
      "Epoch 866/2000: loss: 0.09116967767477036, auc: 0.9645203679369252\n",
      "Epoch 867/2000: loss: 0.09111522138118744, auc: 0.9645203679369251\n",
      "Epoch 868/2000: loss: 0.09106092154979706, auc: 0.9645203679369251\n",
      "Epoch 869/2000: loss: 0.09100674837827682, auc: 0.9645203679369252\n",
      "Epoch 870/2000: loss: 0.09095270186662674, auc: 0.9645203679369251\n",
      "Epoch 871/2000: loss: 0.0908987820148468, auc: 0.9645203679369251\n",
      "Epoch 872/2000: loss: 0.09084498882293701, auc: 0.9645461338280384\n",
      "Epoch 873/2000: loss: 0.09079133719205856, auc: 0.9645461338280383\n",
      "Epoch 874/2000: loss: 0.09073782712221146, auc: 0.9645461338280383\n",
      "Epoch 875/2000: loss: 0.0906844213604927, auc: 0.9645461338280383\n",
      "Epoch 876/2000: loss: 0.0906311571598053, auc: 0.9645461338280383\n",
      "Epoch 877/2000: loss: 0.09057800471782684, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 878/2000: loss: 0.09052499383687973, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 879/2000: loss: 0.09047213196754456, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 880/2000: loss: 0.09041935950517654, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 881/2000: loss: 0.09036672115325928, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 882/2000: loss: 0.09031420946121216, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 883/2000: loss: 0.09026183933019638, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 884/2000: loss: 0.09020958095788956, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 885/2000: loss: 0.09015745669603348, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 886/2000: loss: 0.09010544419288635, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 887/2000: loss: 0.09005356580018997, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 888/2000: loss: 0.09000180661678314, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 889/2000: loss: 0.08995017409324646, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 890/2000: loss: 0.08989864587783813, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 891/2000: loss: 0.08984725922346115, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 892/2000: loss: 0.08979598432779312, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 893/2000: loss: 0.08974483609199524, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 894/2000: loss: 0.08969380706548691, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 895/2000: loss: 0.08964288234710693, auc: 0.9645461338280384\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 896/2000: loss: 0.0895921066403389, auc: 0.9645461338280383\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Test Auc: 0.9768659236744344\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "# from dgl.nn import GraphConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from early_stop import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "number_class = 2\n",
    "linear_model = GCNConv(hiddle.shape[1], number_class) \n",
    "optimizer = Adam(linear_model.parameters(),lr = 1e-3)\n",
    "epochs = 2000\n",
    "early_stop = EarlyStopping(patience=20)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    linear_model.train()\n",
    "    logits = linear_model(hiddle, data.edge_index)\n",
    "    train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, 10]))\n",
    "    linear_model.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    linear_model.eval()\n",
    "    logits = linear_model(hiddle, data.edge_index)\n",
    "    val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, 10]))\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "    \n",
    "    early_stop(val_loss, linear_model)\n",
    "    if early_stop.early_stop == True:\n",
    "        print (\"Early stopping\")\n",
    "        break\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: loss: {train_loss}, auc: {auc}\")\n",
    "\n",
    "linear_model.eval()\n",
    "logits = linear_model(hiddle, data.edge_index)\n",
    "probs = logits.softmax(1)\n",
    "auc = roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "print (f\"Test Auc: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重构损失导向，检测融合嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "# from dgl.nn import GraphConv\n",
    "# from torch_geometric.nn import GCNConv\n",
    "from early_stop import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pygod.models.basic_nn import GCN\n",
    "\n",
    "\n",
    "class DOMINANT_recon(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 hid_dim,\n",
    "                 out_dim,\n",
    "                 decoder_layers,\n",
    "                 dropout,\n",
    "                 act):\n",
    "        super(DOMINANT_recon, self).__init__()\n",
    "\n",
    "        # split the number of layers for the encoder and decoders\n",
    "\n",
    "        self.attr_decoder = GCN(in_channels=in_dim,\n",
    "                                hidden_channels=hid_dim,\n",
    "                                num_layers=decoder_layers,\n",
    "                                out_channels=out_dim,\n",
    "                                dropout=dropout,\n",
    "                                act=act)\n",
    "\n",
    "        self.struct_decoder = GCN(in_channels=in_dim,\n",
    "                                  hidden_channels=hid_dim,\n",
    "                                  num_layers=decoder_layers,\n",
    "                                  out_channels=out_dim,\n",
    "                                  dropout=dropout,\n",
    "                                  act=act)\n",
    "\n",
    "    def forward(self, h, edge_index):\n",
    "        # decode feature matrix\n",
    "        x_ = self.attr_decoder(h, edge_index)\n",
    "        # print (x_.shape)\n",
    "        # decode adjacency matrix\n",
    "        h_ = self.struct_decoder(h, edge_index)\n",
    "        # print (h_.shape)\n",
    "        s_ = h_ @ h_.T\n",
    "        # return reconstructed matrices\n",
    "        return x_, s_, h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiddle = hid_dominate.detach()\n",
    "# hiddle = BW_hid.detach()\n",
    "# hiddle = GAT_hid.detach()\n",
    "# 融合的特征未曾归一化\n",
    "\n",
    "# 补充信息最好策略\n",
    "hiddle = torch.concat((1 * feature_normalize(GAT_hid.detach(),axis=0), 1 * feature_normalize(BW_hid.detach(),axis=0), 0.2 * feature_normalize(hid_dominate.detach(),axis=0)), axis=1)\n",
    "\n",
    "# 可学习化的参数\n",
    "# l_weight = [nn.Parameter(torch.randn([hiddle.shape[-1]], dtype=torch.float32, requires_grad=True))]\n",
    "# hiddle = torch.mul(hiddle, torch.softmax(*l_weight, dim = 0))\n",
    "# optimizer_ = Adam(l_weight, lr = 1e-3)\n",
    "\n",
    "# 计算hiddle特征之间的相似度，放缩到0-1之间 （专家系统）\n",
    "# hiddle = zero2one((1-cosine_distance(hiddle.T)).mean(axis=0))*hiddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.8370133638381958\n",
      "[Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)]\n",
      "Epoch 1/100: loss: 561.194091796875, test_auc: 0.6760683760683761\n",
      "Epoch 2/100: loss: 220.81646728515625, test_auc: 0.6513071895424838\n",
      "Epoch 3/100: loss: 150.86837768554688, test_auc: 0.605580693815988\n",
      "Epoch 4/100: loss: 133.05902099609375, test_auc: 0.6114379084967321\n",
      "Epoch 5/100: loss: 112.91568756103516, test_auc: 0.6219708396178985\n",
      "Epoch 6/100: loss: 84.1954345703125, test_auc: 0.6246354952237305\n",
      "Epoch 7/100: loss: 56.54951095581055, test_auc: 0.6240573152337858\n",
      "Epoch 8/100: loss: 35.511192321777344, test_auc: 0.6214429361488185\n",
      "Epoch 9/100: loss: 21.326702117919922, test_auc: 0.6216440422322775\n",
      "Epoch 10/100: loss: 12.360699653625488, test_auc: 0.6313976872800403\n",
      "Epoch 11/100: loss: 7.181057929992676, test_auc: 0.6531422825540473\n",
      "Epoch 12/100: loss: 4.632518768310547, test_auc: 0.664756158873806\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 13/100: loss: 4.845073223114014, test_auc: 0.5477878330819508\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 14/100: loss: 8.13667106628418, test_auc: 0.4071895424836601\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 15/100: loss: 11.9131498336792, test_auc: 0.3551030668677727\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 16/100: loss: 14.027332305908203, test_auc: 0.32745098039215687\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 17/100: loss: 14.007186889648438, test_auc: 0.3131724484665661\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 18/100: loss: 12.25311279296875, test_auc: 0.3149069884364002\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 19/100: loss: 9.520956993103027, test_auc: 0.3359477124183007\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 20/100: loss: 6.680136203765869, test_auc: 0.3838863750628456\n",
      "Epoch 21/100: loss: 4.457671642303467, test_auc: 0.4780794369029664\n",
      "Epoch 22/100: loss: 3.25471830368042, test_auc: 0.6104826546003017\n",
      "Epoch 23/100: loss: 2.9678807258605957, test_auc: 0.6886877828054299\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 24/100: loss: 3.114941120147705, test_auc: 0.6856963298139769\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 25/100: loss: 3.4064104557037354, test_auc: 0.6760180995475114\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 26/100: loss: 3.7867443561553955, test_auc: 0.6681246857717447\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 27/100: loss: 4.208916187286377, test_auc: 0.659703368526898\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 28/100: loss: 4.5728654861450195, test_auc: 0.6519859225741579\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 29/100: loss: 4.781983852386475, test_auc: 0.646480643539467\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 30/100: loss: 4.783099174499512, test_auc: 0.6464303670186023\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 31/100: loss: 4.579795837402344, test_auc: 0.6506535947712418\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 32/100: loss: 4.2261786460876465, test_auc: 0.6587732528909\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 33/100: loss: 3.8038699626922607, test_auc: 0.6694570135746607\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 34/100: loss: 3.3920018672943115, test_auc: 0.681649069884364\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 35/100: loss: 3.0435047149658203, test_auc: 0.6941930618401206\n",
      "Epoch 36/100: loss: 2.7736949920654297, test_auc: 0.7076671694318754\n",
      "Epoch 37/100: loss: 2.5630264282226562, test_auc: 0.7199346405228759\n",
      "Epoch 38/100: loss: 2.3759894371032715, test_auc: 0.7325289089994973\n",
      "Epoch 39/100: loss: 2.184481143951416, test_auc: 0.744544997486174\n",
      "Epoch 40/100: loss: 1.9785746335983276, test_auc: 0.7576420311714429\n",
      "Epoch 41/100: loss: 1.7636374235153198, test_auc: 0.7710155857214681\n",
      "Epoch 42/100: loss: 1.5525120496749878, test_auc: 0.7835595776772247\n",
      "Epoch 43/100: loss: 1.35836923122406, test_auc: 0.7967320261437909\n",
      "Epoch 44/100: loss: 1.1902904510498047, test_auc: 0.811035696329814\n",
      "Epoch 45/100: loss: 1.0515704154968262, test_auc: 0.8264705882352941\n",
      "Epoch 46/100: loss: 0.9404371976852417, test_auc: 0.8435897435897436\n",
      "Epoch 47/100: loss: 0.8525855541229248, test_auc: 0.8621417797888387\n",
      "Epoch 48/100: loss: 0.7837663888931274, test_auc: 0.8798391151332328\n",
      "Epoch 49/100: loss: 0.7307880520820618, test_auc: 0.8968828557063852\n",
      "Epoch 50/100: loss: 0.6910065412521362, test_auc: 0.9142282554047261\n",
      "Epoch 51/100: loss: 0.6614850759506226, test_auc: 0.9267722473604827\n",
      "Epoch 52/100: loss: 0.6388868093490601, test_auc: 0.937833081950729\n",
      "Epoch 53/100: loss: 0.6202107071876526, test_auc: 0.9458019105077928\n",
      "Epoch 54/100: loss: 0.6036096811294556, test_auc: 0.9478129713423831\n",
      "Epoch 55/100: loss: 0.5885815024375916, test_auc: 0.9479135243841126\n",
      "Epoch 56/100: loss: 0.5754797458648682, test_auc: 0.9465309200603318\n",
      "Epoch 57/100: loss: 0.5647830963134766, test_auc: 0.9453242835595776\n",
      "Epoch 58/100: loss: 0.5566300749778748, test_auc: 0.9441176470588235\n",
      "Epoch 59/100: loss: 0.5507808327674866, test_auc: 0.9432378079436904\n",
      "Epoch 60/100: loss: 0.546791672706604, test_auc: 0.9429361488185017\n",
      "Epoch 61/100: loss: 0.5441704988479614, test_auc: 0.9427853192559075\n",
      "Epoch 62/100: loss: 0.5424805283546448, test_auc: 0.9427350427350427\n",
      "Epoch 63/100: loss: 0.5413853526115417, test_auc: 0.9427853192559075\n",
      "Epoch 64/100: loss: 0.5406411290168762, test_auc: 0.9425842131724484\n",
      "Epoch 65/100: loss: 0.5400766730308533, test_auc: 0.9426596279537456\n",
      "Epoch 66/100: loss: 0.5395782589912415, test_auc: 0.9427853192559075\n",
      "Epoch 67/100: loss: 0.5390836596488953, test_auc: 0.9428607340372047\n",
      "Epoch 68/100: loss: 0.538572371006012, test_auc: 0.9427350427350428\n",
      "Epoch 69/100: loss: 0.5380402207374573, test_auc: 0.9426093514328809\n",
      "Epoch 70/100: loss: 0.5374903082847595, test_auc: 0.942684766214178\n",
      "Epoch 71/100: loss: 0.53694087266922, test_auc: 0.9426596279537457\n",
      "Epoch 72/100: loss: 0.5364173650741577, test_auc: 0.9425339366515838\n",
      "Epoch 73/100: loss: 0.5359358191490173, test_auc: 0.9426093514328809\n",
      "Epoch 74/100: loss: 0.5354984402656555, test_auc: 0.9425087983911513\n",
      "Epoch 75/100: loss: 0.5351008772850037, test_auc: 0.9424585218702866\n",
      "Epoch 76/100: loss: 0.5347444415092468, test_auc: 0.9423831070889895\n",
      "Epoch 77/100: loss: 0.5344340801239014, test_auc: 0.9423579688285572\n",
      "Epoch 78/100: loss: 0.5341665148735046, test_auc: 0.9423579688285572\n",
      "Epoch 79/100: loss: 0.5339295268058777, test_auc: 0.9424585218702867\n",
      "Epoch 80/100: loss: 0.5337148904800415, test_auc: 0.9425842131724486\n",
      "Epoch 81/100: loss: 0.5335219502449036, test_auc: 0.9425590749120162\n",
      "Epoch 82/100: loss: 0.5333504676818848, test_auc: 0.9425842131724486\n",
      "Epoch 83/100: loss: 0.5331980586051941, test_auc: 0.9426093514328809\n",
      "Epoch 84/100: loss: 0.5330615043640137, test_auc: 0.9425842131724486\n",
      "Epoch 85/100: loss: 0.5329375863075256, test_auc: 0.9426093514328809\n",
      "Epoch 86/100: loss: 0.5328232645988464, test_auc: 0.9426847662141781\n",
      "Epoch 87/100: loss: 0.5327173471450806, test_auc: 0.9426596279537457\n",
      "Epoch 88/100: loss: 0.5326202511787415, test_auc: 0.9426596279537457\n",
      "Epoch 89/100: loss: 0.5325313806533813, test_auc: 0.9426847662141781\n",
      "Epoch 90/100: loss: 0.532448410987854, test_auc: 0.9427099044746103\n",
      "Epoch 91/100: loss: 0.532368540763855, test_auc: 0.9426847662141781\n",
      "Epoch 92/100: loss: 0.5322906970977783, test_auc: 0.9426847662141781\n",
      "Epoch 93/100: loss: 0.5322163105010986, test_auc: 0.9426847662141781\n",
      "Epoch 94/100: loss: 0.5321455597877502, test_auc: 0.9426847662141781\n",
      "Epoch 95/100: loss: 0.5320768356323242, test_auc: 0.9426847662141781\n",
      "Epoch 96/100: loss: 0.532008171081543, test_auc: 0.9426847662141781\n",
      "Epoch 97/100: loss: 0.5319388508796692, test_auc: 0.9427099044746104\n",
      "Epoch 98/100: loss: 0.5318701863288879, test_auc: 0.9427099044746104\n",
      "Epoch 99/100: loss: 0.5318027138710022, test_auc: 0.9427099044746104\n",
      "Epoch 100/100: loss: 0.5317358374595642, test_auc: 0.9427099044746104\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "hid_dim = 32\n",
    "decode_layer = 1\n",
    "# domin_recon_model = DOMINANT_recon(hiddle.shape[1], hid_dim, data.x.shape[1], decode_layer, dropout=0.3, act= F.relu)\n",
    "domin_recon_model = DOMINANT_recon(hiddle.shape[1], data.x.shape[1], data.x.shape[1], decode_layer, dropout=0.3, act= F.relu)\n",
    "\n",
    "optimizer = Adam(domin_recon_model.parameters(), lr = 5e-3, weight_decay=5e-4)\n",
    "epochs = 100\n",
    "early_stop = EarlyStopping(patience=20)\n",
    "\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "s = to_dense_adj(data.edge_index)[0]\n",
    "alpha = torch.std(s).detach() / (torch.std(data.x).detach() + torch.std(s).detach())\n",
    "print (f\"Alpha: {alpha}\")\n",
    "\n",
    "# l_weight = [nn.Parameter(torch.tensor(zero2one((1-cosine_distance(hiddle.T)).sum(axis=0))*20, dtype=torch.float32, requires_grad=True))]\n",
    "# l_weight = [nn.Parameter(torch.randn([hiddle.shape[-1]], dtype=torch.float32, requires_grad=True))]\n",
    "l_weight = [nn.Parameter(torch.ones([hiddle.shape[-1]], dtype=torch.float32, requires_grad=True))]\n",
    "b_weight = [nn.Parameter(torch.ones([3], dtype=torch.float32, requires_grad=True))]\n",
    "\n",
    "optimizer_ = Adam(l_weight, lr = 5e-2, weight_decay=5e-2)\n",
    "optimizer_b = Adam(b_weight, lr = 1e-1, weight_decay=5e-2)\n",
    "print (l_weight)\n",
    "def reco_loss_func(x, x_, s, s_):\n",
    "    # attribute reconstruction loss\n",
    "    diff_attribute = torch.pow(x - x_, 2)\n",
    "    attribute_errors = torch.sqrt(torch.sum(diff_attribute, 1))\n",
    "\n",
    "    # structure reconstruction loss\n",
    "    diff_structure = torch.pow(s - s_, 2)\n",
    "    structure_errors = torch.sqrt(torch.sum(diff_structure, 1))\n",
    "\n",
    "    score = alpha * attribute_errors \\\n",
    "            + (1 - alpha) * structure_errors\n",
    "    return score\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hiddle = torch.concat((b_weight[0][0] * feature_normalize(GAT_hid.detach(),axis=0), b_weight[0][1] * feature_normalize(BW_hid.detach(),axis=0), b_weight[0][2] * feature_normalize(hid_dominate.detach(),axis=0)), axis=1)\n",
    "    # hiddle = zero2one((1-cosine_distance(hiddle.T)).mean(axis=0))*hiddle\n",
    "    hiddle_ = torch.mul(hiddle, hiddle.shape[1]*torch.softmax(*l_weight, dim = 0))\n",
    "    # hiddle_ = torch.mul(hiddle, *l_weight)\n",
    "\n",
    "    domin_recon_model.train()\n",
    "    x_, s_, hid  = domin_recon_model(hiddle_, data.edge_index)\n",
    "    \n",
    "    nodes_loss = reco_loss_func(data.x, x_, s, s_)\n",
    "    train_loss = nodes_loss.mean()\n",
    "    nodes_loss_numpy = nodes_loss.detach().numpy()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_.zero_grad()\n",
    "    optimizer_b.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer_.step()\n",
    "    optimizer_b.step()\n",
    "    dropout_auc = roc_auc_score(data.y[data.test_mask].numpy(), nodes_loss_numpy[data.test_mask])\n",
    "    \n",
    "    early_stop(train_loss, domin_recon_model)\n",
    "    if early_stop.early_stop == True:\n",
    "        print (\"Early stopping\")\n",
    "        break\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: loss: {train_loss}, test_auc: {dropout_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([ 0.0080, -0.0008,  0.0118], requires_grad=True)]\n",
      "tensor([0.3339, 0.3310, 0.3351], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print (hiddle.shape[1]*torch.softmax(*l_weight, dim = 0))\n",
    "# hiddle = hid_dominate.detach()\n",
    "# hiddle = BW_hid.detach()\n",
    "print (b_weight)\n",
    "print (torch.softmax(*b_weight, dim = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 标签导向与重构导向，先完成重构导向普遍性的实验\n",
    "\n",
    "### 2. 特征包含无用，冗余以及互补信息。冗余与互补信息通过计算相似度矩阵完成；仍需完善对无用信息针对任务导向的舍弃（考虑通过网络自己学习）\n",
    "\n",
    "### 3. 一个案例实现，完成三个视角的信息融合，实现在生成，最小类以及原生异常数据集上的通用，且性能不差于任何一个已经存在的SOTA\n",
    "\n",
    "### 4. 一般化框架的构建，做出多组数据融合对比实验，实验做到这一步，基本就杀青了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **考虑到，表征学习与重构损失是独立训练的。表征的获取必须完成其他方法的完整运行，如此进行融合学习未免南辕北辙了**\n",
    "\n",
    "* **可以将表征学习与重构损失联合训练训练的。不能，一旦联合就变成 DOMINANTE 模型，任务上就不通用了**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000: Loss 0.3043\n",
      "Final Test AUC: 0.4690079151617613\n",
      "--------------------- The Embedding of Dominate have done!!! ------------------\n",
      "Epoch 1/100: val_loss: 0.6960328221321106, val_auc: 0.48437074829931975\n",
      "Epoch 2/100: val_loss: 0.6958662867546082, val_auc: 0.4855952380952381\n",
      "Epoch 3/100: val_loss: 0.6957038044929504, val_auc: 0.4876020408163265\n",
      "Epoch 4/100: val_loss: 0.6955432891845703, val_auc: 0.500297619047619\n",
      "Epoch 5/100: val_loss: 0.6953902840614319, val_auc: 0.5039625850340136\n",
      "Epoch 6/100: val_loss: 0.6952409744262695, val_auc: 0.5020918367346939\n",
      "Epoch 7/100: val_loss: 0.6950961947441101, val_auc: 0.5043537414965987\n",
      "Epoch 8/100: val_loss: 0.6949560046195984, val_auc: 0.5217687074829932\n",
      "Epoch 9/100: val_loss: 0.6948179602622986, val_auc: 0.5336904761904762\n",
      "Epoch 10/100: val_loss: 0.6946843862533569, val_auc: 0.5298554421768708\n",
      "Epoch 11/100: val_loss: 0.6945534944534302, val_auc: 0.5292857142857144\n",
      "Epoch 12/100: val_loss: 0.6944274306297302, val_auc: 0.5294387755102041\n",
      "Epoch 13/100: val_loss: 0.6943038105964661, val_auc: 0.5297874149659864\n",
      "Epoch 14/100: val_loss: 0.6941824555397034, val_auc: 0.5301700680272108\n",
      "Epoch 15/100: val_loss: 0.6940671801567078, val_auc: 0.5305612244897958\n",
      "Epoch 16/100: val_loss: 0.6939562559127808, val_auc: 0.5311054421768708\n",
      "Epoch 17/100: val_loss: 0.6938485503196716, val_auc: 0.5312755102040817\n",
      "Epoch 18/100: val_loss: 0.6937434673309326, val_auc: 0.5315136054421769\n",
      "Epoch 19/100: val_loss: 0.6936408281326294, val_auc: 0.5316156462585034\n",
      "Epoch 20/100: val_loss: 0.6935412883758545, val_auc: 0.5320408163265307\n",
      "Epoch 21/100: val_loss: 0.6934409737586975, val_auc: 0.5323639455782314\n",
      "Epoch 22/100: val_loss: 0.693342924118042, val_auc: 0.5328061224489797\n",
      "Epoch 23/100: val_loss: 0.6932480931282043, val_auc: 0.5332993197278912\n",
      "Epoch 24/100: val_loss: 0.6931563019752502, val_auc: 0.5336224489795918\n",
      "Epoch 25/100: val_loss: 0.6930674910545349, val_auc: 0.5342857142857143\n",
      "Epoch 26/100: val_loss: 0.6929823160171509, val_auc: 0.5346598639455783\n",
      "Epoch 27/100: val_loss: 0.6929002404212952, val_auc: 0.5350170068027211\n",
      "Epoch 28/100: val_loss: 0.6928216814994812, val_auc: 0.5351020408163265\n",
      "Epoch 29/100: val_loss: 0.6927487254142761, val_auc: 0.5351190476190477\n",
      "Epoch 30/100: val_loss: 0.6926795840263367, val_auc: 0.535\n",
      "Epoch 31/100: val_loss: 0.6926127672195435, val_auc: 0.5352721088435374\n",
      "Epoch 32/100: val_loss: 0.6925469040870667, val_auc: 0.5357482993197278\n",
      "Epoch 33/100: val_loss: 0.692482590675354, val_auc: 0.5362414965986395\n",
      "Epoch 34/100: val_loss: 0.6924203634262085, val_auc: 0.5365816326530612\n",
      "Epoch 35/100: val_loss: 0.6923603415489197, val_auc: 0.5370493197278912\n",
      "Epoch 36/100: val_loss: 0.6923019289970398, val_auc: 0.5375\n",
      "Epoch 37/100: val_loss: 0.6922439336776733, val_auc: 0.5379336734693877\n",
      "Epoch 38/100: val_loss: 0.6921858787536621, val_auc: 0.5383503401360544\n",
      "Epoch 39/100: val_loss: 0.6921276450157166, val_auc: 0.5387414965986395\n",
      "Epoch 40/100: val_loss: 0.6920680403709412, val_auc: 0.5393197278911565\n",
      "Epoch 41/100: val_loss: 0.6920077800750732, val_auc: 0.5397619047619047\n",
      "Epoch 42/100: val_loss: 0.6919475793838501, val_auc: 0.5403061224489797\n",
      "Epoch 43/100: val_loss: 0.6918874979019165, val_auc: 0.5409098639455783\n",
      "Epoch 44/100: val_loss: 0.6918268203735352, val_auc: 0.5413945578231293\n",
      "Epoch 45/100: val_loss: 0.6917657256126404, val_auc: 0.5423299319727891\n",
      "Epoch 46/100: val_loss: 0.6917039752006531, val_auc: 0.5430612244897959\n",
      "Epoch 47/100: val_loss: 0.6916416883468628, val_auc: 0.5436224489795919\n",
      "Epoch 48/100: val_loss: 0.6915795803070068, val_auc: 0.5444472789115646\n",
      "Epoch 49/100: val_loss: 0.6915172338485718, val_auc: 0.5451700680272109\n",
      "Epoch 50/100: val_loss: 0.6914547681808472, val_auc: 0.5457482993197279\n",
      "Epoch 51/100: val_loss: 0.6913926601409912, val_auc: 0.5462414965986394\n",
      "Epoch 52/100: val_loss: 0.69133061170578, val_auc: 0.546734693877551\n",
      "Epoch 53/100: val_loss: 0.6912687420845032, val_auc: 0.547219387755102\n",
      "Epoch 54/100: val_loss: 0.6912065744400024, val_auc: 0.547704081632653\n",
      "Epoch 55/100: val_loss: 0.6911438703536987, val_auc: 0.5482312925170069\n",
      "Epoch 56/100: val_loss: 0.691080629825592, val_auc: 0.5490136054421768\n",
      "Epoch 57/100: val_loss: 0.6910160779953003, val_auc: 0.5494557823129252\n",
      "Epoch 58/100: val_loss: 0.6909501552581787, val_auc: 0.55\n",
      "Epoch 59/100: val_loss: 0.69088214635849, val_auc: 0.5508163265306123\n",
      "Epoch 60/100: val_loss: 0.6908125281333923, val_auc: 0.5514625850340137\n",
      "Epoch 61/100: val_loss: 0.6907419562339783, val_auc: 0.5523299319727892\n",
      "Epoch 62/100: val_loss: 0.6906698346138, val_auc: 0.5530612244897959\n",
      "Epoch 63/100: val_loss: 0.6905959248542786, val_auc: 0.5538095238095239\n",
      "Epoch 64/100: val_loss: 0.6905198693275452, val_auc: 0.5545918367346939\n",
      "Epoch 65/100: val_loss: 0.6904423236846924, val_auc: 0.5552380952380953\n",
      "Epoch 66/100: val_loss: 0.6903631091117859, val_auc: 0.5561054421768707\n",
      "Epoch 67/100: val_loss: 0.6902817487716675, val_auc: 0.5567687074829931\n",
      "Epoch 68/100: val_loss: 0.6901984214782715, val_auc: 0.5579251700680272\n",
      "Epoch 69/100: val_loss: 0.6901131272315979, val_auc: 0.5589625850340136\n",
      "Epoch 70/100: val_loss: 0.6900259256362915, val_auc: 0.5596938775510204\n",
      "Epoch 71/100: val_loss: 0.689936637878418, val_auc: 0.5609353741496599\n",
      "Epoch 72/100: val_loss: 0.689845085144043, val_auc: 0.5622448979591838\n",
      "Epoch 73/100: val_loss: 0.689751386642456, val_auc: 0.5633843537414965\n",
      "Epoch 74/100: val_loss: 0.6896553635597229, val_auc: 0.5643197278911565\n",
      "Epoch 75/100: val_loss: 0.6895567178726196, val_auc: 0.5652721088435374\n",
      "Epoch 76/100: val_loss: 0.6894550919532776, val_auc: 0.5662925170068027\n",
      "Epoch 77/100: val_loss: 0.6893505454063416, val_auc: 0.5676020408163265\n",
      "Epoch 78/100: val_loss: 0.6892431974411011, val_auc: 0.5690561224489796\n",
      "Epoch 79/100: val_loss: 0.6891326308250427, val_auc: 0.5701700680272108\n",
      "Epoch 80/100: val_loss: 0.6890187859535217, val_auc: 0.5713945578231292\n",
      "Epoch 81/100: val_loss: 0.6889015436172485, val_auc: 0.5722789115646258\n",
      "Epoch 82/100: val_loss: 0.6887809634208679, val_auc: 0.573265306122449\n",
      "Epoch 83/100: val_loss: 0.6886568665504456, val_auc: 0.5747448979591837\n",
      "Epoch 84/100: val_loss: 0.688529372215271, val_auc: 0.5758163265306122\n",
      "Epoch 85/100: val_loss: 0.6883980631828308, val_auc: 0.5769557823129251\n",
      "Epoch 86/100: val_loss: 0.6882628798484802, val_auc: 0.5781632653061225\n",
      "Epoch 87/100: val_loss: 0.6881242990493774, val_auc: 0.5795918367346939\n",
      "Epoch 88/100: val_loss: 0.6879821419715881, val_auc: 0.5809013605442177\n",
      "Epoch 89/100: val_loss: 0.6878358125686646, val_auc: 0.5823299319727891\n",
      "Epoch 90/100: val_loss: 0.6876851916313171, val_auc: 0.5834268707482994\n",
      "Epoch 91/100: val_loss: 0.6875298023223877, val_auc: 0.5848809523809524\n",
      "Epoch 92/100: val_loss: 0.6873698830604553, val_auc: 0.5862925170068027\n",
      "Epoch 93/100: val_loss: 0.6872052550315857, val_auc: 0.5877040816326531\n",
      "Epoch 94/100: val_loss: 0.6870355606079102, val_auc: 0.5891326530612245\n",
      "Epoch 95/100: val_loss: 0.6868603825569153, val_auc: 0.5904591836734694\n",
      "Epoch 96/100: val_loss: 0.6866795420646667, val_auc: 0.5921428571428571\n",
      "Epoch 97/100: val_loss: 0.6864932179450989, val_auc: 0.5935544217687074\n",
      "Epoch 98/100: val_loss: 0.686301052570343, val_auc: 0.5951020408163264\n",
      "Epoch 99/100: val_loss: 0.686103343963623, val_auc: 0.5967346938775511\n",
      "Epoch 100/100: val_loss: 0.6858996152877808, val_auc: 0.598452380952381\n",
      "Final Test Auc: 0.6190501805886421\n",
      "--------------------- The Embedding of BWGNN have done!!! ------------------\n",
      "Epoch 1/100: val_loss: 0.692168116569519, val_auc: 0.8323469387755101\n",
      "Epoch 2/100: val_loss: 0.6911870837211609, val_auc: 0.9022023809523809\n",
      "Epoch 3/100: val_loss: 0.6900129318237305, val_auc: 0.9237244897959184\n",
      "Epoch 4/100: val_loss: 0.688667893409729, val_auc: 0.9310204081632654\n",
      "Epoch 5/100: val_loss: 0.6871971487998962, val_auc: 0.9351190476190476\n",
      "Epoch 6/100: val_loss: 0.6856333017349243, val_auc: 0.9375340136054422\n",
      "Epoch 7/100: val_loss: 0.6840052604675293, val_auc: 0.9389455782312924\n",
      "Epoch 8/100: val_loss: 0.6823282241821289, val_auc: 0.939812925170068\n",
      "Epoch 9/100: val_loss: 0.6805980801582336, val_auc: 0.9407823129251702\n",
      "Epoch 10/100: val_loss: 0.6788182258605957, val_auc: 0.9416496598639456\n",
      "Epoch 11/100: val_loss: 0.6769835352897644, val_auc: 0.9422278911564627\n",
      "Epoch 12/100: val_loss: 0.6750954985618591, val_auc: 0.9427891156462584\n",
      "Epoch 13/100: val_loss: 0.673163115978241, val_auc: 0.9431802721088436\n",
      "Epoch 14/100: val_loss: 0.6711905598640442, val_auc: 0.9434013605442176\n",
      "Epoch 15/100: val_loss: 0.6691762208938599, val_auc: 0.9434693877551021\n",
      "Epoch 16/100: val_loss: 0.6671236753463745, val_auc: 0.9436989795918367\n",
      "Epoch 17/100: val_loss: 0.6650308966636658, val_auc: 0.9437074829931973\n",
      "Epoch 18/100: val_loss: 0.6629031896591187, val_auc: 0.9437925170068027\n",
      "Epoch 19/100: val_loss: 0.6607430577278137, val_auc: 0.9440136054421768\n",
      "Epoch 20/100: val_loss: 0.6585524678230286, val_auc: 0.9440986394557823\n",
      "Epoch 21/100: val_loss: 0.6563297510147095, val_auc: 0.9441496598639456\n",
      "Epoch 22/100: val_loss: 0.65407794713974, val_auc: 0.944251700680272\n",
      "Epoch 23/100: val_loss: 0.6517929434776306, val_auc: 0.9444217687074831\n",
      "Epoch 24/100: val_loss: 0.6494756937026978, val_auc: 0.9443537414965987\n",
      "Epoch 25/100: val_loss: 0.647122323513031, val_auc: 0.9443707482993197\n",
      "Epoch 26/100: val_loss: 0.6447305083274841, val_auc: 0.944421768707483\n",
      "Epoch 27/100: val_loss: 0.6422964930534363, val_auc: 0.944438775510204\n",
      "Epoch 28/100: val_loss: 0.6398182511329651, val_auc: 0.9444387755102042\n",
      "Epoch 29/100: val_loss: 0.6372966766357422, val_auc: 0.9444897959183673\n",
      "Epoch 30/100: val_loss: 0.6347280144691467, val_auc: 0.9445068027210883\n",
      "Epoch 31/100: val_loss: 0.6321080923080444, val_auc: 0.9445408163265306\n",
      "Epoch 32/100: val_loss: 0.6294383406639099, val_auc: 0.9445408163265305\n",
      "Epoch 33/100: val_loss: 0.626727283000946, val_auc: 0.9445408163265306\n",
      "Epoch 34/100: val_loss: 0.6239732503890991, val_auc: 0.9445918367346938\n",
      "Epoch 35/100: val_loss: 0.6211861371994019, val_auc: 0.9446428571428571\n",
      "Epoch 36/100: val_loss: 0.6183595061302185, val_auc: 0.9447448979591837\n",
      "Epoch 37/100: val_loss: 0.6154965162277222, val_auc: 0.9447108843537415\n",
      "Epoch 38/100: val_loss: 0.6126010417938232, val_auc: 0.9447789115646259\n",
      "Epoch 39/100: val_loss: 0.6096787452697754, val_auc: 0.944795918367347\n",
      "Epoch 40/100: val_loss: 0.6067295074462891, val_auc: 0.9447789115646259\n",
      "Epoch 41/100: val_loss: 0.6037551164627075, val_auc: 0.9447959183673469\n",
      "Epoch 42/100: val_loss: 0.6007564663887024, val_auc: 0.9446768707482994\n",
      "Epoch 43/100: val_loss: 0.597726583480835, val_auc: 0.9446598639455783\n",
      "Epoch 44/100: val_loss: 0.594670295715332, val_auc: 0.9445918367346939\n",
      "Epoch 45/100: val_loss: 0.5915977954864502, val_auc: 0.9446088435374149\n",
      "Epoch 46/100: val_loss: 0.5885019898414612, val_auc: 0.9445748299319728\n",
      "Epoch 47/100: val_loss: 0.5853826999664307, val_auc: 0.9445408163265306\n",
      "Epoch 48/100: val_loss: 0.5822434425354004, val_auc: 0.9445408163265305\n",
      "Epoch 49/100: val_loss: 0.5790721774101257, val_auc: 0.9445578231292517\n",
      "Epoch 50/100: val_loss: 0.5758775472640991, val_auc: 0.944608843537415\n",
      "Epoch 51/100: val_loss: 0.5726611614227295, val_auc: 0.9445918367346938\n",
      "Epoch 52/100: val_loss: 0.5694198608398438, val_auc: 0.9446428571428572\n",
      "Epoch 53/100: val_loss: 0.5661594271659851, val_auc: 0.9446598639455782\n",
      "Epoch 54/100: val_loss: 0.5628862977027893, val_auc: 0.9446088435374149\n",
      "Epoch 55/100: val_loss: 0.5595975518226624, val_auc: 0.944608843537415\n",
      "Epoch 56/100: val_loss: 0.5562967658042908, val_auc: 0.9445748299319727\n",
      "Epoch 57/100: val_loss: 0.5529829859733582, val_auc: 0.9445578231292517\n",
      "Epoch 58/100: val_loss: 0.5496582984924316, val_auc: 0.9445748299319727\n",
      "Epoch 59/100: val_loss: 0.5463289022445679, val_auc: 0.9445748299319727\n",
      "Epoch 60/100: val_loss: 0.5429847240447998, val_auc: 0.9445748299319728\n",
      "Epoch 61/100: val_loss: 0.5396272540092468, val_auc: 0.9445408163265306\n",
      "Epoch 62/100: val_loss: 0.5362755060195923, val_auc: 0.9446088435374149\n",
      "Epoch 63/100: val_loss: 0.5329154133796692, val_auc: 0.9445408163265308\n",
      "Epoch 64/100: val_loss: 0.5295582413673401, val_auc: 0.9445578231292517\n",
      "Epoch 65/100: val_loss: 0.5262149572372437, val_auc: 0.9445408163265306\n",
      "Epoch 66/100: val_loss: 0.5228897929191589, val_auc: 0.944438775510204\n",
      "Epoch 67/100: val_loss: 0.5195786952972412, val_auc: 0.9443707482993198\n",
      "Epoch 68/100: val_loss: 0.5162680149078369, val_auc: 0.9443367346938776\n",
      "Epoch 69/100: val_loss: 0.5129619836807251, val_auc: 0.944387755102041\n",
      "Epoch 70/100: val_loss: 0.5096603035926819, val_auc: 0.9443707482993198\n",
      "Epoch 71/100: val_loss: 0.5063656568527222, val_auc: 0.9443707482993198\n",
      "Epoch 72/100: val_loss: 0.5030794143676758, val_auc: 0.9443537414965987\n",
      "Epoch 73/100: val_loss: 0.499787837266922, val_auc: 0.9442687074829932\n",
      "Epoch 74/100: val_loss: 0.49650102853775024, val_auc: 0.94421768707483\n",
      "Epoch 75/100: val_loss: 0.4932163953781128, val_auc: 0.9442687074829932\n",
      "Epoch 76/100: val_loss: 0.48994556069374084, val_auc: 0.944234693877551\n",
      "Epoch 77/100: val_loss: 0.48669707775115967, val_auc: 0.9442176870748299\n",
      "Epoch 78/100: val_loss: 0.48347944021224976, val_auc: 0.944234693877551\n",
      "Epoch 79/100: val_loss: 0.48029640316963196, val_auc: 0.9443197278911565\n",
      "Epoch 80/100: val_loss: 0.4771709740161896, val_auc: 0.9443197278911565\n",
      "Epoch 81/100: val_loss: 0.47406500577926636, val_auc: 0.9442346938775511\n",
      "Epoch 82/100: val_loss: 0.47100022435188293, val_auc: 0.944234693877551\n",
      "Epoch 83/100: val_loss: 0.4679439663887024, val_auc: 0.9442176870748299\n",
      "Epoch 84/100: val_loss: 0.46491560339927673, val_auc: 0.9442006802721088\n",
      "Epoch 85/100: val_loss: 0.4619317054748535, val_auc: 0.9442346938775511\n",
      "Epoch 86/100: val_loss: 0.45895588397979736, val_auc: 0.9442346938775511\n",
      "Epoch 87/100: val_loss: 0.4559979736804962, val_auc: 0.944234693877551\n",
      "Epoch 88/100: val_loss: 0.4530898928642273, val_auc: 0.9441666666666667\n",
      "Epoch 89/100: val_loss: 0.450229674577713, val_auc: 0.9442006802721089\n",
      "Epoch 90/100: val_loss: 0.44740623235702515, val_auc: 0.9442346938775511\n",
      "Epoch 91/100: val_loss: 0.444618284702301, val_auc: 0.9443027210884354\n",
      "Epoch 92/100: val_loss: 0.44187939167022705, val_auc: 0.9443197278911565\n",
      "Epoch 93/100: val_loss: 0.4392043948173523, val_auc: 0.9442687074829932\n",
      "Epoch 94/100: val_loss: 0.4365949332714081, val_auc: 0.9442857142857144\n",
      "Epoch 95/100: val_loss: 0.43402501940727234, val_auc: 0.9442517006802721\n",
      "Epoch 96/100: val_loss: 0.4315204918384552, val_auc: 0.9441836734693877\n",
      "Epoch 97/100: val_loss: 0.4289945662021637, val_auc: 0.9441666666666666\n",
      "Epoch 98/100: val_loss: 0.4265258312225342, val_auc: 0.9441836734693877\n",
      "Epoch 99/100: val_loss: 0.4241415858268738, val_auc: 0.9441326530612245\n",
      "Epoch 100/100: val_loss: 0.421783983707428, val_auc: 0.94406462585034\n",
      "Final Test Auc: 0.9604434027510952\n",
      "--------------------- The Embedding of GAT have done!!! ------------------\n",
      "Alpha: 0.9064301252365112\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.GAT_model import GAT\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from models.BWGNN_model import BWGNN_em\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pygod.models import DOMINANT\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from utils import anomaly_weight, train_for_GCN, feature_fusion\n",
    "\n",
    "data = pyg_dataset(dataset_name=\"citeseer\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "# data2 = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "a_weight = anomaly_weight(data)\n",
    "\n",
    "# data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.2], anomaly_type=\"min\").dataset\n",
    "model = DOMINANT(verbose=True, gpu=-1, epoch=1, lr=1e-3)\n",
    "model = model.fit(data)\n",
    "\n",
    "x_, s_, hid_dom = model.model(data.x, data.edge_index)\n",
    "\n",
    "s = to_dense_adj(data.edge_index)[0]\n",
    "score = model.loss_func(data.x,x_,s,s_)\n",
    "score = score.detach().cpu().numpy()\n",
    "outlier_scores = model.decision_function(data)\n",
    "test_auc_do = roc_auc_score(data.y[data.test_mask].numpy(), score[data.test_mask])\n",
    "print('Final Test AUC:', test_auc_do)\n",
    "print (\"--------------------- The Embedding of Dominate have done!!! ------------------\")\n",
    "\n",
    "\n",
    "number_class = 2\n",
    "hid_dim = 64\n",
    "BWGNN_model = BWGNN_em(data.x.shape[1], 64, number_class, dgl_data)\n",
    "BW_optimizer = Adam(BWGNN_model.parameters(), lr = 1e-4)\n",
    "epochs = 100\n",
    "hid_bw, test_auc_bw, best_auc_bw = train_for_GCN(BWGNN_model, BW_optimizer, data, a_weight, epochs)\n",
    "print (\"--------------------- The Embedding of BWGNN have done!!! ------------------\")\n",
    "\n",
    "\n",
    "hid_dim = 64\n",
    "edge_index = data.edge_index\n",
    "gat_model = GAT(data.x.shape[1], 64, number_class, data)\n",
    "GAT_optimizer = Adam(gat_model.parameters(), lr = 1e-3)\n",
    "epochs = 100\n",
    "hid_gat, test_auc_gat, best_auc_gat = train_for_GCN(gat_model, GAT_optimizer, data, a_weight, epochs)\n",
    "print (\"--------------------- The Embedding of GAT have done!!! ------------------\")\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pygod.models.basic_nn import GCN\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "class DOMINANT_recon(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_dim,\n",
    "                hid_dim,\n",
    "                out_dim,\n",
    "                decoder_layers,\n",
    "                dropout,\n",
    "                act):\n",
    "        super(DOMINANT_recon, self).__init__()\n",
    "\n",
    "        # split the number of layers for the encoder and decoders\n",
    "        self.attr_decoder = GCN(in_channels=in_dim,\n",
    "                                hidden_channels=hid_dim,\n",
    "                                num_layers=decoder_layers,\n",
    "                                out_channels=out_dim,\n",
    "                                dropout=dropout,\n",
    "                                act=act)\n",
    "\n",
    "        self.struct_decoder = GCN(in_channels=in_dim,\n",
    "                                hidden_channels=hid_dim,\n",
    "                                num_layers=decoder_layers,\n",
    "                                out_channels=out_dim,\n",
    "                                dropout=dropout,\n",
    "                                act=act)\n",
    "\n",
    "    def forward(self, h, edge_index):\n",
    "        # decode feature matrix\n",
    "        x_ = self.attr_decoder(h, edge_index)\n",
    "        # print (x_.shape)\n",
    "        # decode adjacency matrix\n",
    "        h_ = self.struct_decoder(h, edge_index)\n",
    "        # print (h_.shape)\n",
    "        s_ = h_ @ h_.T\n",
    "        # return reconstructed matrices\n",
    "        return x_, s_, h\n",
    "\n",
    "def reco_loss_func(x, x_, s, s_):\n",
    "    # attribute reconstruction loss\n",
    "    diff_attribute = torch.pow(x - x_, 2)\n",
    "    attribute_errors = torch.sqrt(torch.sum(diff_attribute, 1))\n",
    "\n",
    "    # structure reconstruction loss\n",
    "    diff_structure = torch.pow(s - s_, 2)\n",
    "    structure_errors = torch.sqrt(torch.sum(diff_structure, 1))\n",
    "\n",
    "    score = alpha * attribute_errors \\\n",
    "            + (1 - alpha) * structure_errors\n",
    "    return score\n",
    "\n",
    "s = to_dense_adj(data.edge_index)[0]\n",
    "alpha = torch.std(s).detach() / (torch.std(data.x).detach() + torch.std(s).detach())\n",
    "print (f\"Alpha: {alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 1/75: val_loss: 0.6857784986495972, val_auc: 0.875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 2/75: val_loss: 0.6829900145530701, val_auc: 0.875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 3/75: val_loss: 0.6806588768959045, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 4/75: val_loss: 0.678452730178833, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 5/75: val_loss: 0.6762498021125793, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 6/75: val_loss: 0.6741844415664673, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 7/75: val_loss: 0.6719791889190674, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 8/75: val_loss: 0.6694870591163635, val_auc: 0.9375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 9/75: val_loss: 0.6669013500213623, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 10/75: val_loss: 0.6642493009567261, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 11/75: val_loss: 0.6613300442695618, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 12/75: val_loss: 0.6583888530731201, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 13/75: val_loss: 0.6551105976104736, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 14/75: val_loss: 0.6511895656585693, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 15/75: val_loss: 0.6467039585113525, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 16/75: val_loss: 0.641365647315979, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 17/75: val_loss: 0.6353943347930908, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 18/75: val_loss: 0.6289992928504944, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 19/75: val_loss: 0.6222814321517944, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 20/75: val_loss: 0.6146273016929626, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 21/75: val_loss: 0.6060010194778442, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 22/75: val_loss: 0.5970242619514465, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 23/75: val_loss: 0.5879506468772888, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 24/75: val_loss: 0.578117847442627, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 25/75: val_loss: 0.5672570466995239, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 26/75: val_loss: 0.5554264187812805, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 27/75: val_loss: 0.5424264669418335, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 28/75: val_loss: 0.5287145376205444, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 29/75: val_loss: 0.5135330557823181, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 30/75: val_loss: 0.4969485402107239, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 31/75: val_loss: 0.4793747067451477, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 32/75: val_loss: 0.46064677834510803, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 33/75: val_loss: 0.44076257944107056, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 34/75: val_loss: 0.4196135401725769, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 35/75: val_loss: 0.39724892377853394, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 36/75: val_loss: 0.3736407160758972, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 37/75: val_loss: 0.3491073548793793, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 38/75: val_loss: 0.32398903369903564, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 39/75: val_loss: 0.2986918091773987, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 40/75: val_loss: 0.27328312397003174, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 41/75: val_loss: 0.24796760082244873, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 42/75: val_loss: 0.22337795794010162, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 43/75: val_loss: 0.19973556697368622, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 44/75: val_loss: 0.17733685672283173, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 45/75: val_loss: 0.15615537762641907, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 46/75: val_loss: 0.13631021976470947, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 47/75: val_loss: 0.1178898960351944, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 48/75: val_loss: 0.10113397240638733, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 49/75: val_loss: 0.08592981845140457, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 50/75: val_loss: 0.07229869812726974, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 51/75: val_loss: 0.060298461467027664, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 52/75: val_loss: 0.050015583634376526, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 53/75: val_loss: 0.041257016360759735, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 54/75: val_loss: 0.03385934978723526, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 55/75: val_loss: 0.02767307311296463, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 56/75: val_loss: 0.022595839574933052, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 57/75: val_loss: 0.01841610111296177, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 58/75: val_loss: 0.014980202540755272, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 59/75: val_loss: 0.012195074930787086, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 60/75: val_loss: 0.00990462675690651, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 61/75: val_loss: 0.008043401874601841, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 62/75: val_loss: 0.00656207837164402, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 63/75: val_loss: 0.005379087291657925, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 64/75: val_loss: 0.004427372943609953, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 65/75: val_loss: 0.0036650807596743107, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 66/75: val_loss: 0.0030434629879891872, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 67/75: val_loss: 0.0025341741275042295, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 68/75: val_loss: 0.0021247081458568573, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 69/75: val_loss: 0.0017950329929590225, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 70/75: val_loss: 0.0015278473729267716, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 71/75: val_loss: 0.0013077643234282732, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 72/75: val_loss: 0.0011257383739575744, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 73/75: val_loss: 0.0009738308726809919, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 74/75: val_loss: 0.000846907205414027, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 75/75: val_loss: 0.0007412417908199131, val_auc: 1.0\n",
      "Final Test Auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 1/75: val_loss: 0.7173371911048889, val_auc: 0.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 2/75: val_loss: 0.7196869254112244, val_auc: 0.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 3/75: val_loss: 0.7223286628723145, val_auc: 0.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 4/75: val_loss: 0.7254649996757507, val_auc: 0.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 5/75: val_loss: 0.7292212247848511, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 6/75: val_loss: 0.73365318775177, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 7/75: val_loss: 0.738821268081665, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 8/75: val_loss: 0.7445828914642334, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 9/75: val_loss: 0.7511301636695862, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 10/75: val_loss: 0.7586314678192139, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 11/75: val_loss: 0.7667751312255859, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 12/75: val_loss: 0.7750863432884216, val_auc: 0.04761904761904763\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 13/75: val_loss: 0.7837927937507629, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 14/75: val_loss: 0.7931740283966064, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 15/75: val_loss: 0.8036245703697205, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 16/75: val_loss: 0.814794659614563, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 17/75: val_loss: 0.8274908661842346, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 18/75: val_loss: 0.8417166471481323, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 19/75: val_loss: 0.8577435612678528, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 20/75: val_loss: 0.8757601380348206, val_auc: 0.09523809523809523\n",
      "[[0.0, 0.0, 1.0]]\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Final Test Auc: 0.5\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 1/75: val_loss: 0.7023745775222778, val_auc: 0.1875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 2/75: val_loss: 0.7007881999015808, val_auc: 0.1875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 3/75: val_loss: 0.6989425420761108, val_auc: 0.1875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 4/75: val_loss: 0.6968678832054138, val_auc: 0.1875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 5/75: val_loss: 0.6946882009506226, val_auc: 0.25\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 6/75: val_loss: 0.6925795078277588, val_auc: 0.3125\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 7/75: val_loss: 0.6902116537094116, val_auc: 0.375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 8/75: val_loss: 0.6878561973571777, val_auc: 0.375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 9/75: val_loss: 0.6854346394538879, val_auc: 0.4375\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 10/75: val_loss: 0.6824696063995361, val_auc: 0.625\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 11/75: val_loss: 0.6790603995323181, val_auc: 0.6875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 12/75: val_loss: 0.675287127494812, val_auc: 0.6875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 13/75: val_loss: 0.6712332367897034, val_auc: 0.6875\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 14/75: val_loss: 0.6673741936683655, val_auc: 0.8125\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 15/75: val_loss: 0.6632860898971558, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 16/75: val_loss: 0.6587267518043518, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 17/75: val_loss: 0.6535986065864563, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 18/75: val_loss: 0.6481794714927673, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 19/75: val_loss: 0.6424219608306885, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 20/75: val_loss: 0.6360820531845093, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 21/75: val_loss: 0.6290805339813232, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 22/75: val_loss: 0.6213464736938477, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 23/75: val_loss: 0.6128398180007935, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 24/75: val_loss: 0.6037960648536682, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 25/75: val_loss: 0.5937603712081909, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 26/75: val_loss: 0.5826250314712524, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 27/75: val_loss: 0.5705031156539917, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 28/75: val_loss: 0.5573471784591675, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 29/75: val_loss: 0.5434029698371887, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 30/75: val_loss: 0.5283665657043457, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 31/75: val_loss: 0.5123530626296997, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 32/75: val_loss: 0.49492430686950684, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 33/75: val_loss: 0.47622767090797424, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 34/75: val_loss: 0.4563230872154236, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 35/75: val_loss: 0.43561896681785583, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 36/75: val_loss: 0.41390612721443176, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 37/75: val_loss: 0.39119112491607666, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 38/75: val_loss: 0.367916464805603, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 39/75: val_loss: 0.34443676471710205, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 40/75: val_loss: 0.32005441188812256, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 41/75: val_loss: 0.29703187942504883, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 42/75: val_loss: 0.27342790365219116, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 43/75: val_loss: 0.2497953176498413, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 44/75: val_loss: 0.22680240869522095, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 45/75: val_loss: 0.20450234413146973, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 46/75: val_loss: 0.18298734724521637, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 47/75: val_loss: 0.16196772456169128, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 48/75: val_loss: 0.14192678034305573, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 49/75: val_loss: 0.12329747527837753, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 50/75: val_loss: 0.1080273985862732, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 51/75: val_loss: 0.09374086558818817, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 52/75: val_loss: 0.08048445731401443, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 53/75: val_loss: 0.06852786242961884, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 54/75: val_loss: 0.0578998401761055, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 55/75: val_loss: 0.04854195937514305, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 56/75: val_loss: 0.04042051360011101, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 57/75: val_loss: 0.03345540910959244, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 58/75: val_loss: 0.027509324252605438, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 59/75: val_loss: 0.022510051727294922, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 60/75: val_loss: 0.0183608066290617, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 61/75: val_loss: 0.014940930530428886, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 62/75: val_loss: 0.012127122841775417, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 63/75: val_loss: 0.009823773987591267, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 64/75: val_loss: 0.007958712056279182, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 65/75: val_loss: 0.00645794253796339, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 66/75: val_loss: 0.0052513256669044495, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 67/75: val_loss: 0.0042822095565497875, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 68/75: val_loss: 0.0035041666124016047, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 69/75: val_loss: 0.002878662897273898, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 70/75: val_loss: 0.0023743947967886925, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 71/75: val_loss: 0.0019659569952636957, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 72/75: val_loss: 0.0016354667022824287, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 73/75: val_loss: 0.0013676712987944484, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 74/75: val_loss: 0.0011509560281410813, val_auc: 1.0\n",
      "[[0.0, 0.0, 1.0]]\n",
      "Epoch 75/75: val_loss: 0.0009732411708682775, val_auc: 1.0\n",
      "Final Test Auc: 1.0\n",
      "[1.0, 0.5, 1.0] \n",
      " Test mean: 0.8333333333333334 \n",
      " Test std: 0.23570226039551584\n",
      "[1.0, 0.09523809523809523, 1.0] \n",
      " Val best: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from models.GAT_model import GAT\n",
    "from models.GCN_model import GCN\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from models.BWGNN_model import BWGNN_em\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pygod.models import DOMINANT\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from utils import *\n",
    "\n",
    "torch.manual_seed(21)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def train_for_mul_model(model_list, optimizer_list, linear_model, linear_optimizer, data, a_weight, epochs, b_weight, b_optimizer=None):\n",
    "    \"\"\"Train for multi model togeother. Like GAT, GCN, BWGNN or anyelse. \n",
    "    Args: \n",
    "        b_weight: option for numbers list or learnable parameters\n",
    "        b_optimizer: None if b_weight is numbers list, real torch optimizer if b_weight belongs to the learnable parameters.\n",
    "    \"\"\"\n",
    "    best_val_auc = 0\n",
    "    early_stop = EarlyStopping(patience=20)\n",
    "    for epoch in range(epochs):\n",
    "        print (b_weight)\n",
    "        hiddle_list = []\n",
    "        for pos, model in enumerate(model_list):\n",
    "            model.train()\n",
    "            logits, hid = model(data.x)\n",
    "            hid = hid * b_weight[0][pos]\n",
    "            # if pos != 2:\n",
    "                # hid = hid * 0\n",
    "            hiddle_list.append(hid)\n",
    "        # 特征融合以及特征学习\n",
    "        hiddle = torch.concat(hiddle_list, axis=1)\n",
    "        hiddle = zero2one((1-cosine_distance(hiddle.T)).mean(axis=0))*hiddle\n",
    "        \n",
    "        logits = linear_model(hiddle, data.edge_index)\n",
    "        train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "        \n",
    "        for optimizer in optimizer_list:\n",
    "            optimizer.zero_grad()\n",
    "        linear_optimizer.zero_grad()\n",
    "        if b_optimizer != None:\n",
    "            b_optimizer.zero_grad()\n",
    "        \n",
    "        train_loss.backward()\n",
    "        \n",
    "        for optimizer in optimizer_list:\n",
    "            optimizer.step()\n",
    "        linear_optimizer.step()\n",
    "        if b_optimizer != None:\n",
    "            b_optimizer.step()\n",
    "\n",
    "        hiddle_list = []\n",
    "        for model in model_list:\n",
    "            model.eval()\n",
    "            logits, hid = model(data.x)\n",
    "            hiddle_list.append(hid)\n",
    "        # 特征融合以及特征学习\n",
    "        hiddle = torch.concat(hiddle_list, axis=1)\n",
    "        logits = linear_model(hiddle, data.edge_index)\n",
    "\n",
    "        val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "        probs = logits.softmax(1)\n",
    "        \n",
    "        auc = roc_auc_score(data.y[data.val_mask].cpu().numpy(), probs[data.val_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "             roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "        \n",
    "        if auc >= best_val_auc:\n",
    "            best_val_auc = auc\n",
    "\n",
    "        early_stop(val_loss, model)\n",
    "        if early_stop.early_stop == True:\n",
    "            print (\"Early stopping\")\n",
    "            break\n",
    "        print (f\"Epoch {epoch+1}/{epochs}: val_loss: {val_loss}, val_auc: {auc}\")\n",
    "    hiddle_list = []\n",
    "    for model in model_list:\n",
    "        model.eval()\n",
    "        logits, hid = model(data.x)\n",
    "        hiddle_list.append(hid)\n",
    "    # 特征融合以及特征学习\n",
    "    hiddle = torch.concat(hiddle_list, axis=1)\n",
    "    logits = linear_model(hiddle, data.edge_index)\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.test_mask].cpu().numpy(), probs[data.test_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "             roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "    print (f\"Final Test Auc: {auc}\")\n",
    "    return hid, auc, best_val_auc\n",
    "\n",
    "def train_for_param(data_name):\n",
    "    parameter_list = np.linspace(start=0,stop=1,num=1)\n",
    "    param2performance_list = []\n",
    "    for param1 in parameter_list:\n",
    "        param2 = 1 - param1\n",
    "\n",
    "        # run five times to get mean and std for test. best performance for val.\n",
    "        run_times = 3\n",
    "        test_auc_mean = []\n",
    "        test_auc_std = []\n",
    "        val_auc_best = []\n",
    "        test_list = []\n",
    "        val_list = []\n",
    "        for i in range(run_times):\n",
    "            np.random.seed(i*2)\n",
    "            # data = pyg_dataset(dataset_name=data_name, dataset_spilt=[0.4,0.29,0.3]).dataset\n",
    "            data = pyg_dataset(dataset_name=data_name, dataset_spilt=[0.4,0.29,0.3], anomaly_type=\"min\").dataset\n",
    "            # data2 = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "            dgl_data = pyg_to_dgl(data)\n",
    "            data = data.to(device)\n",
    "            dgl_data = dgl_data.to(device)\n",
    "\n",
    "            a_weight = anomaly_weight(data)\n",
    "\n",
    "            epochs = 75\n",
    "            hid_dim = 64\n",
    "            number_class = 2\n",
    "            gcn_model = GCN(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "            gat_model = GAT(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "            bw_model = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data).to(device)\n",
    "            model_list = [gcn_model, gat_model, bw_model]\n",
    "            cla_model = GCNConv(hid_dim*len(model_list), number_class).to(device)\n",
    "\n",
    "            # l_weight = [nn.Parameter(torch.ones([hid_dom.shape[-1] * feature_length], dtype=torch.float32, requires_grad=True))]\n",
    "            # b_weight = [nn.Parameter(torch.ones([len(model_list)], dtype=torch.float32, requires_grad=True))]\n",
    "            b_weight = [[0.0, param1, param2]]\n",
    "            # b_optimizer = Adam(b_weight, lr = 1e-2, weight_decay=5e-2)\n",
    "            # l_optimizer_ = Adam(l_weight, lr = 5e-2, weight_decay=5e-2)\n",
    "\n",
    "            gcn_optimizer = Adam(gcn_model.parameters(), lr = 1e-3)\n",
    "            gat_optimizer = Adam(gat_model.parameters(), lr = 1e-3)\n",
    "            bw_optimizer = Adam(bw_model.parameters(), lr = 1e-3)\n",
    "            cla_optimizer = Adam(cla_model.parameters(), lr = 1e-3)\n",
    "\n",
    "            optimizer_list = [gcn_optimizer,gat_optimizer,bw_optimizer]    \n",
    "            _, auc, best_val_auc = train_for_mul_model(model_list, optimizer_list, cla_model, cla_optimizer, data, a_weight, epochs, b_weight)\n",
    "            \n",
    "            test_list.append(auc)\n",
    "            val_list.append(best_val_auc)\n",
    "        \n",
    "        test_auc_mean.append(np.array(test_list).mean())\n",
    "        test_auc_std.append(np.array(test_list).std())\n",
    "        val_auc_best.append(np.array(val_list).max())\n",
    "\n",
    "        param2performance_list.append([param1, param2, test_auc_mean[-1], test_auc_std[-1], val_auc_best[-1]])\n",
    "        print (test_list,f\"\\n Test mean: {np.array(test_list).mean()}\",f\"\\n Test std: {np.array(test_list).std()}\")\n",
    "        print (val_list, f\"\\n Val best: {np.array(val_list).max()}\")\n",
    "        \n",
    "    np.savetxt(f\"./result/param2performance_{data_name}_gatbw.txt\", np.array(param2performance_list))\n",
    "\n",
    "dataset_ava_list = [\"karate\"]\n",
    "for data_name in dataset_ava_list:\n",
    "    train_for_param(data_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如上是遍历参数空间\n",
    "### 如下是使用自学习参数，对表征数据进行融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)]\n",
      "Epoch 1/10: val_loss: 11.458383560180664, val_auc: 0.29654899244159727\n",
      "[Parameter containing:\n",
      "tensor([1.0100, 0.9900, 1.0100], requires_grad=True)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9444\\2017241959.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mcla_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcla_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0moptimizer_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgcn_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgat_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbw_optimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_val_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_for_mul_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcla_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcla_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mtest_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9444\\2513425722.py\u001b[0m in \u001b[0;36mtrain_for_mul_model\u001b[1;34m(model_list, optimizer_list, linear_model, linear_optimizer, data, a_weight, epochs, b_weight, b_optimizer)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mb_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moptimizer_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pygod2\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pygod2\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pygod2\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mBackwardCFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FunctionBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFunctionCtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_HookMixin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[1;31m# _forward_cls is defined by derived class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;31m# The user should define either backward or vjp but never both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_times = 1\n",
    "test_auc_mean = []\n",
    "test_auc_std = []\n",
    "val_auc_best = []\n",
    "test_list = []\n",
    "val_list = []\n",
    "device = torch.device(\"cuda\")\n",
    "# param2performance_list = []\n",
    "for i in range(run_times):\n",
    "    np.random.seed(3)\n",
    "    data = pyg_dataset(dataset_name=\"weibo\", dataset_spilt=[0.4,0.29,0.3], anomaly_type=\"min\").dataset\n",
    "    # data2 = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "    dgl_data = pyg_to_dgl(data)\n",
    "    a_weight = anomaly_weight(data)\n",
    "\n",
    "    epochs = 10\n",
    "    hid_dim = 64\n",
    "    number_class = 2\n",
    "    edge_index = data.edge_index\n",
    "    gcn_model = GCN(data.x.shape[1], hid_dim, number_class, data)\n",
    "    gat_model = GAT(data.x.shape[1], hid_dim, number_class, data)\n",
    "    bw_model = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data)\n",
    "    model_list = [gcn_model,gat_model,bw_model]\n",
    "    cla_model = GCNConv(hid_dim*len(model_list), number_class)\n",
    "\n",
    "    # l_weight = [nn.Parameter(torch.ones([hid_dom.shape[-1] * feature_length], dtype=torch.float32, requires_grad=True))]\n",
    "    b_weight = [nn.Parameter(torch.ones([len(model_list)], dtype=torch.float32, requires_grad=True))]\n",
    "    # b_weight = [[0.0, param1, param2]]\n",
    "    b_optimizer = Adam(b_weight, lr = 1e-2, weight_decay=5e-2)\n",
    "    \n",
    "    # l_optimizer_ = Adam(l_weight, lr = 5e-2, weight_decay=5e-2)\n",
    "\n",
    "    gcn_optimizer = Adam(gcn_model.parameters(), lr = 1e-3)\n",
    "    gat_optimizer = Adam(gat_model.parameters(), lr = 1e-3)\n",
    "    bw_optimizer = Adam(bw_model.parameters(), lr = 1e-3)\n",
    "    cla_optimizer = Adam(cla_model.parameters(), lr = 1e-3)\n",
    "    optimizer_list = [gcn_optimizer,gat_optimizer,bw_optimizer]    \n",
    "    _, auc, best_val_auc = train_for_mul_model(model_list, optimizer_list, cla_model, cla_optimizer, data, a_weight, epochs, b_weight, b_optimizer)\n",
    "    \n",
    "    test_list.append(auc)\n",
    "    val_list.append(best_val_auc)\n",
    "\n",
    "test_auc_mean.append(np.array(test_list).mean())\n",
    "test_auc_std.append(np.array(test_list).std())\n",
    "val_auc_best.append(np.array(val_list).max())\n",
    "\n",
    "# param2performance_list.append([param1, param2, test_auc_mean[-1], test_auc_std[-1], val_auc_best[-1]])\n",
    "print (test_list,f\"\\n Test mean: {np.array(test_list).mean()}\",f\"\\n Test std: {np.array(test_list).std()}\")\n",
    "print (val_list, f\"\\n Val best: {np.array(val_list).max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design for reconstrcture-oriented method, Dominant and AnomalyDae. \n",
    "\n",
    "* Fusion training\n",
    "\n",
    "* iteratering all fusion parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly syntheic is on processing\n",
      "using 0.06 seconds\n",
      "Epoch 0000: Loss 0.3074\n",
      "Epoch 0001: Loss 0.3120\n",
      "Epoch 0002: Loss 0.3053\n",
      "Epoch 0003: Loss 0.3049\n",
      "Epoch 0004: Loss 0.3071\n",
      "Epoch 0000: Loss 2.9281\n",
      "Epoch 0001: Loss 2.8720\n",
      "Epoch 0002: Loss 2.8381\n",
      "Epoch 0003: Loss 2.8277\n",
      "Epoch 0004: Loss 2.8327\n",
      "Final Test AUC: 0.8643752190676481\n",
      "--------------------- The Embedding of Dominate have done!!! ------------------\n",
      "Final Test AUC: 0.7123860848229933\n",
      "--------------------- The Embedding of Dominate have done!!! ------------------\n"
     ]
    }
   ],
   "source": [
    "# Anomaly detection using autoencoders with nonlinear dimensionality reduction\n",
    "from pygod.models import DOMINANT, AnomalyDAE\n",
    "\n",
    "data = pyg_dataset(dataset_name=\"citeseer\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"syn\").dataset\n",
    "# data2 = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "a_weight = anomaly_weight(data)\n",
    "\n",
    "# data = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.2], anomaly_type=\"min\").dataset\n",
    "do_model = DOMINANT(verbose=True, gpu=-1, epoch=5, lr=1e-3)\n",
    "do_model = do_model.fit(data)\n",
    "\n",
    "dae_model = AnomalyDAE(verbose=True, gpu=-1, epoch=5, lr=1e-3, batch_size=0)\n",
    "dae_model = dae_model.fit(data)\n",
    "\n",
    "x_, s_, hid_dom = do_model.model(data.x, data.edge_index)\n",
    "s = to_dense_adj(data.edge_index)[0]\n",
    "score = do_model.loss_func(data.x,x_,s,s_)\n",
    "score = score.detach().cpu().numpy()\n",
    "outlier_scores = do_model.decision_function(data)\n",
    "test_auc_do = roc_auc_score(data.y[data.test_mask].numpy(), score[data.test_mask])\n",
    "print('Final Test AUC:', test_auc_do)\n",
    "print (\"--------------------- The Embedding of Dominate have done!!! ------------------\")\n",
    "\n",
    "x_, s_ = dae_model.model(data.x, data.edge_index, batch_size=data.x.shape[0])\n",
    "score = dae_model.loss_func(data.x,x_,s,s_)\n",
    "score = score.detach().cpu().numpy()\n",
    "outlier_scores = dae_model.decision_function(data)\n",
    "test_auc_do = roc_auc_score(data.y[data.test_mask].numpy(), score[data.test_mask])\n",
    "print('Final Test AUC:', test_auc_do)\n",
    "print (\"--------------------- The Embedding of Dominate have done!!! ------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new idea from deconpling representation learning. Test decoupling learning on GAT, BWGNN, GCN with the SSL loss function DCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\AI Learning\\Mul-Graph_baseline\\dataset.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.dataset = Data(x=temp.x, edge_index=temp.edge_index,y=torch.tensor(temp.y, dtype=torch.long),train_mask=position,val_mask=position,test_mask=position)\n",
      "g:\\AI Learning\\Mul-Graph_baseline\\dataset.py:103: UserWarning: Anomaly is min class of dataset and anomaly rate is not conformed to setting\n",
      "  warnings.warn(f\"Anomaly is min class of dataset and anomaly rate is not conformed to setting\")\n",
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\dgl\\heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/30, loss: 0.6849053502082825\n",
      "epoch: 2/30, loss: 0.5997412204742432\n",
      "epoch: 3/30, loss: 0.5254267454147339\n",
      "epoch: 4/30, loss: 0.4518432319164276\n",
      "epoch: 5/30, loss: 0.3799225687980652\n",
      "epoch: 6/30, loss: 0.31303009390830994\n",
      "epoch: 7/30, loss: 0.2539626657962799\n",
      "epoch: 8/30, loss: 0.20350222289562225\n",
      "epoch: 9/30, loss: 0.16036009788513184\n",
      "epoch: 10/30, loss: 0.12313470244407654\n",
      "epoch: 11/30, loss: 0.09116248041391373\n",
      "epoch: 12/30, loss: 0.06474782526493073\n",
      "epoch: 13/30, loss: 0.04359951987862587\n",
      "epoch: 14/30, loss: 0.027867743745446205\n",
      "epoch: 15/30, loss: 0.01692819595336914\n",
      "epoch: 16/30, loss: 0.0098188491538167\n",
      "epoch: 17/30, loss: 0.0054903775453567505\n",
      "epoch: 18/30, loss: 0.0029963827691972256\n",
      "epoch: 19/30, loss: 0.0016164698172360659\n",
      "epoch: 20/30, loss: 0.0008717927848920226\n",
      "epoch: 21/30, loss: 0.00047432718565687537\n",
      "epoch: 22/30, loss: 0.00026286402135156095\n",
      "epoch: 23/30, loss: 0.00014828865823801607\n",
      "epoch: 24/30, loss: 8.568770863348618e-05\n",
      "epoch: 25/30, loss: 5.082969437353313e-05\n",
      "epoch: 26/30, loss: 3.0990951927378774e-05\n",
      "epoch: 27/30, loss: 1.9434864952927455e-05\n",
      "epoch: 28/30, loss: 1.2535439964267425e-05\n",
      "epoch: 29/30, loss: 8.312988029501867e-06\n",
      "epoch: 30/30, loss: 5.6628696256666444e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from models.GAT_model import GAT\n",
    "from models.GCN_model import GCN\n",
    "from models.GIN_model import GIN\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from models.BWGNN_model import BWGNN_em\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from models.dci import DCI_loss\n",
    "from utils import *\n",
    "\n",
    "torch.manual_seed(21)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "data_name = \"reddit\"\n",
    "\n",
    "# run five times to get mean and std for test. best performance for val.\n",
    "test_auc_mean = []\n",
    "test_auc_std = []\n",
    "val_auc_best = []\n",
    "test_list = []\n",
    "val_list = []\n",
    "\n",
    "np.random.seed(2)\n",
    "# dataset preparing\n",
    "data = pyg_dataset(dataset_name=data_name, dataset_spilt=[0.4,0.29,0.3], anomaly_type=\"min\").dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "data = data.to(device)\n",
    "dgl_data = dgl_data.to(device)\n",
    "a_weight = anomaly_weight(data)\n",
    "\n",
    "epochs = 30\n",
    "hid_dim = 64\n",
    "number_class = 2\n",
    "recluster_interval = 10\n",
    "kmeans = KMeans(n_clusters=number_class, random_state=0).fit(data.x)\n",
    "ss_label = kmeans.labels_\n",
    "cluster_info = [list(np.where(ss_label==i)[0]) for i in range(number_class)]\n",
    "idx = np.random.permutation(data.x.shape[0])\n",
    "shuf_feats = data.x[idx, :]\n",
    "\n",
    "gcn_model1 = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data).to(device)\n",
    "gcn_model2 = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data).to(device)\n",
    "# GCN(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "loss_dci = DCI_loss(hid_dim, device)\n",
    "gcn_optimizer = Adam([{\"params\": gcn_model1.parameters(), \"lr\": 5e-3},\\\n",
    "                        {\"params\": gcn_model2.parameters(), \"lr\": 5e-3}])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    gcn_model1.train()\n",
    "    gcn_model2.train()\n",
    "    _, hid1 = gcn_model1(data.x)\n",
    "    _, hid2 = gcn_model2(shuf_feats)\n",
    "    train_loss = loss_dci(hid1, hid2, None, None, None, cluster_info, number_class)\n",
    "\n",
    "    gcn_optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    gcn_optimizer.step()\n",
    "    print (f\"epoch: {epoch + 1}/{epochs}, loss: {train_loss}\")\n",
    "    # re-clustering\n",
    "    if epoch % recluster_interval == 0:\n",
    "        gcn_model1.eval()\n",
    "        _, emb = gcn_model1(data.x)\n",
    "        kmeans = KMeans(n_clusters=number_class, random_state=0).fit(emb.detach().cpu().numpy())\n",
    "        ss_label = kmeans.labels_\n",
    "        cluster_info = [list(np.where(ss_label==i)[0]) for i in range(number_class)]\n",
    "\n",
    "cls_model = Linear(in_features=hid_dim, out_features=number_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: val_loss: 0.6682317852973938, val_auc: 0.5756489563567362\n",
      "Epoch 2/50: val_loss: 0.668691098690033, val_auc: 0.5770986717267552\n",
      "Epoch 3/50: val_loss: 0.6908826231956482, val_auc: 0.5781935483870968\n",
      "Epoch 4/50: val_loss: 0.7352685928344727, val_auc: 0.5794402277039848\n",
      "Epoch 5/50: val_loss: 0.7459756731987, val_auc: 0.5786356736242884\n",
      "Epoch 6/50: val_loss: 0.7277727723121643, val_auc: 0.579314990512334\n",
      "Epoch 7/50: val_loss: 0.7037367224693298, val_auc: 0.5785901328273244\n",
      "Epoch 8/50: val_loss: 0.6855451464653015, val_auc: 0.5789506641366224\n",
      "Epoch 9/50: val_loss: 0.6747745275497437, val_auc: 0.5784800759013283\n",
      "Epoch 10/50: val_loss: 0.6692759990692139, val_auc: 0.5781043643263757\n",
      "Epoch 11/50: val_loss: 0.666764497756958, val_auc: 0.5780702087286528\n",
      "Epoch 12/50: val_loss: 0.6657822132110596, val_auc: 0.5782296015180266\n",
      "Epoch 13/50: val_loss: 0.6656140685081482, val_auc: 0.5787836812144213\n",
      "Epoch 14/50: val_loss: 0.6660711169242859, val_auc: 0.5794364326375712\n",
      "Epoch 15/50: val_loss: 0.667267382144928, val_auc: 0.5791100569259962\n",
      "Epoch 16/50: val_loss: 0.6694085001945496, val_auc: 0.5799146110056925\n",
      "Epoch 17/50: val_loss: 0.6726173758506775, val_auc: 0.5787039848197344\n",
      "Epoch 18/50: val_loss: 0.6768320798873901, val_auc: 0.5788216318785578\n",
      "Epoch 19/50: val_loss: 0.6817041635513306, val_auc: 0.5795806451612903\n",
      "Epoch 20/50: val_loss: 0.6865946650505066, val_auc: 0.5799146110056926\n",
      "Epoch 21/50: val_loss: 0.690647304058075, val_auc: 0.579011385199241\n",
      "Epoch 22/50: val_loss: 0.6929910778999329, val_auc: 0.578988614800759\n",
      "Epoch 23/50: val_loss: 0.6930824518203735, val_auc: 0.5790493358633776\n",
      "Epoch 24/50: val_loss: 0.6909826397895813, val_auc: 0.5790645161290323\n",
      "Epoch 25/50: val_loss: 0.6873656511306763, val_auc: 0.5789848197343453\n",
      "Epoch 26/50: val_loss: 0.6832177639007568, val_auc: 0.5800398481973434\n",
      "Epoch 27/50: val_loss: 0.6794247031211853, val_auc: 0.5799753320683112\n",
      "Epoch 28/50: val_loss: 0.6765096187591553, val_auc: 0.5798500948766603\n",
      "Epoch 29/50: val_loss: 0.6746499538421631, val_auc: 0.5794478178368121\n",
      "Epoch 30/50: val_loss: 0.673764705657959, val_auc: 0.5791290322580646\n",
      "Epoch 31/50: val_loss: 0.6737037897109985, val_auc: 0.5787229601518027\n",
      "Epoch 32/50: val_loss: 0.6743978261947632, val_auc: 0.5785294117647058\n",
      "Epoch 33/50: val_loss: 0.6757981777191162, val_auc: 0.5783472485768502\n",
      "Epoch 34/50: val_loss: 0.6777611970901489, val_auc: 0.5784345351043644\n",
      "Epoch 35/50: val_loss: 0.6799795031547546, val_auc: 0.5792314990512334\n",
      "Epoch 36/50: val_loss: 0.6820343732833862, val_auc: 0.5800132827324478\n",
      "Epoch 37/50: val_loss: 0.6834811568260193, val_auc: 0.5796166982922201\n",
      "Epoch 38/50: val_loss: 0.6840260624885559, val_auc: 0.5792504743833017\n",
      "Epoch 39/50: val_loss: 0.6835966110229492, val_auc: 0.5788368121442126\n",
      "Epoch 40/50: val_loss: 0.6823440790176392, val_auc: 0.5785370018975332\n",
      "Epoch 41/50: val_loss: 0.6805754899978638, val_auc: 0.5791593927893739\n",
      "Epoch 42/50: val_loss: 0.6786361932754517, val_auc: 0.5788709677419355\n",
      "Epoch 43/50: val_loss: 0.6768417358398438, val_auc: 0.5787001897533207\n",
      "Epoch 44/50: val_loss: 0.6754214763641357, val_auc: 0.5784573055028464\n",
      "Epoch 45/50: val_loss: 0.6745177507400513, val_auc: 0.5783130929791271\n",
      "Epoch 46/50: val_loss: 0.6741840243339539, val_auc: 0.5781688804554079\n",
      "Epoch 47/50: val_loss: 0.6744164228439331, val_auc: 0.57819165085389\n",
      "Epoch 48/50: val_loss: 0.6751627922058105, val_auc: 0.5781878557874763\n",
      "Epoch 49/50: val_loss: 0.6763153672218323, val_auc: 0.5782979127134724\n",
      "Epoch 50/50: val_loss: 0.6776974201202393, val_auc: 0.578415559772296\n",
      "Epoch 50/50: test_auc: 0.6135972861192435\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "cls_optimizer = Adam(cls_model.parameters(), lr = 5e-3)\n",
    "gcn_optimizer = Adam([{\"params\": gcn_model1.parameters(), \"lr\": 5e-3},\\\n",
    "                        {\"params\": gcn_model2.parameters(), \"lr\": 5e-3}])\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cls_model.train()\n",
    "    gcn_model1.eval()\n",
    "\n",
    "    _, hid = gcn_model1(data.x)\n",
    "    logits = cls_model(hid)\n",
    "    train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "    # train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    cls_model.zero_grad()\n",
    "    gcn_model1.zero_grad()\n",
    "    train_loss.backward()\n",
    "    cls_optimizer.step()\n",
    "    gcn_optimizer.step()\n",
    "\n",
    "    cls_model.eval()\n",
    "    logits = cls_model(hid)\n",
    "    val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "    # val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask])\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.val_mask].cpu().numpy(), probs[data.val_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "            roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: val_loss: {val_loss}, val_auc: {auc}\")\n",
    "\n",
    "cls_model.eval()\n",
    "logits = cls_model(hid)\n",
    "# val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask])\n",
    "probs = logits.softmax(1)\n",
    "auc = roc_auc_score(data.y[data.test_mask].cpu().numpy(), probs[data.test_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "        roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "print (f\"Epoch {epoch+1}/{epochs}: test_auc: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dci 联合学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)]\n",
      "epoch: 1/30, loss: 0.6948443651199341\n",
      "[Parameter containing:\n",
      "tensor([0.9900, 0.9900, 0.9900], requires_grad=True)]\n",
      "epoch: 2/30, loss: 0.26396119594573975\n",
      "[Parameter containing:\n",
      "tensor([0.9825, 0.9801, 0.9803], requires_grad=True)]\n",
      "epoch: 3/30, loss: 0.13121739029884338\n",
      "[Parameter containing:\n",
      "tensor([0.9805, 0.9705, 0.9714], requires_grad=True)]\n",
      "epoch: 4/30, loss: 0.05474701523780823\n",
      "[Parameter containing:\n",
      "tensor([0.9787, 0.9610, 0.9632], requires_grad=True)]\n",
      "epoch: 5/30, loss: 0.015590299852192402\n",
      "[Parameter containing:\n",
      "tensor([0.9748, 0.9513, 0.9548], requires_grad=True)]\n",
      "epoch: 6/30, loss: 0.0030336028430610895\n",
      "[Parameter containing:\n",
      "tensor([0.9691, 0.9415, 0.9459], requires_grad=True)]\n",
      "epoch: 7/30, loss: 0.0006729051820002496\n",
      "[Parameter containing:\n",
      "tensor([0.9622, 0.9316, 0.9367], requires_grad=True)]\n",
      "epoch: 8/30, loss: 0.00019654440984595567\n",
      "[Parameter containing:\n",
      "tensor([0.9546, 0.9216, 0.9272], requires_grad=True)]\n",
      "epoch: 9/30, loss: 6.018045314704068e-05\n",
      "[Parameter containing:\n",
      "tensor([0.9464, 0.9116, 0.9175], requires_grad=True)]\n",
      "epoch: 10/30, loss: 1.4303236639534589e-05\n",
      "[Parameter containing:\n",
      "tensor([0.9377, 0.9015, 0.9076], requires_grad=True)]\n",
      "epoch: 11/30, loss: 3.6126973554928554e-06\n",
      "[Parameter containing:\n",
      "tensor([0.9288, 0.8914, 0.8977], requires_grad=True)]\n",
      "epoch: 12/30, loss: 1.1041773859687964e-06\n",
      "[Parameter containing:\n",
      "tensor([0.9196, 0.8813, 0.8876], requires_grad=True)]\n",
      "epoch: 13/30, loss: 3.7577973444058443e-07\n",
      "[Parameter containing:\n",
      "tensor([0.9102, 0.8712, 0.8775], requires_grad=True)]\n",
      "epoch: 14/30, loss: 1.4234559841952432e-07\n",
      "[Parameter containing:\n",
      "tensor([0.9007, 0.8611, 0.8674], requires_grad=True)]\n",
      "epoch: 15/30, loss: 5.519149226529407e-08\n",
      "[Parameter containing:\n",
      "tensor([0.8911, 0.8510, 0.8572], requires_grad=True)]\n",
      "epoch: 16/30, loss: 2.3247819669336423e-08\n",
      "[Parameter containing:\n",
      "tensor([0.8813, 0.8409, 0.8471], requires_grad=True)]\n",
      "epoch: 17/30, loss: 1.028012963644187e-08\n",
      "[Parameter containing:\n",
      "tensor([0.8715, 0.8308, 0.8369], requires_grad=True)]\n",
      "epoch: 18/30, loss: 4.535431141761137e-09\n",
      "[Parameter containing:\n",
      "tensor([0.8617, 0.8208, 0.8267], requires_grad=True)]\n",
      "epoch: 19/30, loss: 7.03820612901751e-10\n",
      "[Parameter containing:\n",
      "tensor([0.8518, 0.8108, 0.8165], requires_grad=True)]\n",
      "epoch: 20/30, loss: 5.978399025829972e-11\n",
      "[Parameter containing:\n",
      "tensor([0.8419, 0.8008, 0.8064], requires_grad=True)]\n",
      "epoch: 21/30, loss: 8.152362465652097e-12\n",
      "[Parameter containing:\n",
      "tensor([0.8320, 0.7909, 0.7963], requires_grad=True)]\n",
      "epoch: 22/30, loss: 2.717454299777655e-12\n",
      "[Parameter containing:\n",
      "tensor([0.8221, 0.7810, 0.7862], requires_grad=True)]\n",
      "epoch: 23/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.8122, 0.7711, 0.7761], requires_grad=True)]\n",
      "epoch: 24/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.8023, 0.7613, 0.7661], requires_grad=True)]\n",
      "epoch: 25/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.7924, 0.7515, 0.7562], requires_grad=True)]\n",
      "epoch: 26/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.7826, 0.7418, 0.7462], requires_grad=True)]\n",
      "epoch: 27/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.7727, 0.7321, 0.7363], requires_grad=True)]\n",
      "epoch: 28/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.7629, 0.7225, 0.7265], requires_grad=True)]\n",
      "epoch: 29/30, loss: 0.0\n",
      "[Parameter containing:\n",
      "tensor([0.7532, 0.7129, 0.7167], requires_grad=True)]\n",
      "epoch: 30/30, loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "from models.GAT_model import GAT\n",
    "from models.GCN_model import GCN\n",
    "from models.GIN_model import GIN\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from early_stop import EarlyStopping\n",
    "from models.BWGNN_model import BWGNN_em\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pygod.models import DOMINANT\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from dataset import pyg_dataset, pyg_to_dgl\n",
    "from utils import *\n",
    "from sklearn.cluster import KMeans\n",
    "from models.ssl_loss import SSL_loss\n",
    "\n",
    "torch.manual_seed(21)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def train_for_mul_model_dci(model_list, optimizer_list, loss_dci, data, shuf_feat, epochs, b_weight, recluster_interval, cluster_info, number_class, b_optimizer=None):\n",
    "    \"\"\"Train for multi model togeother. Like GAT, GCN, BWGNN or anyelse. \n",
    "    Args: \n",
    "        b_weight: option for numbers list or learnable parameters\n",
    "        b_optimizer: None if b_weight is numbers list, real torch optimizer if b_weight belongs to the learnable parameters.\n",
    "    \"\"\"\n",
    "    best_val_auc = 0\n",
    "    for epoch in range(epochs):\n",
    "        print (b_weight)\n",
    "        hiddle_list1 = []\n",
    "        hiddle_list2 = []\n",
    "        for pos, model in enumerate(model_list):\n",
    "            model.train()\n",
    "            if pos % 2 == 0:\n",
    "                _, hid1 = model(data.x)\n",
    "                hid1 = hid1 * b_weight[0][int(pos/2)]\n",
    "                hiddle_list1.append(hid1)\n",
    "            else:\n",
    "                _, hid2 = model(shuf_feat)\n",
    "                hid2 = hid2 * b_weight[0][int(pos/2)]\n",
    "                hiddle_list2.append(hid2)\n",
    "            # if pos != 2:\n",
    "                # hid = hid * 0\n",
    "        # 特征融合以及特征学习\n",
    "        hiddle1 = torch.concat(hiddle_list1, axis=1)\n",
    "        hiddle1 = zero2one((1-cosine_distance(hiddle1.T)).mean(axis=0))*hiddle1\n",
    "        hiddle2 = torch.concat(hiddle_list2, axis=1)\n",
    "        hiddle2 = zero2one((1-cosine_distance(hiddle2.T)).mean(axis=0))*hiddle2\n",
    "\n",
    "        train_loss = loss_dci(hiddle1, hiddle2, None, None, None, cluster_info, number_class)\n",
    "        \n",
    "        optimizer_list.zero_grad()\n",
    "        b_optimizer.zero_grad()\n",
    "        optimizer_loss.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer_list.step()\n",
    "        b_optimizer.step()\n",
    "        optimizer_loss.step()\n",
    "        print (f\"epoch: {epoch + 1}/{epochs}, loss: {train_loss}\")\n",
    "\n",
    "\n",
    "        # re-clustering\n",
    "        if epoch % recluster_interval == 0:\n",
    "            hiddle_list1 = []\n",
    "            for pos, model in enumerate(model_list):\n",
    "                model.eval()\n",
    "                if pos % 2 == 0:\n",
    "                    _, hid1 = model(data.x)\n",
    "                    hid1 = hid1 * b_weight[0][int(pos/2)]\n",
    "                    hiddle_list1.append(hid1)\n",
    "            hiddle1 = torch.concat(hiddle_list1, axis=1)\n",
    "            hiddle1 = zero2one((1-cosine_distance(hiddle1.T)).mean(axis=0))*hiddle1\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=number_class, random_state=0).fit(hiddle1.detach().cpu().numpy())\n",
    "            ss_label = kmeans.labels_\n",
    "            cluster_info = [list(np.where(ss_label==i)[0]) for i in range(number_class)]\n",
    "\n",
    "data_name = \"reddit\"\n",
    "parameter_list = np.linspace(start=0,stop=1,num=1)\n",
    "np.random.seed(2)\n",
    "# data = pyg_dataset(dataset_name=data_name, dataset_spilt=[0.4,0.29,0.3]).dataset\n",
    "data = pyg_dataset(dataset_name=data_name, dataset_spilt=[0.4,0.29,0.3], anomaly_type=\"min\").dataset\n",
    "# data2 = pyg_dataset(dataset_name=\"cora\", dataset_spilt=[0.4,0.2,0.3], anomaly_type=\"min\", anomaly_ratio=0.1).dataset\n",
    "dgl_data = pyg_to_dgl(data)\n",
    "data = data.to(device)\n",
    "dgl_data = dgl_data.to(device)\n",
    "a_weight = anomaly_weight(data)\n",
    "\n",
    "epochs = 30\n",
    "hid_dim = 64\n",
    "number_class = 2\n",
    "recluster_interval = 10\n",
    "kmeans = KMeans(n_clusters=number_class, random_state=0).fit(data.x)\n",
    "ss_label = kmeans.labels_\n",
    "cluster_info = [list(np.where(ss_label==i)[0]) for i in range(number_class)]\n",
    "idx = np.random.permutation(data.x.shape[0])\n",
    "shuf_feats = data.x[idx, :]\n",
    "\n",
    "gcn_model1 = GIN(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "gcn_model2 = GIN(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "gat_model1 = GAT(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "gat_model2 = GAT(data.x.shape[1], hid_dim, number_class, data).to(device)\n",
    "bw_model1 = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data).to(device)\n",
    "bw_model2 = BWGNN_em(data.x.shape[1], hid_dim, number_class, dgl_data).to(device)\n",
    "model_list = [gcn_model1, gcn_model2, gat_model1, gat_model2, bw_model1, bw_model2]\n",
    "loss_dci = SSL_loss(hid_dim * int(len(model_list)/2), device)\n",
    "optimizer_loss = Adam(loss_dci.parameters(), lr = 5e-3)\n",
    "optimizer_list = Adam([{\"params\": model.parameters(), \"lr\": 5e-3} for model in model_list])\n",
    "\n",
    "b_weight = [nn.Parameter(torch.ones([int(len(model_list)/2)], dtype=torch.float32, requires_grad=True))]\n",
    "# b_weight = [[0.0, param1, param2]]\n",
    "b_optimizer = Adam(b_weight, lr = 1e-2, weight_decay=5e-2)    \n",
    "train_for_mul_model_dci(model_list, optimizer_list, loss_dci, data, shuf_feats, epochs, b_weight, recluster_interval, cluster_info, number_class, b_optimizer)\n",
    "cls_model = Linear(hid_dim*int(len(model_list)/2), number_class).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([0.7435, 0.7034, 0.7070], requires_grad=True)]\n",
      "Epoch 1/50: val_loss: 0.6922867894172668, val_auc: 0.49755028462998097\n",
      "Epoch 1/50: test_auc: 0.4683506436172684\n",
      "[Parameter containing:\n",
      "tensor([0.7338, 0.6940, 0.6972], requires_grad=True)]\n",
      "Epoch 2/50: val_loss: 0.6964511275291443, val_auc: 0.47800569259962045\n",
      "Epoch 2/50: test_auc: 0.4461159324318045\n",
      "[Parameter containing:\n",
      "tensor([0.7242, 0.6846, 0.6875], requires_grad=True)]\n",
      "Epoch 3/50: val_loss: 0.701653003692627, val_auc: 0.4929222011385199\n",
      "Epoch 3/50: test_auc: 0.493555114085227\n",
      "[Parameter containing:\n",
      "tensor([0.7146, 0.6752, 0.6780], requires_grad=True)]\n",
      "Epoch 4/50: val_loss: 0.706293523311615, val_auc: 0.4906129032258064\n",
      "Epoch 4/50: test_auc: 0.4938252242204563\n",
      "[Parameter containing:\n",
      "tensor([0.7051, 0.6660, 0.6686], requires_grad=True)]\n",
      "Epoch 5/50: val_loss: 0.7078270316123962, val_auc: 0.49049905123339665\n",
      "Epoch 5/50: test_auc: 0.47810655699614296\n",
      "[Parameter containing:\n",
      "tensor([0.6956, 0.6567, 0.6594], requires_grad=True)]\n",
      "Epoch 6/50: val_loss: 0.7064949870109558, val_auc: 0.4879753320683112\n",
      "Epoch 6/50: test_auc: 0.47362360007435295\n",
      "[Parameter containing:\n",
      "tensor([0.6863, 0.6476, 0.6501], requires_grad=True)]\n",
      "Epoch 7/50: val_loss: 0.7044622302055359, val_auc: 0.49507969639468696\n",
      "Epoch 7/50: test_auc: 0.4940517682048422\n",
      "[Parameter containing:\n",
      "tensor([0.6769, 0.6384, 0.6410], requires_grad=True)]\n",
      "Epoch 8/50: val_loss: 0.7026996612548828, val_auc: 0.5108557874762809\n",
      "Epoch 8/50: test_auc: 0.513788686741949\n",
      "[Parameter containing:\n",
      "tensor([0.6676, 0.6293, 0.6321], requires_grad=True)]\n",
      "Epoch 9/50: val_loss: 0.6999057531356812, val_auc: 0.503438330170778\n",
      "Epoch 9/50: test_auc: 0.5198835912449463\n",
      "[Parameter containing:\n",
      "tensor([0.6584, 0.6203, 0.6233], requires_grad=True)]\n",
      "Epoch 10/50: val_loss: 0.6992735862731934, val_auc: 0.5131537001897534\n",
      "Epoch 10/50: test_auc: 0.5478879130071099\n",
      "[Parameter containing:\n",
      "tensor([0.6491, 0.6113, 0.6149], requires_grad=True)]\n",
      "Epoch 11/50: val_loss: 0.6965336799621582, val_auc: 0.5357210626185958\n",
      "Epoch 11/50: test_auc: 0.5659330126864631\n",
      "[Parameter containing:\n",
      "tensor([0.6400, 0.6024, 0.6067], requires_grad=True)]\n",
      "Epoch 12/50: val_loss: 0.6937981843948364, val_auc: 0.5452182163187855\n",
      "Epoch 12/50: test_auc: 0.5736238905153584\n",
      "[Parameter containing:\n",
      "tensor([0.6308, 0.5935, 0.5990], requires_grad=True)]\n",
      "Epoch 13/50: val_loss: 0.6920123100280762, val_auc: 0.563753320683112\n",
      "Epoch 13/50: test_auc: 0.5889707932524746\n",
      "[Parameter containing:\n",
      "tensor([0.6217, 0.5847, 0.5917], requires_grad=True)]\n",
      "Epoch 14/50: val_loss: 0.6885631680488586, val_auc: 0.5723187855787476\n",
      "Epoch 14/50: test_auc: 0.602563432315628\n",
      "[Parameter containing:\n",
      "tensor([0.6127, 0.5760, 0.5848], requires_grad=True)]\n",
      "Epoch 15/50: val_loss: 0.6843901872634888, val_auc: 0.5754421252371917\n",
      "Epoch 15/50: test_auc: 0.6081515172638133\n",
      "[Parameter containing:\n",
      "tensor([0.6039, 0.5674, 0.5783], requires_grad=True)]\n",
      "Epoch 16/50: val_loss: 0.6814025044441223, val_auc: 0.5778937381404174\n",
      "Epoch 16/50: test_auc: 0.6132923230633394\n",
      "[Parameter containing:\n",
      "tensor([0.5951, 0.5588, 0.5723], requires_grad=True)]\n",
      "Epoch 17/50: val_loss: 0.681117594242096, val_auc: 0.5796925996204934\n",
      "Epoch 17/50: test_auc: 0.6139777638366095\n",
      "[Parameter containing:\n",
      "tensor([0.5864, 0.5503, 0.5663], requires_grad=True)]\n",
      "Epoch 18/50: val_loss: 0.6852396130561829, val_auc: 0.5803946869070208\n",
      "Epoch 18/50: test_auc: 0.6161125052279381\n",
      "[Parameter containing:\n",
      "tensor([0.5777, 0.5419, 0.5602], requires_grad=True)]\n",
      "Epoch 19/50: val_loss: 0.6884681582450867, val_auc: 0.5803624288425048\n",
      "Epoch 19/50: test_auc: 0.6176866954784144\n",
      "[Parameter containing:\n",
      "tensor([0.5693, 0.5336, 0.5532], requires_grad=True)]\n",
      "Epoch 20/50: val_loss: 0.6882035732269287, val_auc: 0.5801555977229602\n",
      "Epoch 20/50: test_auc: 0.6186596728472513\n",
      "[Parameter containing:\n",
      "tensor([0.5609, 0.5253, 0.5454], requires_grad=True)]\n",
      "Epoch 21/50: val_loss: 0.6850111484527588, val_auc: 0.581404174573055\n",
      "Epoch 21/50: test_auc: 0.6186669338723919\n",
      "[Parameter containing:\n",
      "tensor([0.5527, 0.5172, 0.5367], requires_grad=True)]\n",
      "Epoch 22/50: val_loss: 0.6791086792945862, val_auc: 0.5809563567362429\n",
      "Epoch 22/50: test_auc: 0.6176721734281332\n",
      "[Parameter containing:\n",
      "tensor([0.5447, 0.5091, 0.5276], requires_grad=True)]\n",
      "Epoch 23/50: val_loss: 0.6752520203590393, val_auc: 0.579404174573055\n",
      "Epoch 23/50: test_auc: 0.6170477252660439\n",
      "[Parameter containing:\n",
      "tensor([0.5367, 0.5011, 0.5185], requires_grad=True)]\n",
      "Epoch 24/50: val_loss: 0.6820053458213806, val_auc: 0.5803301707779885\n",
      "Epoch 24/50: test_auc: 0.6185783493656769\n",
      "[Parameter containing:\n",
      "tensor([0.5287, 0.4932, 0.5097], requires_grad=True)]\n",
      "Epoch 25/50: val_loss: 0.675193190574646, val_auc: 0.5780151802656547\n",
      "Epoch 25/50: test_auc: 0.6150959617082579\n",
      "[Parameter containing:\n",
      "tensor([0.5209, 0.4853, 0.5012], requires_grad=True)]\n",
      "Epoch 26/50: val_loss: 0.6764442920684814, val_auc: 0.5783301707779887\n",
      "Epoch 26/50: test_auc: 0.6162141595799061\n",
      "[Parameter containing:\n",
      "tensor([0.5131, 0.4776, 0.4932], requires_grad=True)]\n",
      "Epoch 27/50: val_loss: 0.6708601117134094, val_auc: 0.5756774193548386\n",
      "Epoch 27/50: test_auc: 0.6126620660811376\n",
      "[Parameter containing:\n",
      "tensor([0.5052, 0.4699, 0.4859], requires_grad=True)]\n",
      "Epoch 28/50: val_loss: 0.6754679083824158, val_auc: 0.578201138519924\n",
      "Epoch 28/50: test_auc: 0.6160166596960825\n",
      "[Parameter containing:\n",
      "tensor([0.4973, 0.4623, 0.4795], requires_grad=True)]\n",
      "Epoch 29/50: val_loss: 0.6775140166282654, val_auc: 0.5797722960151803\n",
      "Epoch 29/50: test_auc: 0.6161212184581069\n",
      "[Parameter containing:\n",
      "tensor([0.4894, 0.4548, 0.4735], requires_grad=True)]\n",
      "Epoch 30/50: val_loss: 0.6753793954849243, val_auc: 0.5804629981024668\n",
      "Epoch 30/50: test_auc: 0.6175821367163902\n",
      "[Parameter containing:\n",
      "tensor([0.4816, 0.4473, 0.4677], requires_grad=True)]\n",
      "Epoch 31/50: val_loss: 0.6706953644752502, val_auc: 0.5797874762808349\n",
      "Epoch 31/50: test_auc: 0.6175153352850968\n",
      "[Parameter containing:\n",
      "tensor([0.4739, 0.4400, 0.4621], requires_grad=True)]\n",
      "Epoch 32/50: val_loss: 0.6737623810768127, val_auc: 0.5822884250474384\n",
      "Epoch 32/50: test_auc: 0.6196036061155258\n",
      "[Parameter containing:\n",
      "tensor([0.4662, 0.4327, 0.4568], requires_grad=True)]\n",
      "Epoch 33/50: val_loss: 0.677339494228363, val_auc: 0.5836166982922201\n",
      "Epoch 33/50: test_auc: 0.6217339908917701\n",
      "[Parameter containing:\n",
      "tensor([0.4585, 0.4255, 0.4515], requires_grad=True)]\n",
      "Epoch 34/50: val_loss: 0.6739054322242737, val_auc: 0.5835142314990512\n",
      "Epoch 34/50: test_auc: 0.6224208838700683\n",
      "[Parameter containing:\n",
      "tensor([0.4510, 0.4183, 0.4457], requires_grad=True)]\n",
      "Epoch 35/50: val_loss: 0.6719355583190918, val_auc: 0.5838709677419354\n",
      "Epoch 35/50: test_auc: 0.6239703866350667\n",
      "[Parameter containing:\n",
      "tensor([0.4436, 0.4113, 0.4397], requires_grad=True)]\n",
      "Epoch 36/50: val_loss: 0.6833173036575317, val_auc: 0.5859430740037951\n",
      "Epoch 36/50: test_auc: 0.6255024629397277\n",
      "[Parameter containing:\n",
      "tensor([0.4363, 0.4043, 0.4335], requires_grad=True)]\n",
      "Epoch 37/50: val_loss: 0.6654371023178101, val_auc: 0.5835407969639469\n",
      "Epoch 37/50: test_auc: 0.6245527208513407\n",
      "[Parameter containing:\n",
      "tensor([0.4292, 0.3974, 0.4267], requires_grad=True)]\n",
      "Epoch 38/50: val_loss: 0.6706783771514893, val_auc: 0.5845199240986718\n",
      "Epoch 38/50: test_auc: 0.6255866908313583\n",
      "[Parameter containing:\n",
      "tensor([0.4221, 0.3906, 0.4204], requires_grad=True)]\n",
      "Epoch 39/50: val_loss: 0.6762397885322571, val_auc: 0.5856622390891841\n",
      "Epoch 39/50: test_auc: 0.6265858078907013\n",
      "[Parameter containing:\n",
      "tensor([0.4150, 0.3838, 0.4143], requires_grad=True)]\n",
      "Epoch 40/50: val_loss: 0.6759722232818604, val_auc: 0.5865768500948767\n",
      "Epoch 40/50: test_auc: 0.6265625726102513\n",
      "[Parameter containing:\n",
      "tensor([0.4081, 0.3772, 0.4082], requires_grad=True)]\n",
      "Epoch 41/50: val_loss: 0.6695778965950012, val_auc: 0.5875597722960152\n",
      "Epoch 41/50: test_auc: 0.6274745573679074\n",
      "[Parameter containing:\n",
      "tensor([0.4013, 0.3706, 0.4022], requires_grad=True)]\n",
      "Epoch 42/50: val_loss: 0.6655855774879456, val_auc: 0.5897760910815939\n",
      "Epoch 42/50: test_auc: 0.6272886751243089\n",
      "[Parameter containing:\n",
      "tensor([0.3946, 0.3641, 0.3967], requires_grad=True)]\n",
      "Epoch 43/50: val_loss: 0.6646092534065247, val_auc: 0.5888804554079696\n",
      "Epoch 43/50: test_auc: 0.6283516892048888\n",
      "[Parameter containing:\n",
      "tensor([0.3879, 0.3576, 0.3921], requires_grad=True)]\n",
      "Epoch 44/50: val_loss: 0.6707510352134705, val_auc: 0.5901897533206831\n",
      "Epoch 44/50: test_auc: 0.6284213950462383\n",
      "[Parameter containing:\n",
      "tensor([0.3812, 0.3513, 0.3882], requires_grad=True)]\n",
      "Epoch 45/50: val_loss: 0.6715099215507507, val_auc: 0.5899089184060721\n",
      "Epoch 45/50: test_auc: 0.6286305125702868\n",
      "[Parameter containing:\n",
      "tensor([0.3745, 0.3450, 0.3845], requires_grad=True)]\n",
      "Epoch 46/50: val_loss: 0.6698566675186157, val_auc: 0.5918861480075901\n",
      "Epoch 46/50: test_auc: 0.6294408429759747\n",
      "[Parameter containing:\n",
      "tensor([0.3680, 0.3388, 0.3808], requires_grad=True)]\n",
      "Epoch 47/50: val_loss: 0.6691707372665405, val_auc: 0.5920379506641367\n",
      "Epoch 47/50: test_auc: 0.6301117616989638\n",
      "[Parameter containing:\n",
      "tensor([0.3616, 0.3326, 0.3771], requires_grad=True)]\n",
      "Epoch 48/50: val_loss: 0.6684011816978455, val_auc: 0.5932903225806451\n",
      "Epoch 48/50: test_auc: 0.6302366513313816\n",
      "[Parameter containing:\n",
      "tensor([0.3553, 0.3266, 0.3733], requires_grad=True)]\n",
      "Epoch 49/50: val_loss: 0.6680331230163574, val_auc: 0.5932144212523718\n",
      "Epoch 49/50: test_auc: 0.6304254379850365\n",
      "[Parameter containing:\n",
      "tensor([0.3491, 0.3206, 0.3694], requires_grad=True)]\n",
      "Epoch 50/50: val_loss: 0.6684432625770569, val_auc: 0.5932941176470587\n",
      "Epoch 50/50: test_auc: 0.6309540406152703\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "optimizer_list = Adam([{\"params\": model.parameters(), \"lr\": 5e-3} for model in model_list])\n",
    "cls_optimizer = Adam(cls_model.parameters(), lr = 5e-3)\n",
    "# fine-tuning\n",
    "for epoch in range(epochs):\n",
    "    print (b_weight)\n",
    "    hiddle_list1 = []\n",
    "    cls_model.train()\n",
    "    for pos, model in enumerate(model_list):\n",
    "        model.train()\n",
    "        if pos % 2 == 0:\n",
    "            _, hid1 = model(data.x)\n",
    "            hid1 = hid1 * b_weight[0][int(pos/2)]\n",
    "            hiddle_list1.append(hid1)\n",
    "        # if pos != 2:\n",
    "            # hid = hid * 0\n",
    "    # 特征融合以及特征学习\n",
    "    hiddle1 = torch.concat(hiddle_list1, axis=1)\n",
    "    hiddle1 = zero2one((1-cosine_distance(hiddle1.T)).mean(axis=0))*hiddle1\n",
    "\n",
    "    logits = cls_model(hiddle1)\n",
    "    train_loss = cross_entropy(logits[data.train_mask], data.y[data.train_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "    cls_optimizer.zero_grad()\n",
    "    optimizer_list.zero_grad()\n",
    "    b_optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    cls_optimizer.step()\n",
    "    optimizer_list.step()\n",
    "    b_optimizer.step()\n",
    "\n",
    "    cls_model.eval()\n",
    "    hiddle_list1 = []\n",
    "    for pos, model in enumerate(model_list):\n",
    "        model.train()\n",
    "        if pos % 2 == 0:\n",
    "            _, hid1 = model(data.x)\n",
    "            hid1 = hid1 * b_weight[0][int(pos/2)]\n",
    "            hiddle_list1.append(hid1)\n",
    "    hiddle1 = torch.concat(hiddle_list1, axis=1)\n",
    "    hiddle1 = zero2one((1-cosine_distance(hiddle1.T)).mean(axis=0))*hiddle1\n",
    "\n",
    "    logits = cls_model(hiddle1)\n",
    "    val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask], weight=torch.tensor([1.0, a_weight],device=device))\n",
    "    # val_loss = cross_entropy(logits[data.val_mask], data.y[data.val_mask])\n",
    "    probs = logits.softmax(1)\n",
    "    auc = roc_auc_score(data.y[data.val_mask].cpu().numpy(), probs[data.val_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "            roc_auc_score(data.y[data.val_mask].numpy(), probs[data.val_mask][:,1].detach().numpy())\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: val_loss: {val_loss}, val_auc: {auc}\")\n",
    "    \n",
    "    \n",
    "    auc = roc_auc_score(data.y[data.test_mask].cpu().numpy(), probs[data.test_mask][:,1].detach().cpu().numpy()) if data.y.is_cuda else \\\n",
    "            roc_auc_score(data.y[data.test_mask].numpy(), probs[data.test_mask][:,1].detach().numpy())\n",
    "    print (f\"Epoch {epoch+1}/{epochs}: test_auc: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('pygod2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f987d60b78ec8bc0fb235c2085f536aa1d712f65118694d4c0ee12c7b7a8940"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
