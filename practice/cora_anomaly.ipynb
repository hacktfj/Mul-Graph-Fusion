{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's begin Cora dataset trip from a simple example supported by PYGod\n",
    "\n",
    "**Hear,we push the process like below**\n",
    "\n",
    "1. load Cora datasets using torch_geometric.datasets.Planetoid\n",
    "\n",
    "2. generate the contextual and structure outliers using Pyg tools\n",
    "\n",
    "3. create a classical GNN model and train for the binary classification task\n",
    "\n",
    "4. inference from the trained model under a transductive manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, Load the Cora datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T  \n",
    "\n",
    "data = Planetoid('../data/Cora','Cora',transform = None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index[0]\n",
    "data.num_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 5.,  ..., 1., 4., 4.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "degree(data.edge_index[0],data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import torch\n",
    "\n",
    "y_pan = pandas.Series(data.y)\n",
    "min_class = y_pan.value_counts().argmin()\n",
    "torch.tensor(y_pan.apply(lambda x: 1 if x == min_class else 0).to_numpy(),dtype = torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, generater the synthesis anomaly by pyg tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_mask, test_mask = train_test_split(list(range(data.num_node_features)),train_size=0.6)\n",
    "val_mask, test_mask = train_test_split(test_mask,train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_contextual_outliers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11540\\2564513631.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_contextual_outliers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gen_contextual_outliers' is not defined"
     ]
    }
   ],
   "source": [
    "data, yc = gen_contextual_outliers(data,n=100,k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pygod.generator import gen_contextual_outliers,gen_structural_outliers\n",
    "\n",
    "data, yc = gen_contextual_outliers(data,n=100,k=50)\n",
    "data, ys = gen_structural_outliers(data,m=10,n=10)\n",
    "\n",
    "data.y = yc.logical_or(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9021)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pygod.models import DOMINANT\n",
    "\n",
    "model = DOMINANT()\n",
    "model.act = torch.nn.functional.leaky_relu\n",
    "model.fit(data)\n",
    "\n",
    "model.decision_function(data).max()\n",
    "data.y.eq(torch.tensor(model.predict(data))).sum()/len(data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice for Cora binary classication using GCN by means of PYG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "cora_dataset = Planetoid('./data/Cora',\"Cora\",transform=T.NormalizeFeatures())[0]\n",
    "\n",
    "import torch\n",
    "from pygod.generator import gen_contextual_outliers,gen_structural_outliers\n",
    "\n",
    "cora_dataset, yc = gen_contextual_outliers(cora_dataset,n=100,k=50)\n",
    "cora_dataset, ys = gen_structural_outliers(cora_dataset,m=10,n=10)\n",
    "\n",
    "cora_dataset.y = yc.logical_or(ys).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simplenet(\n",
       "  (conv1): GCNConv(1433, 16)\n",
       "  (conv2): GCNConv(16, 2)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Simplenet(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_feat,h_feat,num_classes):\n",
    "        super(Simplenet,self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_feat,h_feat)\n",
    "        self.conv2 = GCNConv(h_feat,num_classes)\n",
    "\n",
    "    def forward(self,data):\n",
    "        feature, edges = data.x,data.edge_index\n",
    "        feature = F.relu(self.conv1(feature,edges))\n",
    "        feature = F.dropout(feature,training = self.training)\n",
    "        feature = self.conv2(feature,edges)\n",
    "        return feature\n",
    "\n",
    "num_classes = cora_dataset.y.unique().size()[0]\n",
    "model = Simplenet(cora_dataset.num_features,16,num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/200: loss 0.6893231272697449;\n",
      "epoch 1/200: loss 0.6408898234367371;\n",
      "epoch 2/200: loss 0.5857827067375183;\n",
      "epoch 3/200: loss 0.5336357951164246;\n",
      "epoch 4/200: loss 0.48117560148239136;\n",
      "epoch 5/200: loss 0.44403642416000366;\n",
      "epoch 6/200: loss 0.3986646234989166;\n",
      "epoch 7/200: loss 0.35421186685562134;\n",
      "epoch 8/200: loss 0.3231336772441864;\n",
      "epoch 9/200: loss 0.29867130517959595;\n",
      "epoch 10/200: loss 0.2797528803348541;\n",
      "epoch 11/200: loss 0.25842171907424927;\n",
      "epoch 12/200: loss 0.24012736976146698;\n",
      "epoch 13/200: loss 0.22958038747310638;\n",
      "epoch 14/200: loss 0.23098653554916382;\n",
      "epoch 15/200: loss 0.21935051679611206;\n",
      "epoch 16/200: loss 0.20875413715839386;\n",
      "epoch 17/200: loss 0.21216996014118195;\n",
      "epoch 18/200: loss 0.20706915855407715;\n",
      "epoch 19/200: loss 0.20212964713573456;\n",
      "epoch 20/200: loss 0.21409916877746582;\n",
      "epoch 21/200: loss 0.21310937404632568;\n",
      "epoch 22/200: loss 0.20550285279750824;\n",
      "epoch 23/200: loss 0.20233143866062164;\n",
      "epoch 24/200: loss 0.20953412353992462;\n",
      "epoch 25/200: loss 0.19761598110198975;\n",
      "epoch 26/200: loss 0.19428293406963348;\n",
      "epoch 27/200: loss 0.19299280643463135;\n",
      "epoch 28/200: loss 0.2009253054857254;\n",
      "epoch 29/200: loss 0.20099301636219025;\n",
      "epoch 30/200: loss 0.19435426592826843;\n",
      "epoch 31/200: loss 0.18635985255241394;\n",
      "epoch 32/200: loss 0.1988585740327835;\n",
      "epoch 33/200: loss 0.2011232227087021;\n",
      "epoch 34/200: loss 0.19120395183563232;\n",
      "epoch 35/200: loss 0.19749830663204193;\n",
      "epoch 36/200: loss 0.18123199045658112;\n",
      "epoch 37/200: loss 0.18856891989707947;\n",
      "epoch 38/200: loss 0.18481317162513733;\n",
      "epoch 39/200: loss 0.17894792556762695;\n",
      "epoch 40/200: loss 0.188789501786232;\n",
      "epoch 41/200: loss 0.18947741389274597;\n",
      "epoch 42/200: loss 0.18296702206134796;\n",
      "epoch 43/200: loss 0.1764608472585678;\n",
      "epoch 44/200: loss 0.181498184800148;\n",
      "epoch 45/200: loss 0.1841588318347931;\n",
      "epoch 46/200: loss 0.1832636147737503;\n",
      "epoch 47/200: loss 0.1744818389415741;\n",
      "epoch 48/200: loss 0.17715275287628174;\n",
      "epoch 49/200: loss 0.17854280769824982;\n",
      "epoch 50/200: loss 0.18111978471279144;\n",
      "epoch 51/200: loss 0.18273797631263733;\n",
      "epoch 52/200: loss 0.1729138046503067;\n",
      "epoch 53/200: loss 0.17308257520198822;\n",
      "epoch 54/200: loss 0.1797822266817093;\n",
      "epoch 55/200: loss 0.1761641502380371;\n",
      "epoch 56/200: loss 0.17754316329956055;\n",
      "epoch 57/200: loss 0.1701880544424057;\n",
      "epoch 58/200: loss 0.17103488743305206;\n",
      "epoch 59/200: loss 0.17700865864753723;\n",
      "epoch 60/200: loss 0.17437753081321716;\n",
      "epoch 61/200: loss 0.17889516055583954;\n",
      "epoch 62/200: loss 0.1688307821750641;\n",
      "epoch 63/200: loss 0.16957831382751465;\n",
      "epoch 64/200: loss 0.17041295766830444;\n",
      "epoch 65/200: loss 0.17152300477027893;\n",
      "epoch 66/200: loss 0.16849346458911896;\n",
      "epoch 67/200: loss 0.17059855163097382;\n",
      "epoch 68/200: loss 0.17050951719284058;\n",
      "epoch 69/200: loss 0.1576959639787674;\n",
      "epoch 70/200: loss 0.1600703001022339;\n",
      "epoch 71/200: loss 0.16326941549777985;\n",
      "epoch 72/200: loss 0.1680503636598587;\n",
      "epoch 73/200: loss 0.1658986508846283;\n",
      "epoch 74/200: loss 0.16769784688949585;\n",
      "epoch 75/200: loss 0.16219684481620789;\n",
      "epoch 76/200: loss 0.16629277169704437;\n",
      "epoch 77/200: loss 0.15539540350437164;\n",
      "epoch 78/200: loss 0.16267210245132446;\n",
      "epoch 79/200: loss 0.16420260071754456;\n",
      "epoch 80/200: loss 0.16549336910247803;\n",
      "epoch 81/200: loss 0.1622733324766159;\n",
      "epoch 82/200: loss 0.15061098337173462;\n",
      "epoch 83/200: loss 0.1568440943956375;\n",
      "epoch 84/200: loss 0.1641848087310791;\n",
      "epoch 85/200: loss 0.15868249535560608;\n",
      "epoch 86/200: loss 0.15892259776592255;\n",
      "epoch 87/200: loss 0.1602572202682495;\n",
      "epoch 88/200: loss 0.15819799900054932;\n",
      "epoch 89/200: loss 0.1525835245847702;\n",
      "epoch 90/200: loss 0.15554048120975494;\n",
      "epoch 91/200: loss 0.1557297557592392;\n",
      "epoch 92/200: loss 0.15426036715507507;\n",
      "epoch 93/200: loss 0.15144002437591553;\n",
      "epoch 94/200: loss 0.1530476063489914;\n",
      "epoch 95/200: loss 0.14922180771827698;\n",
      "epoch 96/200: loss 0.15835273265838623;\n",
      "epoch 97/200: loss 0.15775476396083832;\n",
      "epoch 98/200: loss 0.1511843055486679;\n",
      "epoch 99/200: loss 0.15113325417041779;\n",
      "epoch 100/200: loss 0.14614605903625488;\n",
      "epoch 101/200: loss 0.15123137831687927;\n",
      "epoch 102/200: loss 0.15566101670265198;\n",
      "epoch 103/200: loss 0.15259744226932526;\n",
      "epoch 104/200: loss 0.15040889382362366;\n",
      "epoch 105/200: loss 0.14458616077899933;\n",
      "epoch 106/200: loss 0.1390525847673416;\n",
      "epoch 107/200: loss 0.14634431898593903;\n",
      "epoch 108/200: loss 0.14638672769069672;\n",
      "epoch 109/200: loss 0.145977184176445;\n",
      "epoch 110/200: loss 0.14969325065612793;\n",
      "epoch 111/200: loss 0.14309710264205933;\n",
      "epoch 112/200: loss 0.1460498571395874;\n",
      "epoch 113/200: loss 0.14659632742404938;\n",
      "epoch 114/200: loss 0.13923561573028564;\n",
      "epoch 115/200: loss 0.14486531913280487;\n",
      "epoch 116/200: loss 0.1431129425764084;\n",
      "epoch 117/200: loss 0.1437305510044098;\n",
      "epoch 118/200: loss 0.1429162323474884;\n",
      "epoch 119/200: loss 0.1344541311264038;\n",
      "epoch 120/200: loss 0.1421201378107071;\n",
      "epoch 121/200: loss 0.14535579085350037;\n",
      "epoch 122/200: loss 0.13730086386203766;\n",
      "epoch 123/200: loss 0.13966074585914612;\n",
      "epoch 124/200: loss 0.14185203611850739;\n",
      "epoch 125/200: loss 0.13818836212158203;\n",
      "epoch 126/200: loss 0.13725754618644714;\n",
      "epoch 127/200: loss 0.13718031346797943;\n",
      "epoch 128/200: loss 0.1396634876728058;\n",
      "epoch 129/200: loss 0.14169207215309143;\n",
      "epoch 130/200: loss 0.1364935338497162;\n",
      "epoch 131/200: loss 0.13682809472084045;\n",
      "epoch 132/200: loss 0.13564252853393555;\n",
      "epoch 133/200: loss 0.1348426789045334;\n",
      "epoch 134/200: loss 0.13825784623622894;\n",
      "epoch 135/200: loss 0.13037551939487457;\n",
      "epoch 136/200: loss 0.13606785237789154;\n",
      "epoch 137/200: loss 0.13250049948692322;\n",
      "epoch 138/200: loss 0.13638247549533844;\n",
      "epoch 139/200: loss 0.13424120843410492;\n",
      "epoch 140/200: loss 0.13087837398052216;\n",
      "epoch 141/200: loss 0.13536302745342255;\n",
      "epoch 142/200: loss 0.13730153441429138;\n",
      "epoch 143/200: loss 0.1320198029279709;\n",
      "epoch 144/200: loss 0.1363225132226944;\n",
      "epoch 145/200: loss 0.13216491043567657;\n",
      "epoch 146/200: loss 0.1389874368906021;\n",
      "epoch 147/200: loss 0.13118164241313934;\n",
      "epoch 148/200: loss 0.1353141814470291;\n",
      "epoch 149/200: loss 0.12882693111896515;\n",
      "epoch 150/200: loss 0.12958909571170807;\n",
      "epoch 151/200: loss 0.13202965259552002;\n",
      "epoch 152/200: loss 0.13260260224342346;\n",
      "epoch 153/200: loss 0.13160254061222076;\n",
      "epoch 154/200: loss 0.12982194125652313;\n",
      "epoch 155/200: loss 0.13128189742565155;\n",
      "epoch 156/200: loss 0.13194403052330017;\n",
      "epoch 157/200: loss 0.1309969276189804;\n",
      "epoch 158/200: loss 0.13106349110603333;\n",
      "epoch 159/200: loss 0.13395732641220093;\n",
      "epoch 160/200: loss 0.1286836415529251;\n",
      "epoch 161/200: loss 0.12757481634616852;\n",
      "epoch 162/200: loss 0.13314604759216309;\n",
      "epoch 163/200: loss 0.12669230997562408;\n",
      "epoch 164/200: loss 0.1290775090456009;\n",
      "epoch 165/200: loss 0.12911701202392578;\n",
      "epoch 166/200: loss 0.1284070611000061;\n",
      "epoch 167/200: loss 0.12392229586839676;\n",
      "epoch 168/200: loss 0.131281778216362;\n",
      "epoch 169/200: loss 0.1311386525630951;\n",
      "epoch 170/200: loss 0.13211949169635773;\n",
      "epoch 171/200: loss 0.12881600856781006;\n",
      "epoch 172/200: loss 0.1279221922159195;\n",
      "epoch 173/200: loss 0.1292223483324051;\n",
      "epoch 174/200: loss 0.12738041579723358;\n",
      "epoch 175/200: loss 0.12491754442453384;\n",
      "epoch 176/200: loss 0.12480735778808594;\n",
      "epoch 177/200: loss 0.1254715472459793;\n",
      "epoch 178/200: loss 0.12555749714374542;\n",
      "epoch 179/200: loss 0.128248929977417;\n",
      "epoch 180/200: loss 0.12528367340564728;\n",
      "epoch 181/200: loss 0.12207697331905365;\n",
      "epoch 182/200: loss 0.12241502851247787;\n",
      "epoch 183/200: loss 0.12091143429279327;\n",
      "epoch 184/200: loss 0.12667815387248993;\n",
      "epoch 185/200: loss 0.12546144425868988;\n",
      "epoch 186/200: loss 0.12598365545272827;\n",
      "epoch 187/200: loss 0.1243753582239151;\n",
      "epoch 188/200: loss 0.12532123923301697;\n",
      "epoch 189/200: loss 0.12242498248815536;\n",
      "epoch 190/200: loss 0.1276312619447708;\n",
      "epoch 191/200: loss 0.12512342631816864;\n",
      "epoch 192/200: loss 0.12852081656455994;\n",
      "epoch 193/200: loss 0.12387548387050629;\n",
      "epoch 194/200: loss 0.12392190843820572;\n",
      "epoch 195/200: loss 0.12620022892951965;\n",
      "epoch 196/200: loss 0.12322308868169785;\n",
      "epoch 197/200: loss 0.12454599887132645;\n",
      "epoch 198/200: loss 0.12248554080724716;\n",
      "epoch 199/200: loss 0.12177524715662003;\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 200\n",
    "lr = 1e-2\n",
    "# weight decay is similar as L2 normalize, what different is weight decay matter in update strategy.\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = lr,weight_decay=1e-3)\n",
    "\n",
    "cora_dataset.x = cora_dataset.x.to(device)\n",
    "cora_dataset.edge_index = cora_dataset.edge_index.to(device)\n",
    "cora_dataset.y = cora_dataset.y.to(device)\n",
    "for i in range(epochs):\n",
    "    model.zero_grad()\n",
    "    logit = model(cora_dataset)\n",
    "    loss = F.cross_entropy(logit[cora_dataset.train_mask],cora_dataset.y[cora_dataset.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print (f\"epoch {i}/{epochs}: loss {loss};\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate is 92.60000610351562%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "\n",
    "logits = model(cora_dataset)\n",
    "probs = F.log_softmax(logits,dim=1)\n",
    "y_hat = probs.max(1)[1][cora_dataset.test_mask]\n",
    "correct_rate = cora_dataset.y[cora_dataset.test_mask].eq(y_hat).sum()/cora_dataset.test_mask.sum()*100\n",
    "print (f\"Correct rate is {correct_rate}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conlusion: Specify a binary anomaly task:\n",
    "\n",
    "1. First, using tradition GCN network shows a less competitive, even not far from a random selection.\n",
    "\n",
    "2. Second, with a test mask (1400 node) to train, we get a 100% precision and low F1 score. Even though, auc runs to 99% is amazing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4428667328235363\n",
      "(array([0.        , 0.00107991, 0.00215983, 0.00215983, 0.01727862,\n",
      "       0.01727862, 0.02159827, 0.02159827, 0.03023758, 0.03023758,\n",
      "       0.03779698, 0.03779698, 0.04859611, 0.04859611, 0.05723542,\n",
      "       0.05831533, 0.06047516, 0.06047516, 0.07019438, 0.07019438,\n",
      "       0.09179266, 0.09179266, 0.11987041, 0.11987041, 0.12203024,\n",
      "       0.12203024, 0.12526998, 0.12526998, 0.12634989, 0.12634989,\n",
      "       0.13390929, 0.13390929, 0.14362851, 0.14362851, 0.15442765,\n",
      "       0.15442765, 0.16198704, 0.16198704, 0.16306695, 0.16306695,\n",
      "       0.16954644, 0.16954644, 0.19114471, 0.19114471, 0.19978402,\n",
      "       0.19978402, 0.20194384, 0.20194384, 0.20302376, 0.20518359,\n",
      "       0.2062635 , 0.2062635 , 0.21274298, 0.21274298, 0.21598272,\n",
      "       0.21598272, 0.22678186, 0.22678186, 0.24082073, 0.24082073,\n",
      "       0.24190065, 0.24190065, 0.24298056, 0.24298056, 0.2699784 ,\n",
      "       0.2699784 , 0.27429806, 0.27429806, 0.31965443, 0.32181425,\n",
      "       0.33261339, 0.33261339, 0.33693305, 0.33693305, 0.3412527 ,\n",
      "       0.34341253, 0.34773218, 0.34773218, 0.3574514 , 0.3574514 ,\n",
      "       0.36393089, 0.36609071, 0.37149028, 0.37149028, 0.37796976,\n",
      "       0.37796976, 0.38444924, 0.38444924, 0.41144708, 0.41360691,\n",
      "       0.42116631, 0.42116631, 0.4600432 , 0.4600432 , 0.46220302,\n",
      "       0.46220302, 0.48596112, 0.48812095, 0.53887689, 0.53887689,\n",
      "       0.54319654, 0.54319654, 0.54859611, 0.54859611, 0.56155508,\n",
      "       0.56155508, 0.58531317, 0.58531317, 0.59395248, 0.59395248,\n",
      "       0.59827214, 0.59827214, 0.62634989, 0.62850972, 0.63282937,\n",
      "       0.6349892 , 0.69114471, 0.69114471, 0.7224622 , 0.7224622 ,\n",
      "       0.75053996, 0.75053996, 0.77969762, 0.77969762, 0.78077754,\n",
      "       0.78077754, 0.78617711, 0.78617711, 0.79265659, 0.79265659,\n",
      "       0.81857451, 0.81857451, 0.82937365, 0.82937365, 0.85421166,\n",
      "       0.85421166, 0.85529158, 0.8574514 , 0.87796976, 0.87796976,\n",
      "       0.87904968, 0.87904968, 0.89956803, 0.89956803, 0.94060475,\n",
      "       0.94276458, 0.94492441, 0.94492441, 0.95140389, 0.95140389,\n",
      "       0.95896328, 0.95896328, 0.96436285, 0.96436285, 0.97732181,\n",
      "       0.97732181, 0.99460043, 0.99460043, 0.99676026, 0.99676026,\n",
      "       0.99784017, 0.99784017, 0.99892009, 1.        ]), array([0.        , 0.        , 0.        , 0.01351351, 0.01351351,\n",
      "       0.02702703, 0.02702703, 0.05405405, 0.05405405, 0.06756757,\n",
      "       0.06756757, 0.08108108, 0.08108108, 0.09459459, 0.09459459,\n",
      "       0.10810811, 0.10810811, 0.12162162, 0.12162162, 0.13513514,\n",
      "       0.13513514, 0.14864865, 0.14864865, 0.16216216, 0.16216216,\n",
      "       0.17567568, 0.17567568, 0.18918919, 0.18918919, 0.2027027 ,\n",
      "       0.2027027 , 0.22972973, 0.22972973, 0.24324324, 0.24324324,\n",
      "       0.25675676, 0.25675676, 0.27027027, 0.27027027, 0.28378378,\n",
      "       0.28378378, 0.2972973 , 0.2972973 , 0.31081081, 0.31081081,\n",
      "       0.32432432, 0.32432432, 0.33783784, 0.33783784, 0.33783784,\n",
      "       0.33783784, 0.35135135, 0.35135135, 0.36486486, 0.36486486,\n",
      "       0.37837838, 0.37837838, 0.39189189, 0.39189189, 0.40540541,\n",
      "       0.40540541, 0.41891892, 0.41891892, 0.43243243, 0.43243243,\n",
      "       0.44594595, 0.44594595, 0.45945946, 0.45945946, 0.45945946,\n",
      "       0.45945946, 0.47297297, 0.47297297, 0.48648649, 0.48648649,\n",
      "       0.48648649, 0.48648649, 0.5       , 0.5       , 0.51351351,\n",
      "       0.51351351, 0.51351351, 0.51351351, 0.52702703, 0.52702703,\n",
      "       0.54054054, 0.54054054, 0.55405405, 0.55405405, 0.55405405,\n",
      "       0.55405405, 0.56756757, 0.56756757, 0.58108108, 0.58108108,\n",
      "       0.59459459, 0.59459459, 0.59459459, 0.59459459, 0.60810811,\n",
      "       0.60810811, 0.62162162, 0.62162162, 0.63513514, 0.63513514,\n",
      "       0.64864865, 0.64864865, 0.66216216, 0.66216216, 0.67567568,\n",
      "       0.67567568, 0.68918919, 0.68918919, 0.68918919, 0.68918919,\n",
      "       0.68918919, 0.68918919, 0.7027027 , 0.7027027 , 0.71621622,\n",
      "       0.71621622, 0.72972973, 0.72972973, 0.74324324, 0.74324324,\n",
      "       0.75675676, 0.75675676, 0.77027027, 0.77027027, 0.78378378,\n",
      "       0.78378378, 0.7972973 , 0.7972973 , 0.81081081, 0.81081081,\n",
      "       0.82432432, 0.82432432, 0.82432432, 0.82432432, 0.83783784,\n",
      "       0.83783784, 0.85135135, 0.85135135, 0.86486486, 0.86486486,\n",
      "       0.86486486, 0.86486486, 0.87837838, 0.87837838, 0.89189189,\n",
      "       0.89189189, 0.90540541, 0.90540541, 0.93243243, 0.93243243,\n",
      "       0.94594595, 0.94594595, 0.95945946, 0.95945946, 0.97297297,\n",
      "       0.97297297, 0.98648649, 0.98648649, 1.        ]), array([ 9.99987125e-01, -1.28745205e-05, -1.13957591e-04, -1.70216372e-04,\n",
      "       -3.66361556e-03, -3.77632515e-03, -5.35280816e-03, -5.82781667e-03,\n",
      "       -8.11647717e-03, -8.15325044e-03, -9.06246714e-03, -9.33827180e-03,\n",
      "       -1.14804581e-02, -1.15270074e-02, -1.24122063e-02, -1.25362948e-02,\n",
      "       -1.26396520e-02, -1.27053326e-02, -1.41103230e-02, -1.42934322e-02,\n",
      "       -1.63554996e-02, -1.63685177e-02, -1.89228784e-02, -1.89576186e-02,\n",
      "       -1.91134121e-02, -1.91710666e-02, -1.93475224e-02, -1.93935912e-02,\n",
      "       -1.94082055e-02, -1.94794051e-02, -2.02037487e-02, -2.03364529e-02,\n",
      "       -2.11820304e-02, -2.13170554e-02, -2.18429789e-02, -2.18912624e-02,\n",
      "       -2.20990721e-02, -2.23068390e-02, -2.23589484e-02, -2.23679245e-02,\n",
      "       -2.32144650e-02, -2.32430007e-02, -2.44407635e-02, -2.44855508e-02,\n",
      "       -2.52081379e-02, -2.52122059e-02, -2.52412669e-02, -2.53742337e-02,\n",
      "       -2.54875440e-02, -2.55385600e-02, -2.56093238e-02, -2.57572308e-02,\n",
      "       -2.63004471e-02, -2.63194889e-02, -2.65495982e-02, -2.65551712e-02,\n",
      "       -2.69486234e-02, -2.69625466e-02, -2.76775509e-02, -2.76845079e-02,\n",
      "       -2.78723352e-02, -2.80712564e-02, -2.82023419e-02, -2.82079037e-02,\n",
      "       -3.02364174e-02, -3.02980617e-02, -3.11438106e-02, -3.12446840e-02,\n",
      "       -3.37303728e-02, -3.37348655e-02, -3.43044102e-02, -3.43046412e-02,\n",
      "       -3.46539468e-02, -3.47339734e-02, -3.50156799e-02, -3.50454934e-02,\n",
      "       -3.52555364e-02, -3.52757908e-02, -3.57824601e-02, -3.57908569e-02,\n",
      "       -3.60353552e-02, -3.61067615e-02, -3.63162383e-02, -3.65149789e-02,\n",
      "       -3.72058451e-02, -3.72848623e-02, -3.77900600e-02, -3.78606506e-02,\n",
      "       -3.93527150e-02, -3.93623412e-02, -3.99175771e-02, -4.00427692e-02,\n",
      "       -4.28265519e-02, -4.29122075e-02, -4.29972857e-02, -4.30081338e-02,\n",
      "       -4.50234003e-02, -4.50324006e-02, -4.92090806e-02, -4.93235812e-02,\n",
      "       -4.98417877e-02, -4.99229878e-02, -5.02804853e-02, -5.03327437e-02,\n",
      "       -5.14857210e-02, -5.15901111e-02, -5.36638722e-02, -5.36895208e-02,\n",
      "       -5.44386208e-02, -5.45860492e-02, -5.48235141e-02, -5.49036339e-02,\n",
      "       -5.80671914e-02, -5.80928363e-02, -5.84224649e-02, -5.84850945e-02,\n",
      "       -6.35843724e-02, -6.40930012e-02, -6.66246042e-02, -6.66906312e-02,\n",
      "       -7.08646253e-02, -7.14719072e-02, -7.54656494e-02, -7.54796937e-02,\n",
      "       -7.55164996e-02, -7.57259503e-02, -7.65777677e-02, -7.66814500e-02,\n",
      "       -7.78873935e-02, -7.79879615e-02, -8.18776190e-02, -8.26154500e-02,\n",
      "       -8.41356367e-02, -8.42274651e-02, -8.96953121e-02, -9.02739540e-02,\n",
      "       -9.07877013e-02, -9.09861401e-02, -9.80765820e-02, -9.82945338e-02,\n",
      "       -9.87415463e-02, -9.94413644e-02, -1.04042083e-01, -1.04045525e-01,\n",
      "       -1.17377751e-01, -1.18477389e-01, -1.21210381e-01, -1.23494670e-01,\n",
      "       -1.28588498e-01, -1.28836587e-01, -1.38486087e-01, -1.38664290e-01,\n",
      "       -1.45821437e-01, -1.49194121e-01, -1.72399923e-01, -1.73079237e-01,\n",
      "       -3.30055654e-01, -3.36074740e-01, -4.01667058e-01, -4.03498381e-01,\n",
      "       -4.16379571e-01, -4.17977154e-01, -4.38268751e-01, -4.47115600e-01],\n",
      "      dtype=float32))\n",
      "[[926   0]\n",
      " [ 74   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# 200/2300\n",
    "from sklearn.metrics import f1_score,roc_auc_score,accuracy_score,recall_score,precision_score,confusion_matrix,roc_curve\n",
    "\n",
    "pred = y_hat.cpu()\n",
    "target = cora_dataset.y[cora_dataset.test_mask].cpu()\n",
    "\n",
    "print (accuracy_score(target, pred))\n",
    "print (precision_score(target, pred))\n",
    "print (recall_score(target, pred))\n",
    "\n",
    "print (f1_score(target, pred))\n",
    "\n",
    "print (roc_auc_score(target, F.softmax(logits,dim=1)[cora_dataset.test_mask].cpu().detach().numpy()[:,1]))\n",
    "print (roc_curve(target, probs[cora_dataset.test_mask].cpu().detach().numpy()[:,0]))\n",
    "\n",
    "print (confusion_matrix(target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y, _ = roc_curve(target, probs[cora_dataset.test_mask].cpu().detach().numpy()[:,1])\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a1210f74d545411e7e37329724f4cd36e65edebc5fef549f67f43defe69c357"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('pygod2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
