{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\pygod2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "cora_dataset = Planetoid('../data/Cora',\"Cora\",transform=T.NormalizeFeatures())[0]\n",
    "\n",
    "import torch\n",
    "from pygod.generator import gen_contextual_outliers,gen_structural_outliers\n",
    "\n",
    "cora_dataset, yc = gen_contextual_outliers(cora_dataset,n=100,k=50)\n",
    "cora_dataset, ys = gen_structural_outliers(cora_dataset,m=10,n=10)\n",
    "\n",
    "cora_dataset.y = yc.logical_or(ys).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simplenet(\n",
       "  (conv1): GCNConv(1433, 16)\n",
       "  (conv2): GCNConv(16, 2)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Simplenet(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_feat,h_feat,num_classes):\n",
    "        super(Simplenet,self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_feat,h_feat)\n",
    "        self.conv2 = GCNConv(h_feat,num_classes)\n",
    "\n",
    "    def forward(self,data):\n",
    "        feature, edges = data.x,data.edge_index\n",
    "        feature = F.relu(self.conv1(feature,edges))\n",
    "        feature = F.dropout(feature,training = self.training)\n",
    "        feature = self.conv2(feature,edges)\n",
    "        return feature\n",
    "\n",
    "num_classes = cora_dataset.y.unique().size()[0]\n",
    "model = Simplenet(cora_dataset.num_features,16,num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/200: loss 0.033317968249320984;\n",
      "epoch 1/200: loss 0.01973869651556015;\n",
      "epoch 2/200: loss 0.015788733959197998;\n",
      "epoch 3/200: loss 0.025798728689551353;\n",
      "epoch 4/200: loss 0.0251594427973032;\n",
      "epoch 5/200: loss 0.01944752410054207;\n",
      "epoch 6/200: loss 0.02333417721092701;\n",
      "epoch 7/200: loss 0.03221406415104866;\n",
      "epoch 8/200: loss 0.03539278730750084;\n",
      "epoch 9/200: loss 0.032997988164424896;\n",
      "epoch 10/200: loss 0.031322378665208817;\n",
      "epoch 11/200: loss 0.029849069193005562;\n",
      "epoch 12/200: loss 0.03398548811674118;\n",
      "epoch 13/200: loss 0.033562492579221725;\n",
      "epoch 14/200: loss 0.030403638258576393;\n",
      "epoch 15/200: loss 0.03825858235359192;\n",
      "epoch 16/200: loss 0.030433187261223793;\n",
      "epoch 17/200: loss 0.03082474321126938;\n",
      "epoch 18/200: loss 0.03312817960977554;\n",
      "epoch 19/200: loss 0.0318792350590229;\n",
      "epoch 20/200: loss 0.028531739488244057;\n",
      "epoch 21/200: loss 0.03930153697729111;\n",
      "epoch 22/200: loss 0.034282829612493515;\n",
      "epoch 23/200: loss 0.02836693823337555;\n",
      "epoch 24/200: loss 0.03299897536635399;\n",
      "epoch 25/200: loss 0.03817310556769371;\n",
      "epoch 26/200: loss 0.0301533080637455;\n",
      "epoch 27/200: loss 0.02814742922782898;\n",
      "epoch 28/200: loss 0.02794610522687435;\n",
      "epoch 29/200: loss 0.02932559698820114;\n",
      "epoch 30/200: loss 0.028968026861548424;\n",
      "epoch 31/200: loss 0.040948402136564255;\n",
      "epoch 32/200: loss 0.030627328902482986;\n",
      "epoch 33/200: loss 0.03635043650865555;\n",
      "epoch 34/200: loss 0.03249707818031311;\n",
      "epoch 35/200: loss 0.03342350572347641;\n",
      "epoch 36/200: loss 0.03459357097744942;\n",
      "epoch 37/200: loss 0.03808051347732544;\n",
      "epoch 38/200: loss 0.04222598299384117;\n",
      "epoch 39/200: loss 0.0360415019094944;\n",
      "epoch 40/200: loss 0.029016491025686264;\n",
      "epoch 41/200: loss 0.030794817954301834;\n",
      "epoch 42/200: loss 0.033036671578884125;\n",
      "epoch 43/200: loss 0.027843182906508446;\n",
      "epoch 44/200: loss 0.030230727046728134;\n",
      "epoch 45/200: loss 0.034124135971069336;\n",
      "epoch 46/200: loss 0.02692936174571514;\n",
      "epoch 47/200: loss 0.031658221036195755;\n",
      "epoch 48/200: loss 0.035804662853479385;\n",
      "epoch 49/200: loss 0.031247327104210854;\n",
      "epoch 50/200: loss 0.02516806498169899;\n",
      "epoch 51/200: loss 0.0339338593184948;\n",
      "epoch 52/200: loss 0.03164990246295929;\n",
      "epoch 53/200: loss 0.03086989000439644;\n",
      "epoch 54/200: loss 0.036960896104574203;\n",
      "epoch 55/200: loss 0.03751373663544655;\n",
      "epoch 56/200: loss 0.036518946290016174;\n",
      "epoch 57/200: loss 0.03437609225511551;\n",
      "epoch 58/200: loss 0.03420098498463631;\n",
      "epoch 59/200: loss 0.03397965803742409;\n",
      "epoch 60/200: loss 0.03391397371888161;\n",
      "epoch 61/200: loss 0.035680994391441345;\n",
      "epoch 62/200: loss 0.02820228599011898;\n",
      "epoch 63/200: loss 0.030815264210104942;\n",
      "epoch 64/200: loss 0.03120272234082222;\n",
      "epoch 65/200: loss 0.0327204130589962;\n",
      "epoch 66/200: loss 0.03168565779924393;\n",
      "epoch 67/200: loss 0.047394853085279465;\n",
      "epoch 68/200: loss 0.03623087704181671;\n",
      "epoch 69/200: loss 0.03150622919201851;\n",
      "epoch 70/200: loss 0.035855941474437714;\n",
      "epoch 71/200: loss 0.04101353883743286;\n",
      "epoch 72/200: loss 0.033247482031583786;\n",
      "epoch 73/200: loss 0.028922131285071373;\n",
      "epoch 74/200: loss 0.032380279153585434;\n",
      "epoch 75/200: loss 0.031175024807453156;\n",
      "epoch 76/200: loss 0.03256385028362274;\n",
      "epoch 77/200: loss 0.031947117298841476;\n",
      "epoch 78/200: loss 0.03690179064869881;\n",
      "epoch 79/200: loss 0.043574489653110504;\n",
      "epoch 80/200: loss 0.03522830829024315;\n",
      "epoch 81/200: loss 0.03187945857644081;\n",
      "epoch 82/200: loss 0.036351580172777176;\n",
      "epoch 83/200: loss 0.040082138031721115;\n",
      "epoch 84/200: loss 0.03478102758526802;\n",
      "epoch 85/200: loss 0.036610476672649384;\n",
      "epoch 86/200: loss 0.025810660794377327;\n",
      "epoch 87/200: loss 0.03448078781366348;\n",
      "epoch 88/200: loss 0.036724649369716644;\n",
      "epoch 89/200: loss 0.029548052698373795;\n",
      "epoch 90/200: loss 0.034372955560684204;\n",
      "epoch 91/200: loss 0.03545510768890381;\n",
      "epoch 92/200: loss 0.03069758415222168;\n",
      "epoch 93/200: loss 0.02828463725745678;\n",
      "epoch 94/200: loss 0.034825995564460754;\n",
      "epoch 95/200: loss 0.03626581281423569;\n",
      "epoch 96/200: loss 0.02705797180533409;\n",
      "epoch 97/200: loss 0.029922038316726685;\n",
      "epoch 98/200: loss 0.02564673125743866;\n",
      "epoch 99/200: loss 0.03398764878511429;\n",
      "epoch 100/200: loss 0.03643526881933212;\n",
      "epoch 101/200: loss 0.03277529403567314;\n",
      "epoch 102/200: loss 0.032486096024513245;\n",
      "epoch 103/200: loss 0.03553559258580208;\n",
      "epoch 104/200: loss 0.029145540669560432;\n",
      "epoch 105/200: loss 0.02976606972515583;\n",
      "epoch 106/200: loss 0.044306475669145584;\n",
      "epoch 107/200: loss 0.04494014009833336;\n",
      "epoch 108/200: loss 0.030095113441348076;\n",
      "epoch 109/200: loss 0.033931538462638855;\n",
      "epoch 110/200: loss 0.029661720618605614;\n",
      "epoch 111/200: loss 0.0347781777381897;\n",
      "epoch 112/200: loss 0.032540593296289444;\n",
      "epoch 113/200: loss 0.03242777660489082;\n",
      "epoch 114/200: loss 0.039143506437540054;\n",
      "epoch 115/200: loss 0.03067632019519806;\n",
      "epoch 116/200: loss 0.03960370644927025;\n",
      "epoch 117/200: loss 0.028633207082748413;\n",
      "epoch 118/200: loss 0.03571981191635132;\n",
      "epoch 119/200: loss 0.031731754541397095;\n",
      "epoch 120/200: loss 0.03522111847996712;\n",
      "epoch 121/200: loss 0.03058473765850067;\n",
      "epoch 122/200: loss 0.03817851468920708;\n",
      "epoch 123/200: loss 0.030022263526916504;\n",
      "epoch 124/200: loss 0.03611479699611664;\n",
      "epoch 125/200: loss 0.0322360023856163;\n",
      "epoch 126/200: loss 0.03095993958413601;\n",
      "epoch 127/200: loss 0.03420063853263855;\n",
      "epoch 128/200: loss 0.030520446598529816;\n",
      "epoch 129/200: loss 0.03332212567329407;\n",
      "epoch 130/200: loss 0.03441210091114044;\n",
      "epoch 131/200: loss 0.02605767361819744;\n",
      "epoch 132/200: loss 0.03607018664479256;\n",
      "epoch 133/200: loss 0.04079573601484299;\n",
      "epoch 134/200: loss 0.03356867656111717;\n",
      "epoch 135/200: loss 0.04089302197098732;\n",
      "epoch 136/200: loss 0.02862028405070305;\n",
      "epoch 137/200: loss 0.03321024030447006;\n",
      "epoch 138/200: loss 0.04071073979139328;\n",
      "epoch 139/200: loss 0.03906271979212761;\n",
      "epoch 140/200: loss 0.03786756470799446;\n",
      "epoch 141/200: loss 0.03157200664281845;\n",
      "epoch 142/200: loss 0.03338285908102989;\n",
      "epoch 143/200: loss 0.028228063136339188;\n",
      "epoch 144/200: loss 0.034917254000902176;\n",
      "epoch 145/200: loss 0.028974130749702454;\n",
      "epoch 146/200: loss 0.03244119510054588;\n",
      "epoch 147/200: loss 0.02717152237892151;\n",
      "epoch 148/200: loss 0.025577791035175323;\n",
      "epoch 149/200: loss 0.03597422316670418;\n",
      "epoch 150/200: loss 0.02673337794840336;\n",
      "epoch 151/200: loss 0.029193788766860962;\n",
      "epoch 152/200: loss 0.029246637597680092;\n",
      "epoch 153/200: loss 0.03584963455796242;\n",
      "epoch 154/200: loss 0.02998543716967106;\n",
      "epoch 155/200: loss 0.040677446871995926;\n",
      "epoch 156/200: loss 0.030408045276999474;\n",
      "epoch 157/200: loss 0.03239535540342331;\n",
      "epoch 158/200: loss 0.03643232583999634;\n",
      "epoch 159/200: loss 0.03478286787867546;\n",
      "epoch 160/200: loss 0.03162636607885361;\n",
      "epoch 161/200: loss 0.03308050334453583;\n",
      "epoch 162/200: loss 0.032510947436094284;\n",
      "epoch 163/200: loss 0.028203682973980904;\n",
      "epoch 164/200: loss 0.03971562534570694;\n",
      "epoch 165/200: loss 0.03445671498775482;\n",
      "epoch 166/200: loss 0.029437825083732605;\n",
      "epoch 167/200: loss 0.030519891530275345;\n",
      "epoch 168/200: loss 0.030673973262310028;\n",
      "epoch 169/200: loss 0.02614372782409191;\n",
      "epoch 170/200: loss 0.030064212158322334;\n",
      "epoch 171/200: loss 0.033788323402404785;\n",
      "epoch 172/200: loss 0.03233961760997772;\n",
      "epoch 173/200: loss 0.03702544420957565;\n",
      "epoch 174/200: loss 0.028331052511930466;\n",
      "epoch 175/200: loss 0.03663915395736694;\n",
      "epoch 176/200: loss 0.030869903042912483;\n",
      "epoch 177/200: loss 0.04208634793758392;\n",
      "epoch 178/200: loss 0.03175843879580498;\n",
      "epoch 179/200: loss 0.031034370884299278;\n",
      "epoch 180/200: loss 0.02990403212606907;\n",
      "epoch 181/200: loss 0.032303690910339355;\n",
      "epoch 182/200: loss 0.04340639337897301;\n",
      "epoch 183/200: loss 0.030379697680473328;\n",
      "epoch 184/200: loss 0.03371913358569145;\n",
      "epoch 185/200: loss 0.035677582025527954;\n",
      "epoch 186/200: loss 0.03967418521642685;\n",
      "epoch 187/200: loss 0.02986823208630085;\n",
      "epoch 188/200: loss 0.030808325856924057;\n",
      "epoch 189/200: loss 0.04271475970745087;\n",
      "epoch 190/200: loss 0.041095998138189316;\n",
      "epoch 191/200: loss 0.03539685532450676;\n",
      "epoch 192/200: loss 0.03828825801610947;\n",
      "epoch 193/200: loss 0.03333340585231781;\n",
      "epoch 194/200: loss 0.03717229887843132;\n",
      "epoch 195/200: loss 0.035575058311223984;\n",
      "epoch 196/200: loss 0.03226184472441673;\n",
      "epoch 197/200: loss 0.03213246166706085;\n",
      "epoch 198/200: loss 0.033162374049425125;\n",
      "epoch 199/200: loss 0.02875535562634468;\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 200\n",
    "lr = 1e-2\n",
    "# weight decay is similar as L2 normalize, what different is weight decay matter in update strategy.\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = lr,weight_decay=1e-3)\n",
    "\n",
    "cora_dataset.x = cora_dataset.x.to(device)\n",
    "cora_dataset.edge_index = cora_dataset.edge_index.to(device)\n",
    "cora_dataset.y = cora_dataset.y.to(device)\n",
    "rate = (1-cora_dataset.y).sum()/cora_dataset.y.sum()\n",
    "for i in range(epochs):\n",
    "    model.zero_grad()\n",
    "    logit = model(cora_dataset)\n",
    "    loss = F.cross_entropy(logit[cora_dataset.train_mask],cora_dataset.y[cora_dataset.train_mask],weight=torch.tensor([1.0,rate]).to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print (f\"epoch {i}/{epochs}: loss {loss};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate is 92.30000305175781%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "\n",
    "logits = model(cora_dataset)\n",
    "probs = F.log_softmax(logits,dim=1)\n",
    "y_hat = probs.max(1)[1][cora_dataset.test_mask]\n",
    "correct_rate = cora_dataset.y[cora_dataset.test_mask].eq(y_hat).sum()/cora_dataset.test_mask.sum()*100\n",
    "print (f\"Correct rate is {correct_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [140, 1000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5852\\1888810453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcora_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcora_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pygod2\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multilabel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pygod2\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\pygod2\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    332\u001b[0m         raise ValueError(\n\u001b[0;32m    333\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m         )\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [140, 1000]"
     ]
    }
   ],
   "source": [
    "# 200/2300\n",
    "from sklearn.metrics import f1_score,roc_auc_score,accuracy_score,recall_score,precision_score,confusion_matrix,roc_curve\n",
    "\n",
    "pred = y_hat.cpu()\n",
    "target = cora_dataset.y[cora_dataset.test_mask].cpu()\n",
    "\n",
    "print (accuracy_score(target, pred))\n",
    "print (precision_score(target, pred))\n",
    "print (recall_score(target, pred))\n",
    "\n",
    "print (f1_score(target, pred))\n",
    "\n",
    "print (roc_auc_score(target, F.softmax(logits,dim=1)[cora_dataset.test_mask].cpu().detach().numpy()[:,1]))\n",
    "print (roc_curve(target, probs[cora_dataset.test_mask].cpu().detach().numpy()[:,0]))\n",
    "\n",
    "print (confusion_matrix(target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a1210f74d545411e7e37329724f4cd36e65edebc5fef549f67f43defe69c357"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('pygod2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
